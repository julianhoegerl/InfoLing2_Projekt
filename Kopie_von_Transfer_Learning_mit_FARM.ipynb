{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Kopie von Transfer Learning mit FARM.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "jTURIXRzPwoB",
        "LFXvXYB4WkNw",
        "nL8B9oqTg4f6",
        "cVnUyU9DhQo5",
        "lXbVD_NEhqZ-",
        "NoF8lgzEimL6",
        "aiUubEaJiBus"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTThkpiIXlJP"
      },
      "source": [
        "# Transfer Learning mit FARM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ipd2E0WSPDW"
      },
      "source": [
        "## Ziele\n",
        "\n",
        "\n",
        "*   BERT und Transfer Learning\n",
        "*   Einführung in FARM\n",
        "\n",
        "*   Wie funktioniert das Fine-Tuning eines BERT-Modells?\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7ivyIhQUc92"
      },
      "source": [
        "## BERT und Transfer Learning\n",
        "Transfer Learning ist eine Machine Learning-Methode, bei der ein Modell, das für einen bestimmten Task trainiert wurde, als Ausgangspunkt für ein Modell für einen anderen Task verwendet wird.\n",
        "\n",
        "Die Idee hinter diesem Konzept ist, dass man das Erlernte von einem Problem auf ein anderes überträgt. Wenn man z.B. Java als erste Programmiersprache gelernt hat, fällt es uns leichter, Python oder eine andere Programmiersprache zu erlernen. Denn die grundlegenden Programmierkonzepte aus Java/OOP können leicht auf jede andere Programmiersprache übertragen werden. Man muss also nicht von Null beginnnen, sondern kann auf existierendem Wissen aufbauen. Ähnlich verhält es sich auch mit dem Transfer Learning von Sprachmodellen wie BERT. Existierende BERT-Modelle wurden hauptsächlich auf der Wikipedia trainiert. Möchte man allerdings Textklassifikationen in bestimmten Domänen durchführen, ist die Wikipedia mit ihrem Vokabular nicht repräsentativ genug. Daher nutzt man das Fine-Tuning von vortrainierten BERT-Modellen, um sie an den neuen Task/das neue Problem anzupassen. Durch diesen Schritt erhofft man sich bessere Performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYna70JXkjFk"
      },
      "source": [
        "## FARM \n",
        "(**F**ramework for **A**dapting **R**epresentation **M**odels)\n",
        "![farm_logo](https://github.com/deepset-ai/FARM/raw/master/docs/img/farm_logo_text_right_wide.png?raw=true)\n",
        "... ermöglicht uns, das Transfer Learning mit BERT (und anderen Sprachmodellen) umzusetzen. Damit kann man einfach und schnell Modelle für Tasks wie Text Classification, NER und Question Answering erstellen:\n",
        "![task_table](https://miro.medium.com/max/3840/1*0RYwLSOMegTKfhnyTwkjIQ.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPltDefXjSiJ"
      },
      "source": [
        "# Fine-Tuning eines BERT-Modells mit FARM (adapted from FARM-Tutorial 1)\n",
        "\n",
        "Welcome to the FARM building blocks tutorial! There are many different ways to make use of this repository, but in this notebook, we will be going through the most import building blocks that will help you harvest the rewards of a successfully trained NLP model.\n",
        "\n",
        "Happy FARMing!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2cj67ZBjYSC"
      },
      "source": [
        "## 1) Text Classification\n",
        "\n",
        "GermEval 2018 (GermEval2018) (https://projects.fzai.h-da.de/iggsa/) is an open data set containing texts that need to be classified by whether they are offensive or not. There are a set of coarse and fine labels, but here we will only be looking at the coarse set which labels each example as either OFFENSE or OTHER. To tackle this task, we are going to build a classifier that is composed of Google's BERT language model and a feed forward neural network prediction head."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQz1S2d5jfST"
      },
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PeIWk6_4jiQ2",
        "outputId": "ea1b3591-afe1-4937-b191-c6dfb7414faa"
      },
      "source": [
        "# Install FARM\n",
        "!pip install torch==1.6.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!pip install farm==0.5.0"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Requirement already satisfied: torch==1.6.0+cu101 in /usr/local/lib/python3.7/dist-packages (1.6.0+cu101)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.6.0+cu101) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.6.0+cu101) (1.19.5)\n",
            "Requirement already satisfied: farm==0.5.0 in /usr/local/lib/python3.7/dist-packages (0.5.0)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (from farm==0.5.0) (0.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (from farm==0.5.0) (0.37.0)\n",
            "Requirement already satisfied: flask-cors in /usr/local/lib/python3.7/dist-packages (from farm==0.5.0) (3.0.10)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.7/dist-packages (from farm==0.5.0) (1.4.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from farm==0.5.0) (4.62.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from farm==0.5.0) (57.4.0)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.7/dist-packages (from farm==0.5.0) (1.1.4)\n",
            "Requirement already satisfied: flask-restplus in /usr/local/lib/python3.7/dist-packages (from farm==0.5.0) (0.13.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from farm==0.5.0) (0.3.4)\n",
            "Requirement already satisfied: dotmap==1.3.0 in /usr/local/lib/python3.7/dist-packages (from farm==0.5.0) (1.3.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from farm==0.5.0) (5.4.8)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.7/dist-packages (from farm==0.5.0) (1.18.40)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from farm==0.5.0) (2.23.0)\n",
            "Requirement already satisfied: Werkzeug==0.16.1 in /usr/local/lib/python3.7/dist-packages (from farm==0.5.0) (0.16.1)\n",
            "Requirement already satisfied: torch<1.7,>1.5 in /usr/local/lib/python3.7/dist-packages (from farm==0.5.0) (1.6.0+cu101)\n",
            "Requirement already satisfied: seqeval==0.0.12 in /usr/local/lib/python3.7/dist-packages (from farm==0.5.0) (0.0.12)\n",
            "Requirement already satisfied: mlflow==1.0.0 in /usr/local/lib/python3.7/dist-packages (from farm==0.5.0) (1.0.0)\n",
            "Requirement already satisfied: transformers==3.3.1 in /usr/local/lib/python3.7/dist-packages (from farm==0.5.0) (3.3.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from mlflow==1.0.0->farm==0.5.0) (3.13)\n",
            "Requirement already satisfied: alembic in /usr/local/lib/python3.7/dist-packages (from mlflow==1.0.0->farm==0.5.0) (1.7.1)\n",
            "Requirement already satisfied: simplejson in /usr/local/lib/python3.7/dist-packages (from mlflow==1.0.0->farm==0.5.0) (3.17.5)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from mlflow==1.0.0->farm==0.5.0) (0.3)\n",
            "Requirement already satisfied: gunicorn in /usr/local/lib/python3.7/dist-packages (from mlflow==1.0.0->farm==0.5.0) (20.1.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from mlflow==1.0.0->farm==0.5.0) (1.1.5)\n",
            "Requirement already satisfied: sqlalchemy in /usr/local/lib/python3.7/dist-packages (from mlflow==1.0.0->farm==0.5.0) (1.4.22)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from mlflow==1.0.0->farm==0.5.0) (2.8.2)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from mlflow==1.0.0->farm==0.5.0) (3.17.3)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from mlflow==1.0.0->farm==0.5.0) (7.1.2)\n",
            "Requirement already satisfied: databricks-cli>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from mlflow==1.0.0->farm==0.5.0) (0.15.0)\n",
            "Requirement already satisfied: docker>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from mlflow==1.0.0->farm==0.5.0) (5.0.2)\n",
            "Requirement already satisfied: querystring-parser in /usr/local/lib/python3.7/dist-packages (from mlflow==1.0.0->farm==0.5.0) (1.2.4)\n",
            "Requirement already satisfied: gitpython>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from mlflow==1.0.0->farm==0.5.0) (3.1.18)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mlflow==1.0.0->farm==0.5.0) (1.19.5)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from mlflow==1.0.0->farm==0.5.0) (1.3.0)\n",
            "Requirement already satisfied: sqlparse in /usr/local/lib/python3.7/dist-packages (from mlflow==1.0.0->farm==0.5.0) (0.4.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from mlflow==1.0.0->farm==0.5.0) (1.15.0)\n",
            "Requirement already satisfied: Keras>=2.2.4 in /usr/local/lib/python3.7/dist-packages (from seqeval==0.0.12->farm==0.5.0) (2.6.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.3.1->farm==0.5.0) (21.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==3.3.1->farm==0.5.0) (0.0.45)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc2 in /usr/local/lib/python3.7/dist-packages (from transformers==3.3.1->farm==0.5.0) (0.8.1rc2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3.3.1->farm==0.5.0) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.3.1->farm==0.5.0) (3.0.12)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.7/dist-packages (from transformers==3.3.1->farm==0.5.0) (0.1.96)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.7/dist-packages (from databricks-cli>=0.8.0->mlflow==1.0.0->farm==0.5.0) (0.8.9)\n",
            "Requirement already satisfied: websocket-client>=0.32.0 in /usr/local/lib/python3.7/dist-packages (from docker>=3.6.0->mlflow==1.0.0->farm==0.5.0) (1.2.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from gitpython>=2.1.0->mlflow==1.0.0->farm==0.5.0) (4.0.7)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.0 in /usr/local/lib/python3.7/dist-packages (from gitpython>=2.1.0->mlflow==1.0.0->farm==0.5.0) (3.7.4.3)\n",
            "Requirement already satisfied: smmap<5,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->gitpython>=2.1.0->mlflow==1.0.0->farm==0.5.0) (4.0.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->farm==0.5.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->farm==0.5.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->farm==0.5.0) (1.25.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->farm==0.5.0) (2021.5.30)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch<1.7,>1.5->farm==0.5.0) (0.16.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from alembic->mlflow==1.0.0->farm==0.5.0) (4.6.4)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.7/dist-packages (from alembic->mlflow==1.0.0->farm==0.5.0) (1.1.5)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from alembic->mlflow==1.0.0->farm==0.5.0) (5.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy->mlflow==1.0.0->farm==0.5.0) (1.1.1)\n",
            "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from boto3->farm==0.5.0) (0.5.0)\n",
            "Requirement already satisfied: botocore<1.22.0,>=1.21.40 in /usr/local/lib/python3.7/dist-packages (from boto3->farm==0.5.0) (1.21.40)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3->farm==0.5.0) (0.10.0)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from flask->farm==0.5.0) (1.1.0)\n",
            "Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from flask->farm==0.5.0) (2.11.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->flask->farm==0.5.0) (2.0.1)\n",
            "Requirement already satisfied: aniso8601>=0.82 in /usr/local/lib/python3.7/dist-packages (from flask-restplus->farm==0.5.0) (9.0.1)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from flask-restplus->farm==0.5.0) (2.6.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from flask-restplus->farm==0.5.0) (2018.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->alembic->mlflow==1.0.0->farm==0.5.0) (3.5.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3.3.1->farm==0.5.0) (2.4.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.3.1->farm==0.5.0) (1.0.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn->farm==0.5.0) (0.22.2.post1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y6RKA1bxkKij",
        "outputId": "d6c45c49-f47b-4642-9222-350e598881f9"
      },
      "source": [
        "# Here are the imports we need\n",
        "\n",
        "import torch\n",
        "from farm.modeling.tokenization import Tokenizer\n",
        "from farm.data_handler.processor import TextClassificationProcessor\n",
        "from farm.data_handler.data_silo import DataSilo\n",
        "from farm.modeling.language_model import LanguageModel\n",
        "from farm.modeling.prediction_head import TextClassificationHead\n",
        "from farm.modeling.adaptive_model import AdaptiveModel\n",
        "from farm.modeling.optimization import initialize_optimizer\n",
        "from farm.train import Trainer\n",
        "from farm.utils import MLFlowLogger\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "09/13/2021 16:00:53 - INFO - farm.modeling.prediction_head -   Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XL-on42YOWwC",
        "outputId": "cd961977-6b3a-4071-c652-4f256f02f2c5"
      },
      "source": [
        "# Farm allows simple logging of many parameters & metrics. Let's use the MLflow framework to track our experiment ...\n",
        "# You will see your results on https://public-mlflow.deepset.ai/\n",
        "ml_logger = MLFlowLogger(tracking_uri=\"https://public-mlflow.deepset.ai/\")\n",
        "ml_logger.init_experiment(experiment_name=\"Public_FARM\", run_name=\"Tutorial1_Colab\")\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " __          __  _                            _        \n",
            " \\ \\        / / | |                          | |       \n",
            "  \\ \\  /\\  / /__| | ___ ___  _ __ ___   ___  | |_ ___  \n",
            "   \\ \\/  \\/ / _ \\ |/ __/ _ \\| '_ ` _ \\ / _ \\ | __/ _ \\ \n",
            "    \\  /\\  /  __/ | (_| (_) | | | | | |  __/ | || (_) |\n",
            "     \\/  \\/ \\___|_|\\___\\___/|_| |_| |_|\\___|  \\__\\___/ \n",
            "  ______      _____  __  __  \n",
            " |  ____/\\   |  __ \\|  \\/  |              _.-^-._    .--.\n",
            " | |__ /  \\  | |__) | \\  / |           .-'   _   '-. |__|\n",
            " |  __/ /\\ \\ |  _  /| |\\/| |          /     |_|     \\|  |\n",
            " | | / ____ \\| | \\ \\| |  | |         /               \\  |\n",
            " |_|/_/    \\_\\_|  \\_\\_|  |_|        /|     _____     |\\ |\n",
            "                                     |    |==|==|    |  |\n",
            "|---||---|---|---|---|---|---|---|---|    |--|--|    |  |\n",
            "|---||---|---|---|---|---|---|---|---|    |==|==|    |  |\n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            " \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6uTw7w3kPcJ",
        "outputId": "2c1e600d-5e3c-4500-9ff0-828721391b9d"
      },
      "source": [
        "# We need to fetch the right device to drive the growth of our model\n",
        "# Make sure that you have gpu turned on in this notebook by going to\n",
        "# Runtime>Change runtime type and select GPU as Hardware accelerator.\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Devices available: {}\".format(device))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Devices available: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPogG9KLk-Mv"
      },
      "source": [
        "### Data Handling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMQtJfq775KM"
      },
      "source": [
        "In FARM the Processor contains the functions which handle the conversion from file or request to PyTorch Datasets. In essence, it prepares data to be consumed by the modelling components. This is done in stages to allow for easier debugging. It should be able to handle file input or requests. This class contains everything that needs to be customized when adapting a new dataset. Custom datasets can be handled by extending the Processor (e.g. see CONLLProcessor).\n",
        "\n",
        "The DataSilo is a generic class that stores the train, dev and test data sets. It calls upon the methods from the Processor to do the loading and then exposes a DataLoader for each set. In cases where there is not a separate dev file, it will create one by slicing the train set.\n",
        "![data_handling](https://farm.deepset.ai/_images/data_silo_no_bg.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bu6FaHBbm1Kp"
      },
      "source": [
        "## distilbert-base-uncased-finetuned-sst-2-english"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-2k0Zock2J5",
        "outputId": "2aefe0d5-083f-45df-a95c-dc3ed126eaaf"
      },
      "source": [
        "# Here we initialize a tokenizer that will be used for preprocessing text\n",
        "# This is the BERT Tokenizer which uses the byte pair encoding method.\n",
        "# It is currently loaded with a German model\n",
        "\n",
        "tokenizer = Tokenizer.load(\n",
        "    pretrained_model_name_or_path=\"distilbert-base-uncased-finetuned-sst-2-english\",\n",
        "    do_lower_case=False)\n",
        "\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "09/13/2021 16:00:54 - INFO - farm.modeling.tokenization -   Loading tokenizer of type 'DistilBertTokenizer'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08ycfnEIlJa4"
      },
      "source": [
        "# In order to prepare the data for the model, we need a set of\n",
        "# functions to transform data files into PyTorch Datasets.\n",
        "# We group these together in Processor objects.\n",
        "# We will need a new Processor object for each new source of data.\n",
        "# The abstract class can be found in farm.data_handling.processor.Processor\n",
        "# TOXIC = 1\n",
        "# OTHER = 0\n",
        "LABEL_LIST = [\"0\", \"1\"]\n",
        "processor = TextClassificationProcessor(tokenizer=tokenizer,\n",
        "                                        max_seq_len=128,\n",
        "                                        warmup = 600,\n",
        "                                        train_filename=\"train.tsv\",\n",
        "                                        test_filename=\"test_with_solutions.tsv\",\n",
        "                                        data_dir=\"../content\",\n",
        "                                        label_list=LABEL_LIST,\n",
        "                                        metric=\"f1_macro\",\n",
        "                                        text_column_name=\"Comment\",\n",
        "                                        label_column_name=\"Insult\")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zds83m5klLW8",
        "outputId": "67ed08cf-89b4-4602-c6ba-f869ca617dcf"
      },
      "source": [
        "# We need a DataSilo in order to keep our train, dev and test sets separate.\n",
        "# The DataSilo will call the functions in the Processor to generate these sets.\n",
        "# From the DataSilo, we can fetch a PyTorch DataLoader object which will\n",
        "# be passed on to the model.\n",
        "# Here is a good place to define a batch size for the model\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "data_silo = DataSilo(\n",
        "    processor=processor,\n",
        "    batch_size=BATCH_SIZE)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "09/13/2021 16:00:56 - INFO - farm.data_handler.data_silo -   \n",
            "Loading data into the data silo ... \n",
            "              ______\n",
            "               |o  |   !\n",
            "   __          |:`_|---'-.\n",
            "  |__|______.-/ _ \\-----.|       \n",
            " (o)(o)------'\\ _ /     ( )      \n",
            " \n",
            "09/13/2021 16:00:56 - INFO - farm.data_handler.data_silo -   Loading train set from: ../content/train.tsv \n",
            "09/13/2021 16:00:57 - INFO - farm.data_handler.data_silo -   Got ya 1 parallel workers to convert 3947 dictionaries to pytorch datasets (chunksize = 790)...\n",
            "09/13/2021 16:00:57 - INFO - farm.data_handler.data_silo -    0 \n",
            "09/13/2021 16:00:57 - INFO - farm.data_handler.data_silo -   /|\\\n",
            "09/13/2021 16:00:57 - INFO - farm.data_handler.data_silo -   /'\\\n",
            "09/13/2021 16:00:57 - INFO - farm.data_handler.data_silo -   \n",
            "Preprocessing Dataset ../content/train.tsv:   0%|          | 0/3947 [00:00<?, ? Dicts/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils.py:460: FutureWarning: `is_pretokenized` is deprecated and will be removed in a future version, use `is_split_into_words` instead.\n",
            "  FutureWarning,\n",
            "09/13/2021 16:00:59 - INFO - farm.data_handler.processor -   *** Show 2 random examples ***\n",
            "09/13/2021 16:00:59 - INFO - farm.data_handler.processor -   \n",
            "\n",
            "      .--.        _____                       _      \n",
            "    .'_\\/_'.     / ____|                     | |     \n",
            "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
            "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
            "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
            "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
            "   (/\\||/                             |_|           \n",
            "______\\||/___________________________________________                     \n",
            "\n",
            "ID: 390-0\n",
            "Clear Text: \n",
            " \ttext_classification_label: 0\n",
            " \ttext: \"@YinzerinCT\\xa0\\xa0@beeline\\xa0yeah. \\xa0Definitely not a fan of last year, when we had like 2 separate snowstorms with 3+ ft of snow.\\n\\xa0\\nplus like the 8 other snowstorms we had.\\n\\xa0\\nI love snow, except when I have to shovel it and drive in it\"\n",
            "Tokenized: \n",
            " \ttokens: ['\"', '@', '[UNK]', '\\\\', 'x', '##a', '##0', '\\\\', 'x', '##a', '##0', '@', 'bee', '##line', '\\\\', 'x', '##a', '##0', '##ye', '##ah', '.', '\\\\', '[UNK]', 'not', 'a', 'fan', 'of', 'last', 'year', ',', 'when', 'we', 'had', 'like', '2', 'separate', 'snow', '##storm', '##s', 'with', '3', '+', 'ft', 'of', 'snow', '.', '\\\\', 'n', '\\\\', 'x', '##a', '##0', '\\\\', 'np', '##lus', 'like', 'the', '8', 'other', 'snow', '##storm', '##s', 'we', 'had', '.', '\\\\', 'n', '\\\\', 'x', '##a', '##0', '\\\\', '[UNK]', 'love', 'snow', ',', 'except', 'when', '[UNK]', 'have', 'to', 'shovel', 'it', 'and', 'drive', 'in', 'it', '\"']\n",
            " \toffsets: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 15, 19, 20, 21, 22, 23, 25, 27, 38, 39, 53, 57, 59, 63, 66, 71, 75, 77, 82, 85, 89, 94, 96, 105, 109, 114, 116, 121, 122, 124, 127, 130, 134, 135, 136, 137, 138, 139, 140, 141, 142, 144, 148, 153, 157, 159, 165, 169, 174, 176, 179, 182, 183, 184, 185, 186, 187, 188, 189, 190, 193, 198, 202, 204, 211, 216, 218, 223, 226, 233, 236, 240, 246, 249, 251]\n",
            " \tstart_of_word: [True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, True, True, True, True, True, True, False, True, True, True, True, True, True, True, False, False, True, True, False, True, True, True, False, False, False, False, False, False, False, False, False, False, True, True, True, True, True, False, False, True, True, False, False, False, False, False, False, False, False, False, True, True, False, True, True, True, True, True, True, True, True, True, True, True, False]\n",
            "Features: \n",
            " \tinput_ids: [101, 1000, 1030, 100, 1032, 1060, 2050, 2692, 1032, 1060, 2050, 2692, 1030, 10506, 4179, 1032, 1060, 2050, 2692, 6672, 4430, 1012, 1032, 100, 2025, 1037, 5470, 1997, 2197, 2095, 1010, 2043, 2057, 2018, 2066, 1016, 3584, 4586, 19718, 2015, 2007, 1017, 1009, 3027, 1997, 4586, 1012, 1032, 1050, 1032, 1060, 2050, 2692, 1032, 27937, 7393, 2066, 1996, 1022, 2060, 4586, 19718, 2015, 2057, 2018, 1012, 1032, 1050, 1032, 1060, 2050, 2692, 1032, 100, 2293, 4586, 1010, 3272, 2043, 100, 2031, 2000, 24596, 2009, 1998, 3298, 1999, 2009, 1000, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \ttext_classification_label_ids: [0]\n",
            "_____________________________________________________\n",
            "09/13/2021 16:00:59 - INFO - farm.data_handler.processor -   \n",
            "\n",
            "      .--.        _____                       _      \n",
            "    .'_\\/_'.     / ____|                     | |     \n",
            "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
            "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
            "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
            "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
            "   (/\\||/                             |_|           \n",
            "______\\||/___________________________________________                     \n",
            "\n",
            "ID: 739-0\n",
            "Clear Text: \n",
            " \ttext_classification_label: 0\n",
            " \ttext: \"That looked like you at first glance!\"\n",
            "Tokenized: \n",
            " \ttokens: ['\"', '[UNK]', 'looked', 'like', 'you', 'at', 'first', 'glance', '!', '\"']\n",
            " \toffsets: [0, 1, 6, 13, 18, 22, 25, 31, 37, 38]\n",
            " \tstart_of_word: [True, False, True, True, True, True, True, True, False, False]\n",
            "Features: \n",
            " \tinput_ids: [101, 1000, 100, 2246, 2066, 2017, 2012, 2034, 6054, 999, 1000, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \ttext_classification_label_ids: [0]\n",
            "_____________________________________________________\n",
            "Preprocessing Dataset ../content/train.tsv: 100%|██████████| 3947/3947 [00:10<00:00, 392.83 Dicts/s]\n",
            "09/13/2021 16:01:07 - INFO - farm.data_handler.data_silo -   Loading dev set as a slice of train set\n",
            "09/13/2021 16:01:07 - INFO - farm.data_handler.data_silo -   Took 787 samples out of train set to create dev set (dev split is roughly 0.1)\n",
            "09/13/2021 16:01:07 - INFO - farm.data_handler.data_silo -   Loading test set from: ../content/test_with_solutions.tsv\n",
            "09/13/2021 16:01:07 - INFO - farm.data_handler.data_silo -   Got ya 1 parallel workers to convert 2647 dictionaries to pytorch datasets (chunksize = 530)...\n",
            "09/13/2021 16:01:07 - INFO - farm.data_handler.data_silo -    0 \n",
            "09/13/2021 16:01:07 - INFO - farm.data_handler.data_silo -   /w\\\n",
            "09/13/2021 16:01:07 - INFO - farm.data_handler.data_silo -   /'\\\n",
            "09/13/2021 16:01:07 - INFO - farm.data_handler.data_silo -   \n",
            "Preprocessing Dataset ../content/test_with_solutions.tsv:   0%|          | 0/2647 [00:00<?, ? Dicts/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils.py:460: FutureWarning: `is_pretokenized` is deprecated and will be removed in a future version, use `is_split_into_words` instead.\n",
            "  FutureWarning,\n",
            "09/13/2021 16:01:09 - INFO - farm.data_handler.processor -   *** Show 2 random examples ***\n",
            "09/13/2021 16:01:09 - INFO - farm.data_handler.processor -   \n",
            "\n",
            "      .--.        _____                       _      \n",
            "    .'_\\/_'.     / ____|                     | |     \n",
            "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
            "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
            "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
            "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
            "   (/\\||/                             |_|           \n",
            "______\\||/___________________________________________                     \n",
            "\n",
            "ID: 214-0\n",
            "Clear Text: \n",
            " \ttext_classification_label: 1\n",
            " \ttext: \"On the real shut the fuck up, because you don't even know all the details yet you claim to use logic. How the fuck you gonna use logic, when you don't even know the details. Tell me \\xa0what city dudes mom father died in? Oh you can't, because you don't know.\\n\\nAs for your \"mistaken\" supernatural events why don't you share some since you seem to make mistakes using your judgement occasionally.\"\n",
            "Tokenized: \n",
            " \ttokens: ['\"', '[UNK]', 'the', 'real', 'shut', 'the', 'fuck', 'up', ',', 'because', 'you', 'don', \"'\", 't', 'even', 'know', 'all', 'the', 'details', 'yet', 'you', 'claim', 'to', 'use', 'logic', '.', '[UNK]', 'the', 'fuck', 'you', 'gonna', 'use', 'logic', ',', 'when', 'you', 'don', \"'\", 't', 'even', 'know', 'the', 'details', '.', '[UNK]', 'me', '\\\\', 'x', '##a', '##0', '##w', '##hat', 'city', 'dude', '##s', 'mom', 'father', 'died', 'in', '?', '[UNK]', 'you', 'can', \"'\", 't', ',', 'because', 'you', 'don', \"'\", 't', 'know', '.', '\\\\', 'n', '\\\\', '[UNK]', 'for', 'your', '\"', 'mistaken', '\"', 'supernatural', 'events', 'why', 'don', \"'\", 't', 'you', 'share', 'some', 'since', 'you', 'seem', 'to', 'make', 'mistakes', 'using', 'your', 'judgement', 'occasionally', '.', '\"']\n",
            " \toffsets: [0, 1, 4, 8, 13, 18, 22, 27, 29, 31, 39, 43, 46, 47, 49, 54, 59, 63, 67, 75, 79, 83, 89, 92, 96, 101, 103, 107, 111, 116, 120, 126, 130, 135, 137, 142, 146, 149, 150, 152, 157, 162, 166, 173, 175, 180, 183, 184, 185, 186, 187, 188, 192, 197, 201, 203, 207, 214, 219, 221, 223, 226, 230, 233, 234, 235, 237, 245, 249, 252, 253, 255, 259, 260, 261, 262, 263, 267, 271, 276, 277, 285, 287, 300, 307, 311, 314, 315, 317, 321, 327, 332, 338, 342, 347, 350, 355, 364, 370, 375, 385, 397, 398]\n",
            " \tstart_of_word: [True, False, True, True, True, True, True, True, False, True, True, True, False, False, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, False, True, True, True, False, False, True, True, True, True, False, True, True, True, False, False, False, False, False, True, True, False, True, True, True, True, False, True, True, True, False, False, False, True, True, True, False, False, True, False, False, False, False, False, True, True, True, False, False, True, True, True, True, False, False, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False]\n",
            "Features: \n",
            " \tinput_ids: [101, 1000, 100, 1996, 2613, 3844, 1996, 6616, 2039, 1010, 2138, 2017, 2123, 1005, 1056, 2130, 2113, 2035, 1996, 4751, 2664, 2017, 4366, 2000, 2224, 7961, 1012, 100, 1996, 6616, 2017, 6069, 2224, 7961, 1010, 2043, 2017, 2123, 1005, 1056, 2130, 2113, 1996, 4751, 1012, 100, 2033, 1032, 1060, 2050, 2692, 2860, 12707, 2103, 12043, 2015, 3566, 2269, 2351, 1999, 1029, 100, 2017, 2064, 1005, 1056, 1010, 2138, 2017, 2123, 1005, 1056, 2113, 1012, 1032, 1050, 1032, 100, 2005, 2115, 1000, 13534, 1000, 11189, 2824, 2339, 2123, 1005, 1056, 2017, 3745, 2070, 2144, 2017, 4025, 2000, 2191, 12051, 2478, 2115, 16646, 5681, 1012, 1000, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \ttext_classification_label_ids: [1]\n",
            "_____________________________________________________\n",
            "09/13/2021 16:01:09 - INFO - farm.data_handler.processor -   \n",
            "\n",
            "      .--.        _____                       _      \n",
            "    .'_\\/_'.     / ____|                     | |     \n",
            "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
            "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
            "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
            "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
            "   (/\\||/                             |_|           \n",
            "______\\||/___________________________________________                     \n",
            "\n",
            "ID: 228-0\n",
            "Clear Text: \n",
            " \ttext_classification_label: 0\n",
            " \ttext: \"You ain't a troll bro\"\n",
            "Tokenized: \n",
            " \ttokens: ['\"', '[UNK]', 'ain', \"'\", 't', 'a', 'troll', 'bro', '\"']\n",
            " \toffsets: [0, 1, 5, 8, 9, 11, 13, 19, 22]\n",
            " \tstart_of_word: [True, False, True, False, False, True, True, True, False]\n",
            "Features: \n",
            " \tinput_ids: [101, 1000, 100, 7110, 1005, 1056, 1037, 18792, 22953, 1000, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \ttext_classification_label_ids: [0]\n",
            "_____________________________________________________\n",
            "Preprocessing Dataset ../content/test_with_solutions.tsv: 100%|██████████| 2647/2647 [00:07<00:00, 346.68 Dicts/s]\n",
            "09/13/2021 16:01:14 - INFO - farm.data_handler.data_silo -   Examples in train: 3160\n",
            "09/13/2021 16:01:14 - INFO - farm.data_handler.data_silo -   Examples in dev  : 787\n",
            "09/13/2021 16:01:14 - INFO - farm.data_handler.data_silo -   Examples in test : 2647\n",
            "09/13/2021 16:01:14 - INFO - farm.data_handler.data_silo -   \n",
            "09/13/2021 16:01:14 - INFO - farm.data_handler.data_silo -   Longest sequence length observed after clipping:     128\n",
            "09/13/2021 16:01:15 - INFO - farm.data_handler.data_silo -   Average sequence length after clipping: 43.41107594936709\n",
            "09/13/2021 16:01:15 - INFO - farm.data_handler.data_silo -   Proportion clipped:      0.07183544303797468\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YG8toIFclN2F"
      },
      "source": [
        "### Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cw4CJvz5lRbj"
      },
      "source": [
        "![modeling](https://farm.deepset.ai/_images/adaptive_model_no_bg.jpg)\n",
        "In FARM, we make a strong distinction between the language model and prediction head so that you can mix and match different building blocks for your needs.\n",
        "\n",
        "For example, in the transfer learning paradigm, you might have the one language model that you will be using for both document classification and NER. Or perhaps you have a pretrained language model which you would like to adapt to your domain, then use it for a downstream task such as question answering. \n",
        "\n",
        "All this is possible within FARM and requires only the replacement of a few modular components, as we shall see below.\n",
        "\n",
        "Let's first have a look at how we might set up a model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K4TaqNaTlVmm",
        "outputId": "651e1002-ffa8-4821-fd9d-dc934b45fbfe"
      },
      "source": [
        "# The language model is the foundation on which modern NLP systems are built.\n",
        "# They encapsulate a general understanding of sentence semantics\n",
        "# and are not specific to any one task.\n",
        "\n",
        "# Here we are using Google's BERT model as implemented by HuggingFace. \n",
        "# The model being loaded is a German model that we trained. \n",
        "# You can also change the MODEL_NAME_OR_PATH to point to a BERT model that you\n",
        "# have saved or download one connected to the HuggingFace repository.\n",
        "# See farm.modeling.language_model.PRETRAINED_MODEL_ARCHIVE_MAP for a list of\n",
        "# available models\n",
        "\n",
        "MODEL_NAME_OR_PATH = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "# MODEL_NAME_OR_PATH = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "language_model = LanguageModel.load(MODEL_NAME_OR_PATH)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "09/13/2021 16:01:17 - INFO - farm.modeling.language_model -   Automatically detected language from language model name: english\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bKpqm6solWce",
        "outputId": "fda8360a-8ba0-415f-d8fe-fc605ae411e6"
      },
      "source": [
        "# A prediction head is a model that processes the output of the language model\n",
        "# for a specific task.\n",
        "# Prediction heads will look different depending on whether you're doing text classification\n",
        "# Named Entity Recognition (NER), question answering or some other task.\n",
        "# They should generate logits over the available prediction classes and contain methods\n",
        "# to convert these logits to losses or predictions \n",
        "\n",
        "# Here we use TextClassificationHead which receives a single fixed length sentence vector\n",
        "# and processes it using a feed forward neural network. layer_dims is a list of dimensions:\n",
        "# [input_dims, hidden_1_dims, hidden_2_dims ..., output_dims]\n",
        "\n",
        "# Here by default we have a single layer network.\n",
        "# It takes in a vector of length 768 (the default size of BERT's output).\n",
        "# It outputs a vector of length 2 (the number of classes in the GermEval18 (coarse) dataset)\n",
        "\n",
        "prediction_head = TextClassificationHead(num_labels=len(LABEL_LIST), class_weights=data_silo.calculate_class_weights(task_name=\"text_classification\"))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "09/13/2021 16:01:17 - INFO - farm.modeling.prediction_head -   Prediction head initialized with size [768, 2]\n",
            "09/13/2021 16:01:17 - INFO - farm.modeling.prediction_head -   Using class weights for task 'text_classification': [0.6820535 1.8732227]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQL1HGo2lZEo"
      },
      "source": [
        "# The language model and prediction head are coupled together in the Adaptive Model.\n",
        "# This class takes care of model saving and loading and also coordinates\n",
        "# cases where there is more than one prediction head.\n",
        "\n",
        "# EMBEDS_DROPOUT_PROB is the probability that an element of the output vector from the\n",
        "# language model will be set to zero.\n",
        "# EMBEDS_DROPOUT_PROB = 0.1 distilbert-base-uncased-finetuned-sst-2-english_1.PNG\n",
        "# EMBEDS_DROPOUT_PROB = 0.01\n",
        "# EMBEDS_DROPOUT_PROB = 0.2 distilbert-base-uncased-finetuned-sst-2-english_2.PNG\n",
        "EMBEDS_DROPOUT_PROB = 0.01\n",
        "\n",
        "model = AdaptiveModel(\n",
        "    language_model=language_model,\n",
        "    prediction_heads=[prediction_head],\n",
        "    embeds_dropout_prob=EMBEDS_DROPOUT_PROB,\n",
        "    lm_output_types=[\"per_sequence\"],\n",
        "    device=device)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UE861jLalax1"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4db05tC7lcKy",
        "outputId": "af381f68-741a-43ea-bd1e-d034434308f4"
      },
      "source": [
        "# Here we initialize a Bert Adam optimizer that has a linear warmup and warmdown\n",
        "# Here you can set learning rate, the warmup proportion and number of epochs to train for\n",
        "\n",
        "#LEARNING_RATE = 2e-5 # distilbert-base-uncased-finetuned-sst-2-english_1.PNG\n",
        "#LEARNING_RATE = 1e-7\n",
        "LEARNING_RATE = 1e-5  # distilbert-base-uncased-finetuned-sst-2-english_2.PNG\n",
        "\n",
        "N_EPOCHS = 3\n",
        "\n",
        "model, optimizer, lr_schedule = initialize_optimizer(\n",
        "    model=model,\n",
        "    device=device,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    n_batches=len(data_silo.loaders[\"train\"]),\n",
        "    n_epochs=N_EPOCHS)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "09/13/2021 16:01:19 - INFO - farm.modeling.optimization -   Loading optimizer `TransformersAdamW`: '{'correct_bias': False, 'weight_decay': 0.01, 'lr': 1e-05}'\n",
            "09/13/2021 16:01:19 - INFO - farm.modeling.optimization -   Using scheduler 'get_linear_schedule_with_warmup'\n",
            "09/13/2021 16:01:19 - INFO - farm.modeling.optimization -   Loading schedule `get_linear_schedule_with_warmup`: '{'num_warmup_steps': 29.700000000000003, 'num_training_steps': 297}'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpkluUz3lfJn"
      },
      "source": [
        "# Training loop handled by this\n",
        "# It will also trigger evaluation during training using the dev data\n",
        "# and after training using the test data.\n",
        "\n",
        "# Set N_GPU to a positive value if CUDA is available\n",
        "N_GPU = 1\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    optimizer=optimizer,\n",
        "    data_silo=data_silo,\n",
        "    epochs=N_EPOCHS,\n",
        "    n_gpu=N_GPU,\n",
        "    lr_schedule=lr_schedule,\n",
        "    device=device, \n",
        ")"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KWLMesyre1FB",
        "outputId": "d65376f4-a804-4fd9-a9ce-c74373ad37ef"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Sep 13 16:01:20 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.63.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   40C    P0    60W / 149W |    628MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VuSTk5zAlc0A",
        "outputId": "b7c61cd7-e308-4b41-a8b1-5f6e0c545dff"
      },
      "source": [
        "\n",
        "model = trainer.train()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "09/13/2021 16:01:20 - INFO - farm.train -   \n",
            " \n",
            "\n",
            "          &&& &&  & &&             _____                   _             \n",
            "      && &\\/&\\|& ()|/ @, &&       / ____|                 (_)            \n",
            "      &\\/(/&/&||/& /_/)_&/_&     | |  __ _ __ _____      ___ _ __   __ _ \n",
            "   &() &\\/&|()|/&\\/ '%\" & ()     | | |_ | '__/ _ \\ \\ /\\ / / | '_ \\ / _` |\n",
            "  &_\\_&&_\\ |& |&&/&__%_/_& &&    | |__| | | | (_) \\ V  V /| | | | | (_| |\n",
            "&&   && & &| &| /& & % ()& /&&    \\_____|_|  \\___/ \\_/\\_/ |_|_| |_|\\__, |\n",
            " ()&_---()&\\&\\|&&-&&--%---()~                                       __/ |\n",
            "     &&     \\|||                                                   |___/\n",
            "             |||\n",
            "             |||\n",
            "             |||\n",
            "       , -=-~  .-^- _\n",
            "              `\n",
            "\n",
            "Train epoch 0/2 (Cur. train loss: 0.4484): 100%|██████████| 99/99 [01:02<00:00,  1.57it/s]\n",
            "Train epoch 1/2 (Cur. train loss: 0.3446):   1%|          | 1/99 [00:00<00:59,  1.66it/s]\n",
            "Evaluating: 100%|██████████| 25/25 [00:05<00:00,  4.57it/s]\n",
            "09/13/2021 16:02:30 - INFO - farm.eval -   \n",
            "\n",
            "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "***************************************************\n",
            "***** EVALUATION | DEV SET | AFTER 100 BATCHES *****\n",
            "***************************************************\n",
            "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "\n",
            "09/13/2021 16:02:30 - INFO - farm.eval -   \n",
            " _________ text_classification _________\n",
            "09/13/2021 16:02:30 - INFO - farm.eval -   loss: 0.40247450759480535\n",
            "09/13/2021 16:02:30 - INFO - farm.eval -   task_name: text_classification\n",
            "09/13/2021 16:02:30 - INFO - farm.eval -   f1_macro: 0.7751132017408889\n",
            "09/13/2021 16:02:30 - INFO - farm.eval -   report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9455    0.7762    0.8526       581\n",
            "           1     0.5806    0.8738    0.6977       206\n",
            "\n",
            "    accuracy                         0.8018       787\n",
            "   macro avg     0.7631    0.8250    0.7751       787\n",
            "weighted avg     0.8500    0.8018    0.8120       787\n",
            "\n",
            "Train epoch 1/2 (Cur. train loss: 0.2362): 100%|██████████| 99/99 [01:07<00:00,  1.46it/s]\n",
            "Train epoch 2/2 (Cur. train loss: 0.0655):   2%|▏         | 2/99 [00:01<00:59,  1.63it/s]\n",
            "Evaluating: 100%|██████████| 25/25 [00:05<00:00,  4.57it/s]\n",
            "09/13/2021 16:03:38 - INFO - farm.eval -   \n",
            "\n",
            "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "***************************************************\n",
            "***** EVALUATION | DEV SET | AFTER 200 BATCHES *****\n",
            "***************************************************\n",
            "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "\n",
            "09/13/2021 16:03:38 - INFO - farm.eval -   \n",
            " _________ text_classification _________\n",
            "09/13/2021 16:03:38 - INFO - farm.eval -   loss: 0.39976746972361543\n",
            "09/13/2021 16:03:38 - INFO - farm.eval -   task_name: text_classification\n",
            "09/13/2021 16:03:38 - INFO - farm.eval -   f1_macro: 0.8270586488492948\n",
            "09/13/2021 16:03:38 - INFO - farm.eval -   report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9320    0.8726    0.9013       581\n",
            "           1     0.6955    0.8204    0.7528       206\n",
            "\n",
            "    accuracy                         0.8590       787\n",
            "   macro avg     0.8137    0.8465    0.8271       787\n",
            "weighted avg     0.8701    0.8590    0.8625       787\n",
            "\n",
            "Train epoch 2/2 (Cur. train loss: 0.0601): 100%|██████████| 99/99 [01:07<00:00,  1.46it/s]\n",
            "Evaluating: 100%|██████████| 83/83 [00:18<00:00,  4.52it/s]\n",
            "09/13/2021 16:04:57 - INFO - farm.eval -   \n",
            "\n",
            "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "***************************************************\n",
            "***** EVALUATION | TEST SET | AFTER 297 BATCHES *****\n",
            "***************************************************\n",
            "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "\n",
            "09/13/2021 16:04:57 - INFO - farm.eval -   \n",
            " _________ text_classification _________\n",
            "09/13/2021 16:04:57 - INFO - farm.eval -   loss: 0.5199426451763388\n",
            "09/13/2021 16:04:57 - INFO - farm.eval -   task_name: text_classification\n",
            "09/13/2021 16:04:57 - INFO - farm.eval -   f1_macro: 0.8129685637946056\n",
            "09/13/2021 16:04:57 - INFO - farm.eval -   report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9070    0.8930    0.8999      1954\n",
            "           1     0.7109    0.7417    0.7260       693\n",
            "\n",
            "    accuracy                         0.8534      2647\n",
            "   macro avg     0.8089    0.8174    0.8130      2647\n",
            "weighted avg     0.8556    0.8534    0.8544      2647\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0r8b3etug_F4",
        "outputId": "06f956ba-47f8-4cdb-bfc0-fa3354129c07"
      },
      "source": [
        "# Test your model on a sample (Inference)\n",
        "from farm.infer import Inferencer\n",
        "from pprint import PrettyPrinter\n",
        "\n",
        "infer_model = Inferencer(processor=processor, model=model, task_type=\"text_classification\", gpu=True)\n",
        "\n",
        "basic_texts = [\n",
        "    {\"text\": \"Martin is an idiot\"},\n",
        "    {\"text\": \"Martin Müller plays Voleyball in Berlin\"},\n",
        "]\n",
        "result = infer_model.inference_from_dicts(dicts=basic_texts)\n",
        "PrettyPrinter().pprint(result)\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "09/13/2021 16:04:57 - INFO - farm.utils -   device: cuda n_gpu: 1, distributed training: False, automatic mixed precision training: None\n",
            "09/13/2021 16:04:57 - INFO - farm.infer -   Got ya 1 parallel workers to do inference ...\n",
            "09/13/2021 16:04:57 - INFO - farm.infer -    0 \n",
            "09/13/2021 16:04:57 - INFO - farm.infer -   /w\\\n",
            "09/13/2021 16:04:57 - INFO - farm.infer -   /'\\\n",
            "09/13/2021 16:04:57 - INFO - farm.infer -   \n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils.py:460: FutureWarning: `is_pretokenized` is deprecated and will be removed in a future version, use `is_split_into_words` instead.\n",
            "  FutureWarning,\n",
            "09/13/2021 16:04:57 - INFO - farm.data_handler.processor -   *** Show 2 random examples ***\n",
            "09/13/2021 16:04:57 - INFO - farm.data_handler.processor -   \n",
            "\n",
            "      .--.        _____                       _      \n",
            "    .'_\\/_'.     / ____|                     | |     \n",
            "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
            "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
            "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
            "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
            "   (/\\||/                             |_|           \n",
            "______\\||/___________________________________________                     \n",
            "\n",
            "ID: 1-0\n",
            "Clear Text: \n",
            " \ttext: Martin Müller plays Voleyball in Berlin\n",
            "Tokenized: \n",
            " \ttokens: ['[UNK]', '[UNK]', 'plays', '[UNK]', 'in', '[UNK]']\n",
            " \toffsets: [0, 7, 14, 20, 30, 33]\n",
            " \tstart_of_word: [True, True, True, True, True, True]\n",
            "Features: \n",
            " \tinput_ids: [101, 100, 100, 3248, 100, 1999, 100, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "_____________________________________________________\n",
            "09/13/2021 16:04:57 - INFO - farm.data_handler.processor -   \n",
            "\n",
            "      .--.        _____                       _      \n",
            "    .'_\\/_'.     / ____|                     | |     \n",
            "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
            "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
            "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
            "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
            "   (/\\||/                             |_|           \n",
            "______\\||/___________________________________________                     \n",
            "\n",
            "ID: 0-0\n",
            "Clear Text: \n",
            " \ttext: Martin is an idiot\n",
            "Tokenized: \n",
            " \ttokens: ['[UNK]', 'is', 'an', 'idiot']\n",
            " \toffsets: [0, 7, 10, 13]\n",
            " \tstart_of_word: [True, True, True, True]\n",
            "Features: \n",
            " \tinput_ids: [101, 100, 2003, 2019, 10041, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tpadding_mask: [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "_____________________________________________________\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00, 33.38 Batches/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'predictions': [{'context': 'Martin is an idiot',\n",
            "                   'end': None,\n",
            "                   'label': '1',\n",
            "                   'probability': 0.9937976,\n",
            "                   'start': None},\n",
            "                  {'context': 'Martin Müller plays Voleyball in Berlin',\n",
            "                   'end': None,\n",
            "                   'label': '0',\n",
            "                   'probability': 0.89760894,\n",
            "                   'start': None}],\n",
            "  'task': 'text_classification'}]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KiT5IEsV1BhU"
      },
      "source": [
        "#model.save('../data/detecting-insults-in-social-commentary/model')\n",
        "#processor.save('../data/detecting-insults-in-social-commentary/processor')"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGvyT2i5WX-1"
      },
      "source": [
        "## Aufgabe\n",
        "Verwenden Sie den imdb-Datensatz aus der letzten Sitzung und nutzen Sie ein englisches BERT-Modell (z.B. ```bert-base-uncased```), um es für die Sentiment-Analyse von Film-Reviews finezutunen. Bilden Sie Gruppen und gehen Sie wie folgt vor:\n",
        "\n",
        "1. Verwenden Sie ein Subset des Datensatzes. Grund: Es ist unklar, wie viel die (kostenlose) Google Colab-GPU zu leisten im Stande ist. Verwenden Sie innerhalb der Gruppe unterschiedliche Samplezahlen des Datensatzes und testen Sie aus, wo die Grenzen liegen. \n",
        "2. Adjustieren Sie verschiedene Parameter (Learning Rate, Dropout Rate ...), um Unterschiede in der Performance festzustellen.\n",
        "3. Speichern Sie das fine-getunte BERT-Modell ab: \n",
        "```python\n",
        "model.save('path/to/directory')\n",
        "processor.save('path/to/directory')\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFXvXYB4WkNw"
      },
      "source": [
        "## BERT <br>\n",
        "https://huggingface.co/bert-base-uncased "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3k-z8wSLWlBx",
        "outputId": "7e0fa931-a4ab-4e37-d4f1-08e76f1333eb"
      },
      "source": [
        "from transformers import AutoTokenizer\n",
        "import torch\n",
        "from farm.modeling.tokenization import Tokenizer\n",
        "from farm.data_handler.processor import TextClassificationProcessor\n",
        "from farm.data_handler.data_silo import DataSilo\n",
        "from farm.modeling.language_model import LanguageModel\n",
        "from farm.modeling.prediction_head import TextClassificationHead\n",
        "from farm.modeling.adaptive_model import AdaptiveModel\n",
        "from farm.modeling.optimization import initialize_optimizer\n",
        "from farm.train import Trainer\n",
        "from farm.utils import MLFlowLogger\n",
        "\n",
        "# Here we initialize a tokenizer that will be used for preprocessing text\n",
        "# This is the BERT Tokenizer which uses the byte pair encoding method.\n",
        "# It is currently loaded with a German model\n",
        "\n",
        "#tokenizer = Tokenizer.load(\n",
        "#    pretrained_model_name_or_path=\"finiteautomata/bertweet-base-sentiment-analysis\",\n",
        "#    do_lower_case=False)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# In order to prepare the data for the model, we need a set of\n",
        "# functions to transform data files into PyTorch Datasets.\n",
        "# We group these together in Processor objects.\n",
        "# We will need a new Processor object for each new source of data.\n",
        "# The abstract class can be found in farm.data_handling.processor.Processor\n",
        "# TOXIC = 1\n",
        "# OTHER = 0\n",
        "LABEL_LIST = [\"0\", \"1\"]\n",
        "processor = TextClassificationProcessor(tokenizer=tokenizer,\n",
        "                                        max_seq_len=128,\n",
        "                                        warmup = 600,\n",
        "                                        train_filename=\"train.tsv\",\n",
        "                                        test_filename=\"test_with_solutions.tsv\",\n",
        "                                        data_dir=\"../content\",\n",
        "                                        label_list=LABEL_LIST,\n",
        "                                        metric=\"f1_macro\",\n",
        "                                        text_column_name=\"Comment\",\n",
        "                                        label_column_name=\"Insult\")\n",
        "\n",
        "# We need a DataSilo in order to keep our train, dev and test sets separate.\n",
        "# The DataSilo will call the functions in the Processor to generate these sets.\n",
        "# From the DataSilo, we can fetch a PyTorch DataLoader object which will\n",
        "# be passed on to the model.\n",
        "# Here is a good place to define a batch size for the model\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "data_silo = DataSilo(\n",
        "    processor=processor,\n",
        "    batch_size=BATCH_SIZE)\n",
        "\n",
        "# The language model is the foundation on which modern NLP systems are built.\n",
        "# They encapsulate a general understanding of sentence semantics\n",
        "# and are not specific to any one task.\n",
        "\n",
        "# Here we are using Google's BERT model as implemented by HuggingFace. \n",
        "# The model being loaded is a German model that we trained. \n",
        "# You can also change the MODEL_NAME_OR_PATH to point to a BERT model that you\n",
        "# have saved or download one connected to the HuggingFace repository.\n",
        "# See farm.modeling.language_model.PRETRAINED_MODEL_ARCHIVE_MAP for a list of\n",
        "# available models\n",
        "\n",
        "MODEL_NAME_OR_PATH = \"bert-base-uncased\"\n",
        "# MODEL_NAME_OR_PATH = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "language_model = LanguageModel.load(MODEL_NAME_OR_PATH)\n",
        "\n",
        "# A prediction head is a model that processes the output of the language model\n",
        "# for a specific task.\n",
        "# Prediction heads will look different depending on whether you're doing text classification\n",
        "# Named Entity Recognition (NER), question answering or some other task.\n",
        "# They should generate logits over the available prediction classes and contain methods\n",
        "# to convert these logits to losses or predictions \n",
        "\n",
        "# Here we use TextClassificationHead which receives a single fixed length sentence vector\n",
        "# and processes it using a feed forward neural network. layer_dims is a list of dimensions:\n",
        "# [input_dims, hidden_1_dims, hidden_2_dims ..., output_dims]\n",
        "\n",
        "# Here by default we have a single layer network.\n",
        "# It takes in a vector of length 768 (the default size of BERT's output).\n",
        "# It outputs a vector of length 2 (the number of classes in the GermEval18 (coarse) dataset)\n",
        "\n",
        "prediction_head = TextClassificationHead(num_labels=len(LABEL_LIST), class_weights=data_silo.calculate_class_weights(task_name=\"text_classification\"))\n",
        "\n",
        "# The language model and prediction head are coupled together in the Adaptive Model.\n",
        "# This class takes care of model saving and loading and also coordinates\n",
        "# cases where there is more than one prediction head.\n",
        "\n",
        "# EMBEDS_DROPOUT_PROB is the probability that an element of the output vector from the\n",
        "# language model will be set to zero.\n",
        "# EMBEDS_DROPOUT_PROB = 0.1 distilbert-base-uncased-finetuned-sst-2-english_1.PNG\n",
        "# EMBEDS_DROPOUT_PROB = 0.01\n",
        "# EMBEDS_DROPOUT_PROB = 0.2 distilbert-base-uncased-finetuned-sst-2-english_2.PNG\n",
        "EMBEDS_DROPOUT_PROB = 0.01\n",
        "\n",
        "model = AdaptiveModel(\n",
        "    language_model=language_model,\n",
        "    prediction_heads=[prediction_head],\n",
        "    embeds_dropout_prob=EMBEDS_DROPOUT_PROB,\n",
        "    lm_output_types=[\"per_sequence\"],\n",
        "    device=device)\n",
        "\n",
        "# Here we initialize a Bert Adam optimizer that has a linear warmup and warmdown\n",
        "# Here you can set learning rate, the warmup proportion and number of epochs to train for\n",
        "\n",
        "#LEARNING_RATE = 2e-5 # distilbert-base-uncased-finetuned-sst-2-english_1.PNG\n",
        "#LEARNING_RATE = 1e-7\n",
        "LEARNING_RATE = 1e-5  # distilbert-base-uncased-finetuned-sst-2-english_2.PNG\n",
        "\n",
        "N_EPOCHS = 3\n",
        "\n",
        "model, optimizer, lr_schedule = initialize_optimizer(\n",
        "    model=model,\n",
        "    device=device,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    n_batches=len(data_silo.loaders[\"train\"]),\n",
        "    n_epochs=N_EPOCHS)\n",
        "\n",
        "# Training loop handled by this\n",
        "# It will also trigger evaluation during training using the dev data\n",
        "# and after training using the test data.\n",
        "\n",
        "# Set N_GPU to a positive value if CUDA is available\n",
        "N_GPU = 1\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    optimizer=optimizer,\n",
        "    data_silo=data_silo,\n",
        "    epochs=N_EPOCHS,\n",
        "    n_gpu=N_GPU,\n",
        "    lr_schedule=lr_schedule,\n",
        "    device=device, \n",
        ")\n",
        "\n",
        "!nvidia-smi\n",
        "\n",
        "model = trainer.train()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "09/13/2021 16:04:59 - WARNING - farm.utils -   Failed to log params: INVALID_PARAMETER_VALUE: Changing param value is not allowed. Param with key='tokenizer' was already logged with value='DistilBertTokenizer' for run ID='6543ccfdbba146ca82a34b6de1bbb983. Attempted logging new value 'BertTokenizer'.\n",
            "09/13/2021 16:04:59 - INFO - farm.data_handler.data_silo -   \n",
            "Loading data into the data silo ... \n",
            "              ______\n",
            "               |o  |   !\n",
            "   __          |:`_|---'-.\n",
            "  |__|______.-/ _ \\-----.|       \n",
            " (o)(o)------'\\ _ /     ( )      \n",
            " \n",
            "09/13/2021 16:04:59 - INFO - farm.data_handler.data_silo -   Loading train set from: ../content/train.tsv \n",
            "09/13/2021 16:04:59 - INFO - farm.data_handler.data_silo -   Got ya 1 parallel workers to convert 3947 dictionaries to pytorch datasets (chunksize = 790)...\n",
            "09/13/2021 16:04:59 - INFO - farm.data_handler.data_silo -    0 \n",
            "09/13/2021 16:04:59 - INFO - farm.data_handler.data_silo -   /w\\\n",
            "09/13/2021 16:04:59 - INFO - farm.data_handler.data_silo -   / \\\n",
            "09/13/2021 16:04:59 - INFO - farm.data_handler.data_silo -   \n",
            "Preprocessing Dataset ../content/train.tsv:   0%|          | 0/3947 [00:00<?, ? Dicts/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils.py:460: FutureWarning: `is_pretokenized` is deprecated and will be removed in a future version, use `is_split_into_words` instead.\n",
            "  FutureWarning,\n",
            "09/13/2021 16:05:01 - INFO - farm.data_handler.processor -   *** Show 2 random examples ***\n",
            "09/13/2021 16:05:01 - INFO - farm.data_handler.processor -   \n",
            "\n",
            "      .--.        _____                       _      \n",
            "    .'_\\/_'.     / ____|                     | |     \n",
            "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
            "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
            "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
            "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
            "   (/\\||/                             |_|           \n",
            "______\\||/___________________________________________                     \n",
            "\n",
            "ID: 165-0\n",
            "Clear Text: \n",
            " \ttext_classification_label: 1\n",
            " \ttext: \"CLOSE YOUR MOUTH CHIMP\"\n",
            "Tokenized: \n",
            " \ttokens: ['\"', 'close', 'your', 'mouth', 'chi', '##mp', '\"']\n",
            " \toffsets: [0, 1, 7, 12, 18, 21, 23]\n",
            " \tstart_of_word: [True, False, True, True, True, False, False]\n",
            "Features: \n",
            " \tinput_ids: [101, 1000, 2485, 2115, 2677, 9610, 8737, 1000, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \ttext_classification_label_ids: [1]\n",
            "_____________________________________________________\n",
            "09/13/2021 16:05:01 - INFO - farm.data_handler.processor -   \n",
            "\n",
            "      .--.        _____                       _      \n",
            "    .'_\\/_'.     / ____|                     | |     \n",
            "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
            "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
            "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
            "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
            "   (/\\||/                             |_|           \n",
            "______\\||/___________________________________________                     \n",
            "\n",
            "ID: 232-0\n",
            "Clear Text: \n",
            " \ttext_classification_label: 0\n",
            " \ttext: \"Posted farther down in a response...felt it was\\\\xc2\\\\xa0germane to what we are discussing a great deal of today:\\\\n\"The prince of this world\" (John 12:13) \\\\xc2\\\\xa0is a master of false systems. \\\\xc2\\\\xa0He crafts entire schools of thought that can suck us in and destroy us...He\\\\'s behind false religions, false\\\\xc2\\\\xa0philosophies, false doctrine, false morality, and every\\\\xc2\\\\xa0system\\\\xc2\\\\xa0of that that cannot lead anyone to God. \\\\xc2\\\\xa0He has infiltrated governments, economies,\\\\xc2\\\\xa0educational\\\\xc2\\\\xa0institutions, and anything that has influence in this world. \\\\xc2\\\\xa0The conclusion John came to is sobering. \\\\xc2\\\\xa0\"The whole world lies in the power of the evil one\"(John 5:19).\\\\'The Invisible War\\\\', Chip Ingram\"\n",
            "Tokenized: \n",
            " \ttokens: ['\"', 'posted', 'farther', 'down', 'in', 'a', 'response', '.', '.', '.', 'felt', 'it', 'was', '\\\\', '\\\\', 'x', '##c', '##2', '\\\\', '\\\\', 'x', '##a', '##0', '##ger', '##man', '##e', 'to', 'what', 'we', 'are', 'discussing', 'a', 'great', 'deal', 'of', 'today', ':', '\\\\', '\\\\', 'n', '\"', 'the', 'prince', 'of', 'this', 'world', '\"', '(', 'john', '12', ':', '13', ')', '\\\\', '\\\\', 'x', '##c', '##2', '\\\\', '\\\\', 'x', '##a', '##0', '##is', 'a', 'master', 'of', 'false', 'systems', '.', '\\\\', '\\\\', 'x', '##c', '##2', '\\\\', '\\\\', 'x', '##a', '##0', '##he', 'crafts', 'entire', 'schools', 'of', 'thought', 'that', 'can', 'suck', 'us', 'in', 'and', 'destroy', 'us', '.', '.', '.', 'he', '\\\\', '\\\\', \"'\", 's', 'behind', 'false', 'religions', ',', 'false', '\\\\', '\\\\', 'x', '##c', '##2', '\\\\', '\\\\', 'x', '##a', '##0', '##phi', '##los', '##op', '##hi', '##es', ',', 'false', 'doctrine', ',']\n",
            " \toffsets: [0, 1, 8, 16, 21, 24, 26, 34, 35, 36, 37, 42, 45, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 61, 64, 66, 69, 74, 77, 81, 92, 94, 100, 105, 108, 113, 114, 115, 116, 117, 118, 122, 129, 132, 137, 142, 144, 145, 150, 152, 153, 155, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 170, 172, 179, 182, 188, 195, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 210, 217, 224, 232, 235, 243, 248, 252, 257, 260, 263, 267, 275, 277, 278, 279, 280, 282, 283, 284, 285, 287, 294, 300, 309, 311, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 329, 332, 334, 336, 338, 340, 346, 354]\n",
            " \tstart_of_word: [True, False, True, True, True, True, True, False, False, False, False, True, True, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True, True, True, True, True, True, True, True, True, False, False, False, False, False, False, True, True, True, True, False, True, False, True, False, False, False, True, False, False, False, False, False, False, False, False, False, False, True, True, True, True, True, False, True, False, False, False, False, False, False, False, False, False, False, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, False, False, False, False, False, False, True, True, True, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True, False]\n",
            "Features: \n",
            " \tinput_ids: [101, 1000, 6866, 8736, 2091, 1999, 1037, 3433, 1012, 1012, 1012, 2371, 2009, 2001, 1032, 1032, 1060, 2278, 2475, 1032, 1032, 1060, 2050, 2692, 4590, 2386, 2063, 2000, 2054, 2057, 2024, 10537, 1037, 2307, 3066, 1997, 2651, 1024, 1032, 1032, 1050, 1000, 1996, 3159, 1997, 2023, 2088, 1000, 1006, 2198, 2260, 1024, 2410, 1007, 1032, 1032, 1060, 2278, 2475, 1032, 1032, 1060, 2050, 2692, 2483, 1037, 3040, 1997, 6270, 3001, 1012, 1032, 1032, 1060, 2278, 2475, 1032, 1032, 1060, 2050, 2692, 5369, 14030, 2972, 2816, 1997, 2245, 2008, 2064, 11891, 2149, 1999, 1998, 6033, 2149, 1012, 1012, 1012, 2002, 1032, 1032, 1005, 1055, 2369, 6270, 11822, 1010, 6270, 1032, 1032, 1060, 2278, 2475, 1032, 1032, 1060, 2050, 2692, 21850, 10483, 7361, 4048, 2229, 1010, 6270, 8998, 1010, 102]\n",
            " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \ttext_classification_label_ids: [0]\n",
            "_____________________________________________________\n",
            "Preprocessing Dataset ../content/train.tsv: 100%|██████████| 3947/3947 [00:13<00:00, 291.75 Dicts/s]\n",
            "09/13/2021 16:05:12 - INFO - farm.data_handler.data_silo -   Loading dev set as a slice of train set\n",
            "09/13/2021 16:05:12 - INFO - farm.data_handler.data_silo -   Took 787 samples out of train set to create dev set (dev split is roughly 0.1)\n",
            "09/13/2021 16:05:12 - INFO - farm.data_handler.data_silo -   Loading test set from: ../content/test_with_solutions.tsv\n",
            "09/13/2021 16:05:12 - INFO - farm.data_handler.data_silo -   Got ya 1 parallel workers to convert 2647 dictionaries to pytorch datasets (chunksize = 530)...\n",
            "09/13/2021 16:05:12 - INFO - farm.data_handler.data_silo -    0 \n",
            "09/13/2021 16:05:12 - INFO - farm.data_handler.data_silo -   /w\\\n",
            "09/13/2021 16:05:12 - INFO - farm.data_handler.data_silo -   /'\\\n",
            "09/13/2021 16:05:12 - INFO - farm.data_handler.data_silo -   \n",
            "Preprocessing Dataset ../content/test_with_solutions.tsv:   0%|          | 0/2647 [00:00<?, ? Dicts/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils.py:460: FutureWarning: `is_pretokenized` is deprecated and will be removed in a future version, use `is_split_into_words` instead.\n",
            "  FutureWarning,\n",
            "09/13/2021 16:05:16 - INFO - farm.data_handler.processor -   *** Show 2 random examples ***\n",
            "09/13/2021 16:05:16 - INFO - farm.data_handler.processor -   \n",
            "\n",
            "      .--.        _____                       _      \n",
            "    .'_\\/_'.     / ____|                     | |     \n",
            "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
            "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
            "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
            "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
            "   (/\\||/                             |_|           \n",
            "______\\||/___________________________________________                     \n",
            "\n",
            "ID: 156-0\n",
            "Clear Text: \n",
            " \ttext_classification_label: 0\n",
            " \ttext: \"I missed the day when elected you to speak for \"90%\" of us.  Carry on then.\"\n",
            "Tokenized: \n",
            " \ttokens: ['\"', 'i', 'missed', 'the', 'day', 'when', 'elected', 'you', 'to', 'speak', 'for', '\"', '90', '%', '\"', 'of', 'us', '.', 'carry', 'on', 'then', '.', '\"']\n",
            " \toffsets: [0, 1, 3, 10, 14, 18, 23, 31, 35, 38, 44, 48, 49, 51, 52, 54, 57, 59, 62, 68, 71, 75, 76]\n",
            " \tstart_of_word: [True, False, True, True, True, True, True, True, True, True, True, True, False, False, False, True, True, False, True, True, True, False, False]\n",
            "Features: \n",
            " \tinput_ids: [101, 1000, 1045, 4771, 1996, 2154, 2043, 2700, 2017, 2000, 3713, 2005, 1000, 3938, 1003, 1000, 1997, 2149, 1012, 4287, 2006, 2059, 1012, 1000, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \ttext_classification_label_ids: [0]\n",
            "_____________________________________________________\n",
            "09/13/2021 16:05:16 - INFO - farm.data_handler.processor -   \n",
            "\n",
            "      .--.        _____                       _      \n",
            "    .'_\\/_'.     / ____|                     | |     \n",
            "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
            "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
            "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
            "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
            "   (/\\||/                             |_|           \n",
            "______\\||/___________________________________________                     \n",
            "\n",
            "ID: 235-0\n",
            "Clear Text: \n",
            " \ttext_classification_label: 0\n",
            " \ttext: \"@asianqueen  get back in the kitchen and make some madras\"\n",
            "Tokenized: \n",
            " \ttokens: ['\"', '@', 'asian', '##que', '##en', 'get', 'back', 'in', 'the', 'kitchen', 'and', 'make', 'some', 'madras', '\"']\n",
            " \toffsets: [0, 1, 2, 7, 10, 14, 18, 23, 26, 30, 38, 42, 47, 52, 58]\n",
            " \tstart_of_word: [True, False, False, False, False, True, True, True, True, True, True, True, True, True, False]\n",
            "Features: \n",
            " \tinput_ids: [101, 1000, 1030, 4004, 4226, 2368, 2131, 2067, 1999, 1996, 3829, 1998, 2191, 2070, 12993, 1000, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \ttext_classification_label_ids: [0]\n",
            "_____________________________________________________\n",
            "Preprocessing Dataset ../content/test_with_solutions.tsv: 100%|██████████| 2647/2647 [00:09<00:00, 265.40 Dicts/s]\n",
            "09/13/2021 16:05:23 - INFO - farm.data_handler.data_silo -   Examples in train: 3160\n",
            "09/13/2021 16:05:23 - INFO - farm.data_handler.data_silo -   Examples in dev  : 787\n",
            "09/13/2021 16:05:23 - INFO - farm.data_handler.data_silo -   Examples in test : 2647\n",
            "09/13/2021 16:05:23 - INFO - farm.data_handler.data_silo -   \n",
            "09/13/2021 16:05:23 - INFO - farm.data_handler.data_silo -   Longest sequence length observed after clipping:     128\n",
            "09/13/2021 16:05:23 - INFO - farm.data_handler.data_silo -   Average sequence length after clipping: 44.789240506329115\n",
            "09/13/2021 16:05:23 - INFO - farm.data_handler.data_silo -   Proportion clipped:      0.0810126582278481\n",
            "09/13/2021 16:05:23 - WARNING - farm.utils -   Failed to log params: INVALID_PARAMETER_VALUE: Changing param value is not allowed. Param with key='ave_seq_len' was already logged with value='43.41107594936709' for run ID='6543ccfdbba146ca82a34b6de1bbb983. Attempted logging new value '44.789240506329115'.\n",
            "09/13/2021 16:05:27 - WARNING - farm.modeling.language_model -   Could not automatically detect from language model name what language it is. \n",
            "\t We guess it's an *ENGLISH* model ... \n",
            "\t If not: Init the language model by supplying the 'language' param.\n",
            "09/13/2021 16:05:27 - INFO - farm.modeling.prediction_head -   Prediction head initialized with size [768, 2]\n",
            "09/13/2021 16:05:27 - INFO - farm.modeling.prediction_head -   Using class weights for task 'text_classification': [0.67650837 1.9163636 ]\n",
            "09/13/2021 16:05:27 - WARNING - farm.utils -   Failed to log params: INVALID_PARAMETER_VALUE: Changing param value is not allowed. Param with key='lm_type' was already logged with value='DistilBert' for run ID='6543ccfdbba146ca82a34b6de1bbb983. Attempted logging new value 'Bert'.\n",
            "09/13/2021 16:05:27 - INFO - farm.modeling.optimization -   Loading optimizer `TransformersAdamW`: '{'correct_bias': False, 'weight_decay': 0.01, 'lr': 1e-05}'\n",
            "09/13/2021 16:05:27 - INFO - farm.modeling.optimization -   Using scheduler 'get_linear_schedule_with_warmup'\n",
            "09/13/2021 16:05:27 - INFO - farm.modeling.optimization -   Loading schedule `get_linear_schedule_with_warmup`: '{'num_warmup_steps': 29.700000000000003, 'num_training_steps': 297}'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Sep 13 16:05:28 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.63.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   45C    P0    60W / 149W |   3237MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "09/13/2021 16:05:28 - INFO - farm.train -   \n",
            " \n",
            "\n",
            "          &&& &&  & &&             _____                   _             \n",
            "      && &\\/&\\|& ()|/ @, &&       / ____|                 (_)            \n",
            "      &\\/(/&/&||/& /_/)_&/_&     | |  __ _ __ _____      ___ _ __   __ _ \n",
            "   &() &\\/&|()|/&\\/ '%\" & ()     | | |_ | '__/ _ \\ \\ /\\ / / | '_ \\ / _` |\n",
            "  &_\\_&&_\\ |& |&&/&__%_/_& &&    | |__| | | | (_) \\ V  V /| | | | | (_| |\n",
            "&&   && & &| &| /& & % ()& /&&    \\_____|_|  \\___/ \\_/\\_/ |_|_| |_|\\__, |\n",
            " ()&_---()&\\&\\|&&-&&--%---()~                                       __/ |\n",
            "     &&     \\|||                                                   |___/\n",
            "             |||\n",
            "             |||\n",
            "             |||\n",
            "       , -=-~  .-^- _\n",
            "              `\n",
            "\n",
            "Train epoch 0/2 (Cur. train loss: 0.1170): 100%|██████████| 99/99 [02:03<00:00,  1.25s/it]\n",
            "Train epoch 1/2 (Cur. train loss: 0.3749):   1%|          | 1/99 [00:01<02:01,  1.24s/it]\n",
            "Evaluating:   0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|██████████| 25/25 [00:10<00:00,  2.28it/s]\n",
            "09/13/2021 16:07:45 - INFO - farm.eval -   \n",
            "\n",
            "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "***************************************************\n",
            "***** EVALUATION | DEV SET | AFTER 100 BATCHES *****\n",
            "***************************************************\n",
            "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "\n",
            "09/13/2021 16:07:45 - INFO - farm.eval -   \n",
            " _________ text_classification _________\n",
            "09/13/2021 16:07:45 - INFO - farm.eval -   loss: 0.3625577032338861\n",
            "09/13/2021 16:07:45 - INFO - farm.eval -   task_name: text_classification\n",
            "09/13/2021 16:07:45 - INFO - farm.eval -   f1_macro: 0.8362810271855492\n",
            "09/13/2021 16:07:45 - INFO - farm.eval -   report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9481    0.8452    0.8937       562\n",
            "           1     0.6958    0.8844    0.7789       225\n",
            "\n",
            "    accuracy                         0.8564       787\n",
            "   macro avg     0.8220    0.8648    0.8363       787\n",
            "weighted avg     0.8760    0.8564    0.8609       787\n",
            "\n",
            "Train epoch 1/2 (Cur. train loss: 0.3026): 100%|██████████| 99/99 [02:13<00:00,  1.35s/it]\n",
            "Train epoch 2/2 (Cur. train loss: 0.0380):   2%|▏         | 2/99 [00:02<01:58,  1.22s/it]\n",
            "Evaluating:   0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|██████████| 25/25 [00:11<00:00,  2.26it/s]\n",
            "09/13/2021 16:10:00 - INFO - farm.eval -   \n",
            "\n",
            "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "***************************************************\n",
            "***** EVALUATION | DEV SET | AFTER 200 BATCHES *****\n",
            "***************************************************\n",
            "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "\n",
            "09/13/2021 16:10:00 - INFO - farm.eval -   \n",
            " _________ text_classification _________\n",
            "09/13/2021 16:10:00 - INFO - farm.eval -   loss: 0.4552912557594658\n",
            "09/13/2021 16:10:00 - INFO - farm.eval -   task_name: text_classification\n",
            "09/13/2021 16:10:00 - INFO - farm.eval -   f1_macro: 0.8511034647650135\n",
            "09/13/2021 16:10:00 - INFO - farm.eval -   report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8945    0.9502    0.9215       562\n",
            "           1     0.8526    0.7200    0.7807       225\n",
            "\n",
            "    accuracy                         0.8844       787\n",
            "   macro avg     0.8736    0.8351    0.8511       787\n",
            "weighted avg     0.8825    0.8844    0.8812       787\n",
            "\n",
            "Train epoch 2/2 (Cur. train loss: 0.0316): 100%|██████████| 99/99 [02:13<00:00,  1.35s/it]\n",
            "Evaluating: 100%|██████████| 83/83 [00:36<00:00,  2.26it/s]\n",
            "09/13/2021 16:12:35 - INFO - farm.eval -   \n",
            "\n",
            "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "***************************************************\n",
            "***** EVALUATION | TEST SET | AFTER 297 BATCHES *****\n",
            "***************************************************\n",
            "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "\n",
            "09/13/2021 16:12:35 - INFO - farm.eval -   \n",
            " _________ text_classification _________\n",
            "09/13/2021 16:12:36 - INFO - farm.eval -   loss: 0.4463196098691885\n",
            "09/13/2021 16:12:36 - INFO - farm.eval -   task_name: text_classification\n",
            "09/13/2021 16:12:36 - INFO - farm.eval -   f1_macro: 0.8496081281485128\n",
            "09/13/2021 16:12:36 - INFO - farm.eval -   report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9179    0.9268    0.9223      1954\n",
            "           1     0.7878    0.7662    0.7769       693\n",
            "\n",
            "    accuracy                         0.8848      2647\n",
            "   macro avg     0.8529    0.8465    0.8496      2647\n",
            "weighted avg     0.8838    0.8848    0.8843      2647\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nL8B9oqTg4f6"
      },
      "source": [
        "## RoBERTa <br>\n",
        "https://huggingface.co/roberta-base "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0JUahj_Hg6NA",
        "outputId": "61a7ef98-1b8b-43d7-cd46-cccc0f0e6345"
      },
      "source": [
        "from transformers import AutoTokenizer\n",
        "import torch\n",
        "from farm.modeling.tokenization import Tokenizer\n",
        "from farm.data_handler.processor import TextClassificationProcessor\n",
        "from farm.data_handler.data_silo import DataSilo\n",
        "from farm.modeling.language_model import LanguageModel\n",
        "from farm.modeling.prediction_head import TextClassificationHead\n",
        "from farm.modeling.adaptive_model import AdaptiveModel\n",
        "from farm.modeling.optimization import initialize_optimizer\n",
        "from farm.train import Trainer\n",
        "from farm.utils import MLFlowLogger\n",
        "\n",
        "# Here we initialize a tokenizer that will be used for preprocessing text\n",
        "# This is the BERT Tokenizer which uses the byte pair encoding method.\n",
        "# It is currently loaded with a German model\n",
        "\n",
        "#tokenizer = Tokenizer.load(\n",
        "#    pretrained_model_name_or_path=\"finiteautomata/bertweet-base-sentiment-analysis\",\n",
        "#    do_lower_case=False)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
        "\n",
        "# In order to prepare the data for the model, we need a set of\n",
        "# functions to transform data files into PyTorch Datasets.\n",
        "# We group these together in Processor objects.\n",
        "# We will need a new Processor object for each new source of data.\n",
        "# The abstract class can be found in farm.data_handling.processor.Processor\n",
        "# TOXIC = 1\n",
        "# OTHER = 0\n",
        "LABEL_LIST = [\"0\", \"1\"]\n",
        "processor = TextClassificationProcessor(tokenizer=tokenizer,\n",
        "                                        max_seq_len=128,\n",
        "                                        warmup = 600,\n",
        "                                        train_filename=\"train.tsv\",\n",
        "                                        test_filename=\"test_with_solutions.tsv\",\n",
        "                                        data_dir=\"../content\",\n",
        "                                        label_list=LABEL_LIST,\n",
        "                                        metric=\"f1_macro\",\n",
        "                                        text_column_name=\"Comment\",\n",
        "                                        label_column_name=\"Insult\")\n",
        "\n",
        "# We need a DataSilo in order to keep our train, dev and test sets separate.\n",
        "# The DataSilo will call the functions in the Processor to generate these sets.\n",
        "# From the DataSilo, we can fetch a PyTorch DataLoader object which will\n",
        "# be passed on to the model.\n",
        "# Here is a good place to define a batch size for the model\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "data_silo = DataSilo(\n",
        "    processor=processor,\n",
        "    batch_size=BATCH_SIZE)\n",
        "\n",
        "# The language model is the foundation on which modern NLP systems are built.\n",
        "# They encapsulate a general understanding of sentence semantics\n",
        "# and are not specific to any one task.\n",
        "\n",
        "# Here we are using Google's BERT model as implemented by HuggingFace. \n",
        "# The model being loaded is a German model that we trained. \n",
        "# You can also change the MODEL_NAME_OR_PATH to point to a BERT model that you\n",
        "# have saved or download one connected to the HuggingFace repository.\n",
        "# See farm.modeling.language_model.PRETRAINED_MODEL_ARCHIVE_MAP for a list of\n",
        "# available models\n",
        "\n",
        "MODEL_NAME_OR_PATH = \"roberta-base\"\n",
        "# MODEL_NAME_OR_PATH = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "language_model = LanguageModel.load(MODEL_NAME_OR_PATH)\n",
        "\n",
        "# A prediction head is a model that processes the output of the language model\n",
        "# for a specific task.\n",
        "# Prediction heads will look different depending on whether you're doing text classification\n",
        "# Named Entity Recognition (NER), question answering or some other task.\n",
        "# They should generate logits over the available prediction classes and contain methods\n",
        "# to convert these logits to losses or predictions \n",
        "\n",
        "# Here we use TextClassificationHead which receives a single fixed length sentence vector\n",
        "# and processes it using a feed forward neural network. layer_dims is a list of dimensions:\n",
        "# [input_dims, hidden_1_dims, hidden_2_dims ..., output_dims]\n",
        "\n",
        "# Here by default we have a single layer network.\n",
        "# It takes in a vector of length 768 (the default size of BERT's output).\n",
        "# It outputs a vector of length 2 (the number of classes in the GermEval18 (coarse) dataset)\n",
        "\n",
        "prediction_head = TextClassificationHead(num_labels=len(LABEL_LIST), class_weights=data_silo.calculate_class_weights(task_name=\"text_classification\"))\n",
        "\n",
        "# The language model and prediction head are coupled together in the Adaptive Model.\n",
        "# This class takes care of model saving and loading and also coordinates\n",
        "# cases where there is more than one prediction head.\n",
        "\n",
        "# EMBEDS_DROPOUT_PROB is the probability that an element of the output vector from the\n",
        "# language model will be set to zero.\n",
        "# EMBEDS_DROPOUT_PROB = 0.1 distilbert-base-uncased-finetuned-sst-2-english_1.PNG\n",
        "# EMBEDS_DROPOUT_PROB = 0.01\n",
        "# EMBEDS_DROPOUT_PROB = 0.2 distilbert-base-uncased-finetuned-sst-2-english_2.PNG\n",
        "EMBEDS_DROPOUT_PROB = 0.01\n",
        "\n",
        "model = AdaptiveModel(\n",
        "    language_model=language_model,\n",
        "    prediction_heads=[prediction_head],\n",
        "    embeds_dropout_prob=EMBEDS_DROPOUT_PROB,\n",
        "    lm_output_types=[\"per_sequence\"],\n",
        "    device=device)\n",
        "\n",
        "# Here we initialize a Bert Adam optimizer that has a linear warmup and warmdown\n",
        "# Here you can set learning rate, the warmup proportion and number of epochs to train for\n",
        "\n",
        "#LEARNING_RATE = 2e-5 # distilbert-base-uncased-finetuned-sst-2-english_1.PNG\n",
        "#LEARNING_RATE = 1e-7\n",
        "LEARNING_RATE = 1e-5  # distilbert-base-uncased-finetuned-sst-2-english_2.PNG\n",
        "\n",
        "N_EPOCHS = 3\n",
        "\n",
        "model, optimizer, lr_schedule = initialize_optimizer(\n",
        "    model=model,\n",
        "    device=device,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    n_batches=len(data_silo.loaders[\"train\"]),\n",
        "    n_epochs=N_EPOCHS)\n",
        "\n",
        "# Training loop handled by this\n",
        "# It will also trigger evaluation during training using the dev data\n",
        "# and after training using the test data.\n",
        "\n",
        "# Set N_GPU to a positive value if CUDA is available\n",
        "N_GPU = 1\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    optimizer=optimizer,\n",
        "    data_silo=data_silo,\n",
        "    epochs=N_EPOCHS,\n",
        "    n_gpu=N_GPU,\n",
        "    lr_schedule=lr_schedule,\n",
        "    device=device, \n",
        ")\n",
        "\n",
        "!nvidia-smi\n",
        "\n",
        "model = trainer.train()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "09/13/2021 16:12:37 - WARNING - farm.utils -   Failed to log params: INVALID_PARAMETER_VALUE: Changing param value is not allowed. Param with key='tokenizer' was already logged with value='DistilBertTokenizer' for run ID='6543ccfdbba146ca82a34b6de1bbb983. Attempted logging new value 'RobertaTokenizer'.\n",
            "09/13/2021 16:12:37 - INFO - farm.data_handler.data_silo -   \n",
            "Loading data into the data silo ... \n",
            "              ______\n",
            "               |o  |   !\n",
            "   __          |:`_|---'-.\n",
            "  |__|______.-/ _ \\-----.|       \n",
            " (o)(o)------'\\ _ /     ( )      \n",
            " \n",
            "09/13/2021 16:12:37 - INFO - farm.data_handler.data_silo -   Loading train set from: ../content/train.tsv \n",
            "09/13/2021 16:12:37 - INFO - farm.data_handler.data_silo -   Got ya 1 parallel workers to convert 3947 dictionaries to pytorch datasets (chunksize = 790)...\n",
            "09/13/2021 16:12:37 - INFO - farm.data_handler.data_silo -    0 \n",
            "09/13/2021 16:12:37 - INFO - farm.data_handler.data_silo -   /w\\\n",
            "09/13/2021 16:12:37 - INFO - farm.data_handler.data_silo -   /'\\\n",
            "09/13/2021 16:12:37 - INFO - farm.data_handler.data_silo -   \n",
            "Preprocessing Dataset ../content/train.tsv:   0%|          | 0/3947 [00:00<?, ? Dicts/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils.py:460: FutureWarning: `is_pretokenized` is deprecated and will be removed in a future version, use `is_split_into_words` instead.\n",
            "  FutureWarning,\n",
            "09/13/2021 16:12:41 - INFO - farm.data_handler.processor -   *** Show 2 random examples ***\n",
            "09/13/2021 16:12:41 - INFO - farm.data_handler.processor -   \n",
            "\n",
            "      .--.        _____                       _      \n",
            "    .'_\\/_'.     / ____|                     | |     \n",
            "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
            "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
            "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
            "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
            "   (/\\||/                             |_|           \n",
            "______\\||/___________________________________________                     \n",
            "\n",
            "ID: 459-0\n",
            "Clear Text: \n",
            " \ttext_classification_label: 0\n",
            " \ttext: \"YOU HAVE A BEAUTIFUL BODY. What's wrong with a man looking at a woman with a beautiful body ESPECIALLY NICE HIPS? If I'm stalker for that then every dude I know is a stalker as well.\"\n",
            "Tokenized: \n",
            " \ttokens: ['\"', 'YOU', 'ĠHAVE', 'ĠA', 'ĠBE', 'AUT', 'IF', 'UL', 'ĠB', 'ODY', '.', 'ĠWhat', \"'s\", 'Ġwrong', 'Ġwith', 'Ġa', 'Ġman', 'Ġlooking', 'Ġat', 'Ġa', 'Ġwoman', 'Ġwith', 'Ġa', 'Ġbeautiful', 'Ġbody', 'ĠESP', 'EC', 'I', 'ALLY', 'ĠN', 'ICE', 'ĠHI', 'PS', '?', 'ĠIf', 'ĠI', \"'m\", 'Ġst', 'alker', 'Ġfor', 'Ġthat', 'Ġthen', 'Ġevery', 'Ġdude', 'ĠI', 'Ġknow', 'Ġis', 'Ġa', 'Ġst', 'alker', 'Ġas', 'Ġwell', '.\"']\n",
            " \toffsets: [0, 1, 5, 10, 12, 14, 17, 19, 22, 23, 26, 28, 32, 35, 41, 46, 48, 52, 60, 63, 65, 71, 76, 78, 88, 93, 96, 98, 99, 104, 105, 109, 111, 113, 115, 118, 119, 122, 124, 130, 134, 139, 144, 150, 155, 157, 162, 165, 167, 169, 175, 178, 182]\n",
            " \tstart_of_word: [True, False, True, True, True, False, False, False, True, False, False, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, False, True, False, True, False, False, True, True, False, True, False, True, True, True, True, True, True, True, True, True, True, False, True, True, False]\n",
            "Features: \n",
            " \tinput_ids: [0, 113, 38158, 27789, 83, 6362, 42575, 7025, 6597, 163, 37588, 4, 653, 18, 1593, 19, 10, 313, 546, 23, 10, 693, 19, 10, 2721, 809, 34652, 3586, 100, 32191, 234, 9292, 35393, 3888, 116, 318, 38, 437, 1690, 38198, 13, 14, 172, 358, 22633, 38, 216, 16, 10, 1690, 38198, 25, 157, 72, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \ttext_classification_label_ids: [0]\n",
            "_____________________________________________________\n",
            "09/13/2021 16:12:41 - INFO - farm.data_handler.processor -   \n",
            "\n",
            "      .--.        _____                       _      \n",
            "    .'_\\/_'.     / ____|                     | |     \n",
            "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
            "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
            "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
            "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
            "   (/\\||/                             |_|           \n",
            "______\\||/___________________________________________                     \n",
            "\n",
            "ID: 423-0\n",
            "Clear Text: \n",
            " \ttext_classification_label: 0\n",
            " \ttext: \"school lunches suck the big one and are the most unhealthy choices available... who do they think they are to try to tell anyone they have healthier lunches at school??? our school lunches are so ridiculously high in fat and sodium... not to mention processed foods are the majority of what they serve. I would never let my child eat school provided lunches unless they did a major overhaul! \"\n",
            "Tokenized: \n",
            " \ttokens: ['\"', 'school', 'Ġlun', 'ches', 'Ġsuck', 'Ġthe', 'Ġbig', 'Ġone', 'Ġand', 'Ġare', 'Ġthe', 'Ġmost', 'Ġunhealthy', 'Ġchoices', 'Ġavailable', '...', 'Ġwho', 'Ġdo', 'Ġthey', 'Ġthink', 'Ġthey', 'Ġare', 'Ġto', 'Ġtry', 'Ġto', 'Ġtell', 'Ġanyone', 'Ġthey', 'Ġhave', 'Ġhealthier', 'Ġlun', 'ches', 'Ġat', 'Ġschool', '???', 'Ġour', 'Ġschool', 'Ġlun', 'ches', 'Ġare', 'Ġso', 'Ġridiculously', 'Ġhigh', 'Ġin', 'Ġfat', 'Ġand', 'Ġsodium', '...', 'Ġnot', 'Ġto', 'Ġmention', 'Ġprocessed', 'Ġfoods', 'Ġare', 'Ġthe', 'Ġmajority', 'Ġof', 'Ġwhat', 'Ġthey', 'Ġserve', '.', 'ĠI', 'Ġwould', 'Ġnever', 'Ġlet', 'Ġmy', 'Ġchild', 'Ġeat', 'Ġschool', 'Ġprovided', 'Ġlun', 'ches', 'Ġunless', 'Ġthey', 'Ġdid', 'Ġa', 'Ġmajor', 'Ġoverhaul', '!', 'Ġ\"']\n",
            " \toffsets: [0, 1, 8, 11, 16, 21, 25, 29, 33, 37, 41, 45, 50, 60, 68, 77, 81, 85, 88, 93, 99, 104, 108, 111, 115, 118, 123, 130, 135, 140, 150, 153, 158, 161, 167, 171, 175, 182, 185, 190, 194, 197, 210, 215, 218, 222, 226, 232, 236, 240, 243, 251, 261, 267, 271, 275, 284, 287, 292, 297, 302, 304, 306, 312, 318, 322, 325, 331, 335, 342, 351, 354, 359, 366, 371, 375, 377, 383, 391, 393]\n",
            " \tstart_of_word: [True, False, True, False, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, False, True, True, True, False, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, False, True]\n",
            "Features: \n",
            " \tinput_ids: [0, 113, 8813, 11728, 5559, 23829, 5, 380, 65, 8, 32, 5, 144, 21250, 5717, 577, 734, 54, 109, 51, 206, 51, 32, 7, 860, 7, 1137, 1268, 51, 33, 12732, 11728, 5559, 23, 334, 38713, 84, 334, 11728, 5559, 32, 98, 33785, 239, 11, 5886, 8, 26219, 734, 45, 7, 4521, 12069, 6592, 32, 5, 1647, 9, 99, 51, 1807, 4, 38, 74, 393, 905, 127, 920, 3529, 334, 1286, 11728, 5559, 3867, 51, 222, 10, 538, 7455, 328, 22, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \ttext_classification_label_ids: [0]\n",
            "_____________________________________________________\n",
            "Preprocessing Dataset ../content/train.tsv: 100%|██████████| 3947/3947 [00:15<00:00, 252.39 Dicts/s]\n",
            "09/13/2021 16:12:53 - INFO - farm.data_handler.data_silo -   Loading dev set as a slice of train set\n",
            "09/13/2021 16:12:53 - INFO - farm.data_handler.data_silo -   Took 787 samples out of train set to create dev set (dev split is roughly 0.1)\n",
            "09/13/2021 16:12:53 - INFO - farm.data_handler.data_silo -   Loading test set from: ../content/test_with_solutions.tsv\n",
            "09/13/2021 16:12:53 - INFO - farm.data_handler.data_silo -   Got ya 1 parallel workers to convert 2647 dictionaries to pytorch datasets (chunksize = 530)...\n",
            "09/13/2021 16:12:53 - INFO - farm.data_handler.data_silo -    0 \n",
            "09/13/2021 16:12:53 - INFO - farm.data_handler.data_silo -   /w\\\n",
            "09/13/2021 16:12:53 - INFO - farm.data_handler.data_silo -   / \\\n",
            "09/13/2021 16:12:53 - INFO - farm.data_handler.data_silo -   \n",
            "Preprocessing Dataset ../content/test_with_solutions.tsv:   0%|          | 0/2647 [00:00<?, ? Dicts/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils.py:460: FutureWarning: `is_pretokenized` is deprecated and will be removed in a future version, use `is_split_into_words` instead.\n",
            "  FutureWarning,\n",
            "09/13/2021 16:12:57 - INFO - farm.data_handler.processor -   *** Show 2 random examples ***\n",
            "09/13/2021 16:12:57 - INFO - farm.data_handler.processor -   \n",
            "\n",
            "      .--.        _____                       _      \n",
            "    .'_\\/_'.     / ____|                     | |     \n",
            "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
            "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
            "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
            "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
            "   (/\\||/                             |_|           \n",
            "______\\||/___________________________________________                     \n",
            "\n",
            "ID: 48-0\n",
            "Clear Text: \n",
            " \ttext_classification_label: 0\n",
            " \ttext: \"why not hire the most qualified and capable instead of discrimating based on race and gender?\"\n",
            "Tokenized: \n",
            " \ttokens: ['\"', 'why', 'Ġnot', 'Ġhire', 'Ġthe', 'Ġmost', 'Ġqualified', 'Ġand', 'Ġcapable', 'Ġinstead', 'Ġof', 'Ġdiscrim', 'ating', 'Ġbased', 'Ġon', 'Ġrace', 'Ġand', 'Ġgender', '?\"']\n",
            " \toffsets: [0, 1, 5, 9, 14, 18, 23, 33, 37, 45, 53, 56, 63, 69, 75, 78, 83, 87, 93]\n",
            " \tstart_of_word: [True, False, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, False]\n",
            "Features: \n",
            " \tinput_ids: [0, 113, 25800, 45, 6198, 5, 144, 6048, 8, 4453, 1386, 9, 40846, 1295, 716, 15, 1015, 8, 3959, 1917, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \ttext_classification_label_ids: [0]\n",
            "_____________________________________________________\n",
            "09/13/2021 16:12:57 - INFO - farm.data_handler.processor -   \n",
            "\n",
            "      .--.        _____                       _      \n",
            "    .'_\\/_'.     / ____|                     | |     \n",
            "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
            "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
            "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
            "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
            "   (/\\||/                             |_|           \n",
            "______\\||/___________________________________________                     \n",
            "\n",
            "ID: 47-0\n",
            "Clear Text: \n",
            " \ttext_classification_label: 0\n",
            " \ttext: \"@SandyBerman Beached! Whaaaaaaaaale!\"\n",
            "Tokenized: \n",
            " \ttokens: ['\"', '@', 'S', 'andy', 'B', 'erman', 'ĠBe', 'ached', '!', 'ĠWh', 'aaaa', 'aaaa', 'ale', '!\"']\n",
            " \toffsets: [0, 1, 2, 3, 7, 8, 14, 16, 21, 23, 25, 29, 33, 36]\n",
            " \tstart_of_word: [True, False, False, False, False, False, True, False, False, True, False, False, False, False]\n",
            "Features: \n",
            " \tinput_ids: [0, 113, 1039, 104, 10708, 387, 7043, 1456, 12552, 328, 3990, 44172, 44172, 1627, 2901, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \ttext_classification_label_ids: [0]\n",
            "_____________________________________________________\n",
            "Preprocessing Dataset ../content/test_with_solutions.tsv: 100%|██████████| 2647/2647 [00:11<00:00, 230.13 Dicts/s]\n",
            "09/13/2021 16:13:05 - INFO - farm.data_handler.data_silo -   Examples in train: 3160\n",
            "09/13/2021 16:13:05 - INFO - farm.data_handler.data_silo -   Examples in dev  : 787\n",
            "09/13/2021 16:13:05 - INFO - farm.data_handler.data_silo -   Examples in test : 2647\n",
            "09/13/2021 16:13:05 - INFO - farm.data_handler.data_silo -   \n",
            "09/13/2021 16:13:05 - INFO - farm.data_handler.data_silo -   Longest sequence length observed after clipping:     128\n",
            "09/13/2021 16:13:05 - INFO - farm.data_handler.data_silo -   Average sequence length after clipping: 41.50316455696203\n",
            "09/13/2021 16:13:05 - INFO - farm.data_handler.data_silo -   Proportion clipped:      0.06930379746835443\n",
            "09/13/2021 16:13:05 - WARNING - farm.utils -   Failed to log params: INVALID_PARAMETER_VALUE: Changing param value is not allowed. Param with key='ave_seq_len' was already logged with value='43.41107594936709' for run ID='6543ccfdbba146ca82a34b6de1bbb983. Attempted logging new value '41.50316455696203'.\n",
            "09/13/2021 16:13:09 - WARNING - farm.modeling.language_model -   Could not automatically detect from language model name what language it is. \n",
            "\t We guess it's an *ENGLISH* model ... \n",
            "\t If not: Init the language model by supplying the 'language' param.\n",
            "09/13/2021 16:13:09 - INFO - farm.modeling.prediction_head -   Prediction head initialized with size [768, 2]\n",
            "09/13/2021 16:13:09 - INFO - farm.modeling.prediction_head -   Using class weights for task 'text_classification': [0.6791237 1.8956834]\n",
            "09/13/2021 16:13:10 - WARNING - farm.utils -   Failed to log params: INVALID_PARAMETER_VALUE: Changing param value is not allowed. Param with key='lm_type' was already logged with value='DistilBert' for run ID='6543ccfdbba146ca82a34b6de1bbb983. Attempted logging new value 'Roberta'.\n",
            "09/13/2021 16:13:10 - INFO - farm.modeling.optimization -   Loading optimizer `TransformersAdamW`: '{'correct_bias': False, 'weight_decay': 0.01, 'lr': 1e-05}'\n",
            "09/13/2021 16:13:10 - INFO - farm.modeling.optimization -   Using scheduler 'get_linear_schedule_with_warmup'\n",
            "09/13/2021 16:13:10 - INFO - farm.modeling.optimization -   Loading schedule `get_linear_schedule_with_warmup`: '{'num_warmup_steps': 29.700000000000003, 'num_training_steps': 297}'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Sep 13 16:13:10 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.63.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   44C    P0    60W / 149W |   6289MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "09/13/2021 16:13:10 - INFO - farm.train -   \n",
            " \n",
            "\n",
            "          &&& &&  & &&             _____                   _             \n",
            "      && &\\/&\\|& ()|/ @, &&       / ____|                 (_)            \n",
            "      &\\/(/&/&||/& /_/)_&/_&     | |  __ _ __ _____      ___ _ __   __ _ \n",
            "   &() &\\/&|()|/&\\/ '%\" & ()     | | |_ | '__/ _ \\ \\ /\\ / / | '_ \\ / _` |\n",
            "  &_\\_&&_\\ |& |&&/&__%_/_& &&    | |__| | | | (_) \\ V  V /| | | | | (_| |\n",
            "&&   && & &| &| /& & % ()& /&&    \\_____|_|  \\___/ \\_/\\_/ |_|_| |_|\\__, |\n",
            " ()&_---()&\\&\\|&&-&&--%---()~                                       __/ |\n",
            "     &&     \\|||                                                   |___/\n",
            "             |||\n",
            "             |||\n",
            "             |||\n",
            "       , -=-~  .-^- _\n",
            "              `\n",
            "\n",
            "Train epoch 0/2 (Cur. train loss: 0.3214): 100%|██████████| 99/99 [02:04<00:00,  1.26s/it]\n",
            "Train epoch 1/2 (Cur. train loss: 0.4007):   1%|          | 1/99 [00:01<02:00,  1.23s/it]\n",
            "Evaluating:   0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|██████████| 25/25 [00:10<00:00,  2.29it/s]\n",
            "09/13/2021 16:15:28 - INFO - farm.eval -   \n",
            "\n",
            "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "***************************************************\n",
            "***** EVALUATION | DEV SET | AFTER 100 BATCHES *****\n",
            "***************************************************\n",
            "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "\n",
            "09/13/2021 16:15:28 - INFO - farm.eval -   \n",
            " _________ text_classification _________\n",
            "09/13/2021 16:15:28 - INFO - farm.eval -   loss: 0.34151789799730237\n",
            "09/13/2021 16:15:28 - INFO - farm.eval -   task_name: text_classification\n",
            "09/13/2021 16:15:28 - INFO - farm.eval -   f1_macro: 0.8323843449907173\n",
            "09/13/2021 16:15:28 - INFO - farm.eval -   report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9455    0.8511    0.8959       571\n",
            "           1     0.6886    0.8704    0.7689       216\n",
            "\n",
            "    accuracy                         0.8564       787\n",
            "   macro avg     0.8171    0.8608    0.8324       787\n",
            "weighted avg     0.8750    0.8564    0.8610       787\n",
            "\n",
            "Train epoch 1/2 (Cur. train loss: 0.2350): 100%|██████████| 99/99 [02:13<00:00,  1.35s/it]\n",
            "Train epoch 2/2 (Cur. train loss: 0.0258):   2%|▏         | 2/99 [00:02<01:58,  1.23s/it]\n",
            "Evaluating:   0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|██████████| 25/25 [00:10<00:00,  2.30it/s]\n",
            "09/13/2021 16:17:43 - INFO - farm.eval -   \n",
            "\n",
            "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "***************************************************\n",
            "***** EVALUATION | DEV SET | AFTER 200 BATCHES *****\n",
            "***************************************************\n",
            "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "\n",
            "09/13/2021 16:17:43 - INFO - farm.eval -   \n",
            " _________ text_classification _________\n",
            "09/13/2021 16:17:43 - INFO - farm.eval -   loss: 0.4253318412785609\n",
            "09/13/2021 16:17:43 - INFO - farm.eval -   task_name: text_classification\n",
            "09/13/2021 16:17:43 - INFO - farm.eval -   f1_macro: 0.8580862190109624\n",
            "09/13/2021 16:17:43 - INFO - farm.eval -   report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9130    0.9370    0.9248       571\n",
            "           1     0.8209    0.7639    0.7914       216\n",
            "\n",
            "    accuracy                         0.8895       787\n",
            "   macro avg     0.8669    0.8504    0.8581       787\n",
            "weighted avg     0.8877    0.8895    0.8882       787\n",
            "\n",
            "Train epoch 2/2 (Cur. train loss: 0.0150): 100%|██████████| 99/99 [02:13<00:00,  1.35s/it]\n",
            "Evaluating: 100%|██████████| 83/83 [00:36<00:00,  2.28it/s]\n",
            "09/13/2021 16:20:18 - INFO - farm.eval -   \n",
            "\n",
            "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "***************************************************\n",
            "***** EVALUATION | TEST SET | AFTER 297 BATCHES *****\n",
            "***************************************************\n",
            "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "\n",
            "09/13/2021 16:20:18 - INFO - farm.eval -   \n",
            " _________ text_classification _________\n",
            "09/13/2021 16:20:18 - INFO - farm.eval -   loss: 0.4617162967205588\n",
            "09/13/2021 16:20:18 - INFO - farm.eval -   task_name: text_classification\n",
            "09/13/2021 16:20:18 - INFO - farm.eval -   f1_macro: 0.863922646902439\n",
            "09/13/2021 16:20:18 - INFO - farm.eval -   report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9310    0.9253    0.9281      1954\n",
            "           1     0.7929    0.8066    0.7997       693\n",
            "\n",
            "    accuracy                         0.8942      2647\n",
            "   macro avg     0.8620    0.8660    0.8639      2647\n",
            "weighted avg     0.8948    0.8942    0.8945      2647\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXbVD_NEhqZ-"
      },
      "source": [
        "## ALBERT <br>\n",
        "https://huggingface.co/albert-base-v2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Eder9_RhtEA",
        "outputId": "1f55b53d-9a79-4c49-a87b-31d5e1eedd6c"
      },
      "source": [
        "from transformers import AutoTokenizer\n",
        "import torch\n",
        "from farm.modeling.tokenization import Tokenizer\n",
        "from farm.data_handler.processor import TextClassificationProcessor\n",
        "from farm.data_handler.data_silo import DataSilo\n",
        "from farm.modeling.language_model import LanguageModel\n",
        "from farm.modeling.prediction_head import TextClassificationHead\n",
        "from farm.modeling.adaptive_model import AdaptiveModel\n",
        "from farm.modeling.optimization import initialize_optimizer\n",
        "from farm.train import Trainer\n",
        "from farm.utils import MLFlowLogger\n",
        "\n",
        "# Here we initialize a tokenizer that will be used for preprocessing text\n",
        "# This is the BERT Tokenizer which uses the byte pair encoding method.\n",
        "# It is currently loaded with a German model\n",
        "\n",
        "#tokenizer = Tokenizer.load(\n",
        "#    pretrained_model_name_or_path=\"finiteautomata/bertweet-base-sentiment-analysis\",\n",
        "#    do_lower_case=False)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"albert-base-v2\")\n",
        "\n",
        "# In order to prepare the data for the model, we need a set of\n",
        "# functions to transform data files into PyTorch Datasets.\n",
        "# We group these together in Processor objects.\n",
        "# We will need a new Processor object for each new source of data.\n",
        "# The abstract class can be found in farm.data_handling.processor.Processor\n",
        "# TOXIC = 1\n",
        "# OTHER = 0\n",
        "LABEL_LIST = [\"0\", \"1\"]\n",
        "processor = TextClassificationProcessor(tokenizer=tokenizer,\n",
        "                                        max_seq_len=128,\n",
        "                                        warmup = 600,\n",
        "                                        train_filename=\"train.tsv\",\n",
        "                                        test_filename=\"test_with_solutions.tsv\",\n",
        "                                        data_dir=\"../content\",\n",
        "                                        label_list=LABEL_LIST,\n",
        "                                        metric=\"f1_macro\",\n",
        "                                        text_column_name=\"Comment\",\n",
        "                                        label_column_name=\"Insult\")\n",
        "\n",
        "# We need a DataSilo in order to keep our train, dev and test sets separate.\n",
        "# The DataSilo will call the functions in the Processor to generate these sets.\n",
        "# From the DataSilo, we can fetch a PyTorch DataLoader object which will\n",
        "# be passed on to the model.\n",
        "# Here is a good place to define a batch size for the model\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "data_silo = DataSilo(\n",
        "    processor=processor,\n",
        "    batch_size=BATCH_SIZE)\n",
        "\n",
        "# The language model is the foundation on which modern NLP systems are built.\n",
        "# They encapsulate a general understanding of sentence semantics\n",
        "# and are not specific to any one task.\n",
        "\n",
        "# Here we are using Google's BERT model as implemented by HuggingFace. \n",
        "# The model being loaded is a German model that we trained. \n",
        "# You can also change the MODEL_NAME_OR_PATH to point to a BERT model that you\n",
        "# have saved or download one connected to the HuggingFace repository.\n",
        "# See farm.modeling.language_model.PRETRAINED_MODEL_ARCHIVE_MAP for a list of\n",
        "# available models\n",
        "\n",
        "MODEL_NAME_OR_PATH = \"albert-base-v2\"\n",
        "# MODEL_NAME_OR_PATH = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "language_model = LanguageModel.load(MODEL_NAME_OR_PATH)\n",
        "\n",
        "# A prediction head is a model that processes the output of the language model\n",
        "# for a specific task.\n",
        "# Prediction heads will look different depending on whether you're doing text classification\n",
        "# Named Entity Recognition (NER), question answering or some other task.\n",
        "# They should generate logits over the available prediction classes and contain methods\n",
        "# to convert these logits to losses or predictions \n",
        "\n",
        "# Here we use TextClassificationHead which receives a single fixed length sentence vector\n",
        "# and processes it using a feed forward neural network. layer_dims is a list of dimensions:\n",
        "# [input_dims, hidden_1_dims, hidden_2_dims ..., output_dims]\n",
        "\n",
        "# Here by default we have a single layer network.\n",
        "# It takes in a vector of length 768 (the default size of BERT's output).\n",
        "# It outputs a vector of length 2 (the number of classes in the GermEval18 (coarse) dataset)\n",
        "\n",
        "prediction_head = TextClassificationHead(num_labels=len(LABEL_LIST), class_weights=data_silo.calculate_class_weights(task_name=\"text_classification\"))\n",
        "\n",
        "# The language model and prediction head are coupled together in the Adaptive Model.\n",
        "# This class takes care of model saving and loading and also coordinates\n",
        "# cases where there is more than one prediction head.\n",
        "\n",
        "# EMBEDS_DROPOUT_PROB is the probability that an element of the output vector from the\n",
        "# language model will be set to zero.\n",
        "# EMBEDS_DROPOUT_PROB = 0.1 distilbert-base-uncased-finetuned-sst-2-english_1.PNG\n",
        "# EMBEDS_DROPOUT_PROB = 0.01\n",
        "# EMBEDS_DROPOUT_PROB = 0.2 distilbert-base-uncased-finetuned-sst-2-english_2.PNG\n",
        "EMBEDS_DROPOUT_PROB = 0.01\n",
        "\n",
        "model = AdaptiveModel(\n",
        "    language_model=language_model,\n",
        "    prediction_heads=[prediction_head],\n",
        "    embeds_dropout_prob=EMBEDS_DROPOUT_PROB,\n",
        "    lm_output_types=[\"per_sequence\"],\n",
        "    device=device)\n",
        "\n",
        "# Here we initialize a Bert Adam optimizer that has a linear warmup and warmdown\n",
        "# Here you can set learning rate, the warmup proportion and number of epochs to train for\n",
        "\n",
        "#LEARNING_RATE = 2e-5 # distilbert-base-uncased-finetuned-sst-2-english_1.PNG\n",
        "#LEARNING_RATE = 1e-7\n",
        "LEARNING_RATE = 1e-5  # distilbert-base-uncased-finetuned-sst-2-english_2.PNG\n",
        "\n",
        "N_EPOCHS = 3\n",
        "\n",
        "model, optimizer, lr_schedule = initialize_optimizer(\n",
        "    model=model,\n",
        "    device=device,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    n_batches=len(data_silo.loaders[\"train\"]),\n",
        "    n_epochs=N_EPOCHS)\n",
        "\n",
        "# Training loop handled by this\n",
        "# It will also trigger evaluation during training using the dev data\n",
        "# and after training using the test data.\n",
        "\n",
        "# Set N_GPU to a positive value if CUDA is available\n",
        "N_GPU = 1\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    optimizer=optimizer,\n",
        "    data_silo=data_silo,\n",
        "    epochs=N_EPOCHS,\n",
        "    n_gpu=N_GPU,\n",
        "    lr_schedule=lr_schedule,\n",
        "    device=device, \n",
        ")\n",
        "\n",
        "!nvidia-smi\n",
        "\n",
        "model = trainer.train()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "09/13/2021 16:20:20 - WARNING - farm.utils -   Failed to log params: INVALID_PARAMETER_VALUE: Changing param value is not allowed. Param with key='tokenizer' was already logged with value='DistilBertTokenizer' for run ID='6543ccfdbba146ca82a34b6de1bbb983. Attempted logging new value 'AlbertTokenizer'.\n",
            "09/13/2021 16:20:20 - INFO - farm.data_handler.data_silo -   \n",
            "Loading data into the data silo ... \n",
            "              ______\n",
            "               |o  |   !\n",
            "   __          |:`_|---'-.\n",
            "  |__|______.-/ _ \\-----.|       \n",
            " (o)(o)------'\\ _ /     ( )      \n",
            " \n",
            "09/13/2021 16:20:20 - INFO - farm.data_handler.data_silo -   Loading train set from: ../content/train.tsv \n",
            "09/13/2021 16:20:20 - INFO - farm.data_handler.data_silo -   Got ya 1 parallel workers to convert 3947 dictionaries to pytorch datasets (chunksize = 790)...\n",
            "09/13/2021 16:20:20 - INFO - farm.data_handler.data_silo -    0 \n",
            "09/13/2021 16:20:20 - INFO - farm.data_handler.data_silo -   /w\\\n",
            "09/13/2021 16:20:20 - INFO - farm.data_handler.data_silo -   / \\\n",
            "09/13/2021 16:20:20 - INFO - farm.data_handler.data_silo -   \n",
            "Preprocessing Dataset ../content/train.tsv:   0%|          | 0/3947 [00:00<?, ? Dicts/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils.py:460: FutureWarning: `is_pretokenized` is deprecated and will be removed in a future version, use `is_split_into_words` instead.\n",
            "  FutureWarning,\n",
            "09/13/2021 16:20:21 - INFO - farm.data_handler.processor -   *** Show 2 random examples ***\n",
            "09/13/2021 16:20:21 - INFO - farm.data_handler.processor -   \n",
            "\n",
            "      .--.        _____                       _      \n",
            "    .'_\\/_'.     / ____|                     | |     \n",
            "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
            "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
            "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
            "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
            "   (/\\||/                             |_|           \n",
            "______\\||/___________________________________________                     \n",
            "\n",
            "ID: 187-0\n",
            "Clear Text: \n",
            " \ttext_classification_label: 1\n",
            " \ttext: \"You're terrible and you've got 28 dislikes. Look's like you're an idiot just like IGN(orant)\"\n",
            "Tokenized: \n",
            " \ttokens: ['▁', '\"', 'you', \"'\", 're', '▁terrible', '▁and', '▁you', \"'\", 've', '▁got', '▁28', '▁dislike', 's', '.', '▁look', \"'\", 's', '▁like', '▁you', \"'\", 're', '▁an', '▁idiot', '▁just', '▁like', '▁ign', '(', 'or', 'ant', ')', '\"']\n",
            " \toffsets: [0, 0, 1, 4, 5, 8, 17, 21, 24, 25, 28, 32, 35, 42, 43, 45, 49, 50, 52, 57, 60, 61, 64, 67, 73, 78, 83, 86, 87, 89, 92, 93]\n",
            " \tstart_of_word: [True, False, False, False, False, True, True, True, False, False, True, True, True, False, False, True, False, False, True, True, False, False, True, True, True, True, True, False, False, False, False, False]\n",
            "Features: \n",
            " \tinput_ids: [2, 13, 7, 245, 22, 99, 5803, 17, 42, 22, 195, 330, 1274, 18686, 18, 9, 361, 22, 18, 101, 42, 22, 99, 40, 8563, 114, 101, 11491, 5, 248, 1830, 6, 7, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \ttext_classification_label_ids: [1]\n",
            "_____________________________________________________\n",
            "09/13/2021 16:20:21 - INFO - farm.data_handler.processor -   \n",
            "\n",
            "      .--.        _____                       _      \n",
            "    .'_\\/_'.     / ____|                     | |     \n",
            "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
            "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
            "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
            "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
            "   (/\\||/                             |_|           \n",
            "______\\||/___________________________________________                     \n",
            "\n",
            "ID: 561-0\n",
            "Clear Text: \n",
            " \ttext_classification_label: 0\n",
            " \ttext: \"Magic Number, Magic Underpants = No Big Deal\"\n",
            "Tokenized: \n",
            " \ttokens: ['▁', '\"', 'magic', '▁number', ',', '▁magic', '▁under', 'pants', '▁=', '▁no', '▁big', '▁deal', '\"']\n",
            " \toffsets: [0, 0, 1, 7, 13, 15, 21, 26, 32, 34, 37, 41, 45]\n",
            " \tstart_of_word: [True, False, False, True, False, True, True, False, True, True, True, True, False]\n",
            "Features: \n",
            " \tinput_ids: [2, 13, 7, 21200, 234, 15, 2154, 131, 24293, 800, 90, 580, 1183, 7, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \ttext_classification_label_ids: [0]\n",
            "_____________________________________________________\n",
            "Preprocessing Dataset ../content/train.tsv: 100%|██████████| 3947/3947 [00:08<00:00, 477.78 Dicts/s]\n",
            "09/13/2021 16:20:28 - INFO - farm.data_handler.data_silo -   Loading dev set as a slice of train set\n",
            "09/13/2021 16:20:28 - INFO - farm.data_handler.data_silo -   Took 787 samples out of train set to create dev set (dev split is roughly 0.1)\n",
            "09/13/2021 16:20:28 - INFO - farm.data_handler.data_silo -   Loading test set from: ../content/test_with_solutions.tsv\n",
            "09/13/2021 16:20:28 - INFO - farm.data_handler.data_silo -   Got ya 1 parallel workers to convert 2647 dictionaries to pytorch datasets (chunksize = 530)...\n",
            "09/13/2021 16:20:28 - INFO - farm.data_handler.data_silo -    0 \n",
            "09/13/2021 16:20:28 - INFO - farm.data_handler.data_silo -   /w\\\n",
            "09/13/2021 16:20:28 - INFO - farm.data_handler.data_silo -   /'\\\n",
            "09/13/2021 16:20:28 - INFO - farm.data_handler.data_silo -   \n",
            "Preprocessing Dataset ../content/test_with_solutions.tsv:   0%|          | 0/2647 [00:00<?, ? Dicts/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils.py:460: FutureWarning: `is_pretokenized` is deprecated and will be removed in a future version, use `is_split_into_words` instead.\n",
            "  FutureWarning,\n",
            "09/13/2021 16:20:30 - INFO - farm.data_handler.processor -   *** Show 2 random examples ***\n",
            "09/13/2021 16:20:30 - INFO - farm.data_handler.processor -   \n",
            "\n",
            "      .--.        _____                       _      \n",
            "    .'_\\/_'.     / ____|                     | |     \n",
            "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
            "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
            "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
            "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
            "   (/\\||/                             |_|           \n",
            "______\\||/___________________________________________                     \n",
            "\n",
            "ID: 143-0\n",
            "Clear Text: \n",
            " \ttext_classification_label: 0\n",
            " \ttext: \"Holy shit, you're right!\"\n",
            "Tokenized: \n",
            " \ttokens: ['▁', '\"', 'holy', '▁shit', ',', '▁you', \"'\", 're', '▁right', '!', '\"']\n",
            " \toffsets: [0, 0, 1, 6, 10, 12, 15, 16, 19, 24, 25]\n",
            " \tstart_of_word: [True, False, False, True, False, True, False, False, True, False, False]\n",
            "Features: \n",
            " \tinput_ids: [2, 13, 7, 15496, 2920, 15, 42, 22, 99, 193, 187, 7, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \ttext_classification_label_ids: [0]\n",
            "_____________________________________________________\n",
            "09/13/2021 16:20:30 - INFO - farm.data_handler.processor -   \n",
            "\n",
            "      .--.        _____                       _      \n",
            "    .'_\\/_'.     / ____|                     | |     \n",
            "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
            "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
            "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
            "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
            "   (/\\||/                             |_|           \n",
            "______\\||/___________________________________________                     \n",
            "\n",
            "ID: 75-0\n",
            "Clear Text: \n",
            " \ttext_classification_label: 0\n",
            " \ttext: \"@foxrock66 Why would you throw something like Porn out there in the middle of your argument? Is really easy to judge that way. You are right I am just assuming. Because ding ding you make it come across that way. As for the wife thing. I gave an example on how it helps people. You keep talking about yourself. Hence why I think is you don't indulge as you put it.\"\n",
            "Tokenized: \n",
            " \ttokens: ['▁', '\"', '@', 'fox', 'rock', '66', '▁why', '▁would', '▁you', '▁throw', '▁something', '▁like', '▁por', 'n', '▁out', '▁there', '▁in', '▁the', '▁middle', '▁of', '▁your', '▁argument', '?', '▁is', '▁really', '▁easy', '▁to', '▁judge', '▁that', '▁way', '.', '▁you', '▁are', '▁right', '▁i', '▁am', '▁just', '▁assuming', '.', '▁because', '▁ding', '▁ding', '▁you', '▁make', '▁it', '▁come', '▁across', '▁that', '▁way', '.', '▁as', '▁for', '▁the', '▁wife', '▁thing', '.', '▁i', '▁gave', '▁an', '▁example', '▁on', '▁how', '▁it', '▁helps', '▁people', '.', '▁you', '▁keep', '▁talking', '▁about', '▁yourself', '.', '▁hence', '▁why', '▁i', '▁think', '▁is', '▁you', '▁don', \"'\", 't', '▁indulge', '▁as', '▁you', '▁put', '▁it', '.', '\"']\n",
            " \toffsets: [0, 0, 1, 2, 5, 9, 12, 16, 22, 26, 32, 42, 47, 50, 52, 56, 62, 65, 69, 76, 79, 84, 92, 94, 97, 104, 109, 112, 118, 123, 126, 128, 132, 136, 142, 144, 147, 152, 160, 162, 170, 175, 180, 184, 189, 192, 197, 204, 209, 212, 214, 217, 221, 225, 230, 235, 237, 239, 244, 247, 255, 258, 262, 265, 271, 277, 279, 283, 288, 296, 302, 310, 312, 318, 322, 324, 330, 333, 337, 340, 341, 343, 351, 354, 358, 362, 364, 365]\n",
            " \tstart_of_word: [True, False, False, False, False, False, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, False, True, True, True, True, True, True, True, False, False, True, True, True, True, True, False, False]\n",
            "Features: \n",
            " \tinput_ids: [2, 13, 7, 1, 18219, 4651, 3526, 483, 83, 42, 3814, 301, 101, 5156, 103, 70, 80, 19, 14, 772, 16, 154, 5476, 60, 25, 510, 2010, 20, 1878, 30, 161, 9, 42, 50, 193, 31, 589, 114, 11704, 9, 185, 16501, 16501, 42, 233, 32, 340, 464, 30, 161, 9, 28, 26, 14, 663, 584, 9, 31, 492, 40, 823, 27, 184, 32, 7778, 148, 9, 42, 643, 1582, 88, 2834, 9, 5796, 483, 31, 277, 25, 42, 221, 22, 38, 25705, 28, 42, 442, 32, 9, 7, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \ttext_classification_label_ids: [0]\n",
            "_____________________________________________________\n",
            "Preprocessing Dataset ../content/test_with_solutions.tsv: 100%|██████████| 2647/2647 [00:06<00:00, 410.66 Dicts/s]\n",
            "09/13/2021 16:20:35 - INFO - farm.data_handler.data_silo -   Examples in train: 3160\n",
            "09/13/2021 16:20:35 - INFO - farm.data_handler.data_silo -   Examples in dev  : 787\n",
            "09/13/2021 16:20:35 - INFO - farm.data_handler.data_silo -   Examples in test : 2647\n",
            "09/13/2021 16:20:35 - INFO - farm.data_handler.data_silo -   \n",
            "09/13/2021 16:20:35 - INFO - farm.data_handler.data_silo -   Longest sequence length observed after clipping:     128\n",
            "09/13/2021 16:20:35 - INFO - farm.data_handler.data_silo -   Average sequence length after clipping: 45.55917721518988\n",
            "09/13/2021 16:20:35 - INFO - farm.data_handler.data_silo -   Proportion clipped:      0.07943037974683544\n",
            "09/13/2021 16:20:35 - WARNING - farm.utils -   Failed to log params: INVALID_PARAMETER_VALUE: Changing param value is not allowed. Param with key='ave_seq_len' was already logged with value='43.41107594936709' for run ID='6543ccfdbba146ca82a34b6de1bbb983. Attempted logging new value '45.55917721518988'.\n",
            "09/13/2021 16:20:36 - WARNING - farm.modeling.language_model -   Could not automatically detect from language model name what language it is. \n",
            "\t We guess it's an *ENGLISH* model ... \n",
            "\t If not: Init the language model by supplying the 'language' param.\n",
            "09/13/2021 16:20:36 - INFO - farm.modeling.prediction_head -   Prediction head initialized with size [768, 2]\n",
            "09/13/2021 16:20:36 - INFO - farm.modeling.prediction_head -   Using class weights for task 'text_classification': [0.68979055 1.8172414 ]\n",
            "09/13/2021 16:20:36 - WARNING - farm.utils -   Failed to log params: INVALID_PARAMETER_VALUE: Changing param value is not allowed. Param with key='lm_type' was already logged with value='DistilBert' for run ID='6543ccfdbba146ca82a34b6de1bbb983. Attempted logging new value 'Albert'.\n",
            "09/13/2021 16:20:36 - INFO - farm.modeling.optimization -   Loading optimizer `TransformersAdamW`: '{'correct_bias': False, 'weight_decay': 0.01, 'lr': 1e-05}'\n",
            "09/13/2021 16:20:37 - INFO - farm.modeling.optimization -   Using scheduler 'get_linear_schedule_with_warmup'\n",
            "09/13/2021 16:20:37 - INFO - farm.modeling.optimization -   Loading schedule `get_linear_schedule_with_warmup`: '{'num_warmup_steps': 29.700000000000003, 'num_training_steps': 297}'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Sep 13 16:20:37 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.63.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   51C    P0    62W / 149W |   6881MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "09/13/2021 16:20:37 - INFO - farm.train -   \n",
            " \n",
            "\n",
            "          &&& &&  & &&             _____                   _             \n",
            "      && &\\/&\\|& ()|/ @, &&       / ____|                 (_)            \n",
            "      &\\/(/&/&||/& /_/)_&/_&     | |  __ _ __ _____      ___ _ __   __ _ \n",
            "   &() &\\/&|()|/&\\/ '%\" & ()     | | |_ | '__/ _ \\ \\ /\\ / / | '_ \\ / _` |\n",
            "  &_\\_&&_\\ |& |&&/&__%_/_& &&    | |__| | | | (_) \\ V  V /| | | | | (_| |\n",
            "&&   && & &| &| /& & % ()& /&&    \\_____|_|  \\___/ \\_/\\_/ |_|_| |_|\\__, |\n",
            " ()&_---()&\\&\\|&&-&&--%---()~                                       __/ |\n",
            "     &&     \\|||                                                   |___/\n",
            "             |||\n",
            "             |||\n",
            "             |||\n",
            "       , -=-~  .-^- _\n",
            "              `\n",
            "\n",
            "Train epoch 0/2 (Cur. train loss: 0.5243): 100%|██████████| 99/99 [02:08<00:00,  1.30s/it]\n",
            "Train epoch 1/2 (Cur. train loss: 0.2585):   1%|          | 1/99 [00:01<02:06,  1.29s/it]\n",
            "Evaluating:   0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|██████████| 25/25 [00:12<00:00,  2.08it/s]\n",
            "09/13/2021 16:23:00 - INFO - farm.eval -   \n",
            "\n",
            "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "***************************************************\n",
            "***** EVALUATION | DEV SET | AFTER 100 BATCHES *****\n",
            "***************************************************\n",
            "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "\n",
            "09/13/2021 16:23:00 - INFO - farm.eval -   \n",
            " _________ text_classification _________\n",
            "09/13/2021 16:23:00 - INFO - farm.eval -   loss: 0.3303519961491625\n",
            "09/13/2021 16:23:00 - INFO - farm.eval -   task_name: text_classification\n",
            "09/13/2021 16:23:00 - INFO - farm.eval -   f1_macro: 0.8200357221582446\n",
            "09/13/2021 16:23:00 - INFO - farm.eval -   report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9542    0.8583    0.9037       607\n",
            "           1     0.6432    0.8611    0.7363       180\n",
            "\n",
            "    accuracy                         0.8590       787\n",
            "   macro avg     0.7987    0.8597    0.8200       787\n",
            "weighted avg     0.8831    0.8590    0.8654       787\n",
            "\n",
            "Train epoch 1/2 (Cur. train loss: 0.1479): 100%|██████████| 99/99 [02:20<00:00,  1.42s/it]\n",
            "Train epoch 2/2 (Cur. train loss: 0.2237):   2%|▏         | 2/99 [00:02<02:04,  1.28s/it]\n",
            "Evaluating:   0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|██████████| 25/25 [00:12<00:00,  2.07it/s]\n",
            "09/13/2021 16:25:22 - INFO - farm.eval -   \n",
            "\n",
            "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "***************************************************\n",
            "***** EVALUATION | DEV SET | AFTER 200 BATCHES *****\n",
            "***************************************************\n",
            "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "\n",
            "09/13/2021 16:25:22 - INFO - farm.eval -   \n",
            " _________ text_classification _________\n",
            "09/13/2021 16:25:22 - INFO - farm.eval -   loss: 0.31521789030925745\n",
            "09/13/2021 16:25:22 - INFO - farm.eval -   task_name: text_classification\n",
            "09/13/2021 16:25:22 - INFO - farm.eval -   f1_macro: 0.8335448392554992\n",
            "09/13/2021 16:25:22 - INFO - farm.eval -   report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9426    0.8929    0.9171       607\n",
            "           1     0.6934    0.8167    0.7500       180\n",
            "\n",
            "    accuracy                         0.8755       787\n",
            "   macro avg     0.8180    0.8548    0.8335       787\n",
            "weighted avg     0.8856    0.8755    0.8789       787\n",
            "\n",
            "Train epoch 2/2 (Cur. train loss: 0.1158): 100%|██████████| 99/99 [02:20<00:00,  1.42s/it]\n",
            "Evaluating: 100%|██████████| 83/83 [00:40<00:00,  2.05it/s]\n",
            "09/13/2021 16:28:07 - INFO - farm.eval -   \n",
            "\n",
            "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "***************************************************\n",
            "***** EVALUATION | TEST SET | AFTER 297 BATCHES *****\n",
            "***************************************************\n",
            "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "\n",
            "09/13/2021 16:28:07 - INFO - farm.eval -   \n",
            " _________ text_classification _________\n",
            "09/13/2021 16:28:07 - INFO - farm.eval -   loss: 0.4292538761417056\n",
            "09/13/2021 16:28:07 - INFO - farm.eval -   task_name: text_classification\n",
            "09/13/2021 16:28:07 - INFO - farm.eval -   f1_macro: 0.8431358933154944\n",
            "09/13/2021 16:28:07 - INFO - farm.eval -   report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9240    0.9079    0.9158      1954\n",
            "           1     0.7524    0.7893    0.7704       693\n",
            "\n",
            "    accuracy                         0.8768      2647\n",
            "   macro avg     0.8382    0.8486    0.8431      2647\n",
            "weighted avg     0.8790    0.8768    0.8778      2647\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NoF8lgzEimL6"
      },
      "source": [
        "## DistilBERT <br>\n",
        "https://huggingface.co/distilbert-base-uncased"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKfscV12ikAD",
        "outputId": "0aeb6be2-784c-465e-8ae1-368de7588fe2"
      },
      "source": [
        "from transformers import AutoTokenizer\n",
        "import torch\n",
        "from farm.modeling.tokenization import Tokenizer\n",
        "from farm.data_handler.processor import TextClassificationProcessor\n",
        "from farm.data_handler.data_silo import DataSilo\n",
        "from farm.modeling.language_model import LanguageModel\n",
        "from farm.modeling.prediction_head import TextClassificationHead\n",
        "from farm.modeling.adaptive_model import AdaptiveModel\n",
        "from farm.modeling.optimization import initialize_optimizer\n",
        "from farm.train import Trainer\n",
        "from farm.utils import MLFlowLogger\n",
        "\n",
        "# Here we initialize a tokenizer that will be used for preprocessing text\n",
        "# This is the BERT Tokenizer which uses the byte pair encoding method.\n",
        "# It is currently loaded with a German model\n",
        "\n",
        "#tokenizer = Tokenizer.load(\n",
        "#    pretrained_model_name_or_path=\"finiteautomata/bertweet-base-sentiment-analysis\",\n",
        "#    do_lower_case=False)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "# In order to prepare the data for the model, we need a set of\n",
        "# functions to transform data files into PyTorch Datasets.\n",
        "# We group these together in Processor objects.\n",
        "# We will need a new Processor object for each new source of data.\n",
        "# The abstract class can be found in farm.data_handling.processor.Processor\n",
        "# TOXIC = 1\n",
        "# OTHER = 0\n",
        "LABEL_LIST = [\"0\", \"1\"]\n",
        "processor = TextClassificationProcessor(tokenizer=tokenizer,\n",
        "                                        max_seq_len=128,\n",
        "                                        warmup = 600,\n",
        "                                        train_filename=\"train.tsv\",\n",
        "                                        test_filename=\"test_with_solutions.tsv\",\n",
        "                                        data_dir=\"../content\",\n",
        "                                        label_list=LABEL_LIST,\n",
        "                                        metric=\"f1_macro\",\n",
        "                                        text_column_name=\"Comment\",\n",
        "                                        label_column_name=\"Insult\")\n",
        "\n",
        "# We need a DataSilo in order to keep our train, dev and test sets separate.\n",
        "# The DataSilo will call the functions in the Processor to generate these sets.\n",
        "# From the DataSilo, we can fetch a PyTorch DataLoader object which will\n",
        "# be passed on to the model.\n",
        "# Here is a good place to define a batch size for the model\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "data_silo = DataSilo(\n",
        "    processor=processor,\n",
        "    batch_size=BATCH_SIZE)\n",
        "\n",
        "# The language model is the foundation on which modern NLP systems are built.\n",
        "# They encapsulate a general understanding of sentence semantics\n",
        "# and are not specific to any one task.\n",
        "\n",
        "# Here we are using Google's BERT model as implemented by HuggingFace. \n",
        "# The model being loaded is a German model that we trained. \n",
        "# You can also change the MODEL_NAME_OR_PATH to point to a BERT model that you\n",
        "# have saved or download one connected to the HuggingFace repository.\n",
        "# See farm.modeling.language_model.PRETRAINED_MODEL_ARCHIVE_MAP for a list of\n",
        "# available models\n",
        "\n",
        "MODEL_NAME_OR_PATH = \"distilbert-base-uncased\"\n",
        "# MODEL_NAME_OR_PATH = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "language_model = LanguageModel.load(MODEL_NAME_OR_PATH)\n",
        "\n",
        "# A prediction head is a model that processes the output of the language model\n",
        "# for a specific task.\n",
        "# Prediction heads will look different depending on whether you're doing text classification\n",
        "# Named Entity Recognition (NER), question answering or some other task.\n",
        "# They should generate logits over the available prediction classes and contain methods\n",
        "# to convert these logits to losses or predictions \n",
        "\n",
        "# Here we use TextClassificationHead which receives a single fixed length sentence vector\n",
        "# and processes it using a feed forward neural network. layer_dims is a list of dimensions:\n",
        "# [input_dims, hidden_1_dims, hidden_2_dims ..., output_dims]\n",
        "\n",
        "# Here by default we have a single layer network.\n",
        "# It takes in a vector of length 768 (the default size of BERT's output).\n",
        "# It outputs a vector of length 2 (the number of classes in the GermEval18 (coarse) dataset)\n",
        "\n",
        "prediction_head = TextClassificationHead(num_labels=len(LABEL_LIST), class_weights=data_silo.calculate_class_weights(task_name=\"text_classification\"))\n",
        "\n",
        "# The language model and prediction head are coupled together in the Adaptive Model.\n",
        "# This class takes care of model saving and loading and also coordinates\n",
        "# cases where there is more than one prediction head.\n",
        "\n",
        "# EMBEDS_DROPOUT_PROB is the probability that an element of the output vector from the\n",
        "# language model will be set to zero.\n",
        "# EMBEDS_DROPOUT_PROB = 0.1 distilbert-base-uncased-finetuned-sst-2-english_1.PNG\n",
        "# EMBEDS_DROPOUT_PROB = 0.01\n",
        "# EMBEDS_DROPOUT_PROB = 0.2 distilbert-base-uncased-finetuned-sst-2-english_2.PNG\n",
        "EMBEDS_DROPOUT_PROB = 0.01\n",
        "\n",
        "model = AdaptiveModel(\n",
        "    language_model=language_model,\n",
        "    prediction_heads=[prediction_head],\n",
        "    embeds_dropout_prob=EMBEDS_DROPOUT_PROB,\n",
        "    lm_output_types=[\"per_sequence\"],\n",
        "    device=device)\n",
        "\n",
        "# Here we initialize a Bert Adam optimizer that has a linear warmup and warmdown\n",
        "# Here you can set learning rate, the warmup proportion and number of epochs to train for\n",
        "\n",
        "#LEARNING_RATE = 2e-5 # distilbert-base-uncased-finetuned-sst-2-english_1.PNG\n",
        "#LEARNING_RATE = 1e-7\n",
        "LEARNING_RATE = 1e-5  # distilbert-base-uncased-finetuned-sst-2-english_2.PNG\n",
        "\n",
        "N_EPOCHS = 3\n",
        "\n",
        "model, optimizer, lr_schedule = initialize_optimizer(\n",
        "    model=model,\n",
        "    device=device,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    n_batches=len(data_silo.loaders[\"train\"]),\n",
        "    n_epochs=N_EPOCHS)\n",
        "\n",
        "# Training loop handled by this\n",
        "# It will also trigger evaluation during training using the dev data\n",
        "# and after training using the test data.\n",
        "\n",
        "# Set N_GPU to a positive value if CUDA is available\n",
        "N_GPU = 1\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    optimizer=optimizer,\n",
        "    data_silo=data_silo,\n",
        "    epochs=N_EPOCHS,\n",
        "    n_gpu=N_GPU,\n",
        "    lr_schedule=lr_schedule,\n",
        "    device=device, \n",
        ")\n",
        "\n",
        "!nvidia-smi\n",
        "\n",
        "model = trainer.train()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "09/13/2021 16:28:08 - INFO - farm.data_handler.data_silo -   \n",
            "Loading data into the data silo ... \n",
            "              ______\n",
            "               |o  |   !\n",
            "   __          |:`_|---'-.\n",
            "  |__|______.-/ _ \\-----.|       \n",
            " (o)(o)------'\\ _ /     ( )      \n",
            " \n",
            "09/13/2021 16:28:08 - INFO - farm.data_handler.data_silo -   Loading train set from: ../content/train.tsv \n",
            "09/13/2021 16:28:08 - INFO - farm.data_handler.data_silo -   Got ya 1 parallel workers to convert 3947 dictionaries to pytorch datasets (chunksize = 790)...\n",
            "09/13/2021 16:28:08 - INFO - farm.data_handler.data_silo -    0 \n",
            "09/13/2021 16:28:08 - INFO - farm.data_handler.data_silo -   /|\\\n",
            "09/13/2021 16:28:08 - INFO - farm.data_handler.data_silo -   /'\\\n",
            "09/13/2021 16:28:08 - INFO - farm.data_handler.data_silo -   \n",
            "Preprocessing Dataset ../content/train.tsv:   0%|          | 0/3947 [00:00<?, ? Dicts/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils.py:460: FutureWarning: `is_pretokenized` is deprecated and will be removed in a future version, use `is_split_into_words` instead.\n",
            "  FutureWarning,\n",
            "09/13/2021 16:28:11 - INFO - farm.data_handler.processor -   *** Show 2 random examples ***\n",
            "09/13/2021 16:28:11 - INFO - farm.data_handler.processor -   \n",
            "\n",
            "      .--.        _____                       _      \n",
            "    .'_\\/_'.     / ____|                     | |     \n",
            "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
            "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
            "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
            "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
            "   (/\\||/                             |_|           \n",
            "______\\||/___________________________________________                     \n",
            "\n",
            "ID: 509-0\n",
            "Clear Text: \n",
            " \ttext_classification_label: 1\n",
            " \ttext: \"LMAO.. You are another clueless moron....only liberal women are kept..they have the masters..\"\n",
            "Tokenized: \n",
            " \ttokens: ['\"', 'l', '##ma', '##o', '.', '.', 'you', 'are', 'another', 'clue', '##less', 'mor', '##on', '.', '.', '.', '.', 'only', 'liberal', 'women', 'are', 'kept', '.', '.', 'they', 'have', 'the', 'masters', '.', '.', '\"']\n",
            " \toffsets: [0, 1, 2, 4, 5, 6, 8, 12, 16, 24, 28, 33, 36, 38, 39, 40, 41, 42, 47, 55, 61, 65, 69, 70, 71, 76, 81, 85, 92, 93, 94]\n",
            " \tstart_of_word: [True, False, False, False, False, False, True, True, True, True, False, True, False, False, False, False, False, False, True, True, True, True, False, False, False, True, True, True, False, False, False]\n",
            "Features: \n",
            " \tinput_ids: [101, 1000, 1048, 2863, 2080, 1012, 1012, 2017, 2024, 2178, 9789, 3238, 22822, 2239, 1012, 1012, 1012, 1012, 2069, 4314, 2308, 2024, 2921, 1012, 1012, 2027, 2031, 1996, 5972, 1012, 1012, 1000, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \ttext_classification_label_ids: [1]\n",
            "_____________________________________________________\n",
            "09/13/2021 16:28:11 - INFO - farm.data_handler.processor -   \n",
            "\n",
            "      .--.        _____                       _      \n",
            "    .'_\\/_'.     / ____|                     | |     \n",
            "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
            "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
            "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
            "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
            "   (/\\||/                             |_|           \n",
            "______\\||/___________________________________________                     \n",
            "\n",
            "ID: 699-0\n",
            "Clear Text: \n",
            " \ttext_classification_label: 0\n",
            " \ttext: \"very sorry about robin, however I can guarantee that if this was a story about just your average person it wouldn't be getting this much attention. sorry if its offensive but tough shit its the truth and you all know it.\"\n",
            "Tokenized: \n",
            " \ttokens: ['\"', 'very', 'sorry', 'about', 'robin', ',', 'however', 'i', 'can', 'guarantee', 'that', 'if', 'this', 'was', 'a', 'story', 'about', 'just', 'your', 'average', 'person', 'it', 'wouldn', \"'\", 't', 'be', 'getting', 'this', 'much', 'attention', '.', 'sorry', 'if', 'its', 'offensive', 'but', 'tough', 'shit', 'its', 'the', 'truth', 'and', 'you', 'all', 'know', 'it', '.', '\"']\n",
            " \toffsets: [0, 1, 6, 12, 18, 23, 25, 33, 35, 39, 49, 54, 57, 62, 66, 68, 74, 80, 85, 90, 98, 105, 108, 114, 115, 117, 120, 128, 133, 138, 147, 149, 155, 158, 162, 172, 176, 182, 187, 191, 195, 201, 205, 209, 213, 218, 220, 221]\n",
            " \tstart_of_word: [True, False, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False]\n",
            "Features: \n",
            " \tinput_ids: [101, 1000, 2200, 3374, 2055, 5863, 1010, 2174, 1045, 2064, 11302, 2008, 2065, 2023, 2001, 1037, 2466, 2055, 2074, 2115, 2779, 2711, 2009, 2876, 1005, 1056, 2022, 2893, 2023, 2172, 3086, 1012, 3374, 2065, 2049, 5805, 2021, 7823, 4485, 2049, 1996, 3606, 1998, 2017, 2035, 2113, 2009, 1012, 1000, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \ttext_classification_label_ids: [0]\n",
            "_____________________________________________________\n",
            "Preprocessing Dataset ../content/train.tsv: 100%|██████████| 3947/3947 [00:13<00:00, 292.04 Dicts/s]\n",
            "09/13/2021 16:28:22 - INFO - farm.data_handler.data_silo -   Loading dev set as a slice of train set\n",
            "09/13/2021 16:28:22 - INFO - farm.data_handler.data_silo -   Took 787 samples out of train set to create dev set (dev split is roughly 0.1)\n",
            "09/13/2021 16:28:22 - INFO - farm.data_handler.data_silo -   Loading test set from: ../content/test_with_solutions.tsv\n",
            "09/13/2021 16:28:22 - INFO - farm.data_handler.data_silo -   Got ya 1 parallel workers to convert 2647 dictionaries to pytorch datasets (chunksize = 530)...\n",
            "09/13/2021 16:28:22 - INFO - farm.data_handler.data_silo -    0 \n",
            "09/13/2021 16:28:22 - INFO - farm.data_handler.data_silo -   /w\\\n",
            "09/13/2021 16:28:22 - INFO - farm.data_handler.data_silo -   /'\\\n",
            "09/13/2021 16:28:22 - INFO - farm.data_handler.data_silo -   \n",
            "Preprocessing Dataset ../content/test_with_solutions.tsv:   0%|          | 0/2647 [00:00<?, ? Dicts/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils.py:460: FutureWarning: `is_pretokenized` is deprecated and will be removed in a future version, use `is_split_into_words` instead.\n",
            "  FutureWarning,\n",
            "09/13/2021 16:28:25 - INFO - farm.data_handler.processor -   *** Show 2 random examples ***\n",
            "09/13/2021 16:28:25 - INFO - farm.data_handler.processor -   \n",
            "\n",
            "      .--.        _____                       _      \n",
            "    .'_\\/_'.     / ____|                     | |     \n",
            "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
            "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
            "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
            "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
            "   (/\\||/                             |_|           \n",
            "______\\||/___________________________________________                     \n",
            "\n",
            "ID: 109-0\n",
            "Clear Text: \n",
            " \ttext_classification_label: 0\n",
            " \ttext: \"OK, who scored, who died today?\"\n",
            "Tokenized: \n",
            " \ttokens: ['\"', 'ok', ',', 'who', 'scored', ',', 'who', 'died', 'today', '?', '\"']\n",
            " \toffsets: [0, 1, 3, 5, 9, 15, 17, 21, 26, 31, 32]\n",
            " \tstart_of_word: [True, False, False, True, True, False, True, True, True, False, False]\n",
            "Features: \n",
            " \tinput_ids: [101, 1000, 7929, 1010, 2040, 3195, 1010, 2040, 2351, 2651, 1029, 1000, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \ttext_classification_label_ids: [0]\n",
            "_____________________________________________________\n",
            "09/13/2021 16:28:25 - INFO - farm.data_handler.processor -   \n",
            "\n",
            "      .--.        _____                       _      \n",
            "    .'_\\/_'.     / ____|                     | |     \n",
            "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
            "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
            "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
            "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
            "   (/\\||/                             |_|           \n",
            "______\\||/___________________________________________                     \n",
            "\n",
            "ID: 360-0\n",
            "Clear Text: \n",
            " \ttext_classification_label: 0\n",
            " \ttext: \"doesn't matter who you are you could be jay z or nas or dr dre who ever if you fuck up long enough then do some shit like this your no longer a legend period lol\"\n",
            "Tokenized: \n",
            " \ttokens: ['\"', 'doesn', \"'\", 't', 'matter', 'who', 'you', 'are', 'you', 'could', 'be', 'jay', 'z', 'or', 'nas', 'or', 'dr', 'dr', '##e', 'who', 'ever', 'if', 'you', 'fuck', 'up', 'long', 'enough', 'then', 'do', 'some', 'shit', 'like', 'this', 'your', 'no', 'longer', 'a', 'legend', 'period', 'lo', '##l', '\"']\n",
            " \toffsets: [0, 1, 6, 7, 9, 16, 20, 24, 28, 32, 38, 41, 45, 47, 50, 54, 57, 60, 62, 64, 68, 73, 76, 80, 85, 88, 93, 100, 105, 108, 113, 118, 123, 128, 133, 136, 143, 145, 152, 159, 161, 162]\n",
            " \tstart_of_word: [True, False, False, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False]\n",
            "Features: \n",
            " \tinput_ids: [101, 1000, 2987, 1005, 1056, 3043, 2040, 2017, 2024, 2017, 2071, 2022, 6108, 1062, 2030, 17235, 2030, 2852, 2852, 2063, 2040, 2412, 2065, 2017, 6616, 2039, 2146, 2438, 2059, 2079, 2070, 4485, 2066, 2023, 2115, 2053, 2936, 1037, 5722, 2558, 8840, 2140, 1000, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \ttext_classification_label_ids: [0]\n",
            "_____________________________________________________\n",
            "Preprocessing Dataset ../content/test_with_solutions.tsv: 100%|██████████| 2647/2647 [00:10<00:00, 256.03 Dicts/s]\n",
            "09/13/2021 16:28:32 - INFO - farm.data_handler.data_silo -   Examples in train: 3160\n",
            "09/13/2021 16:28:32 - INFO - farm.data_handler.data_silo -   Examples in dev  : 787\n",
            "09/13/2021 16:28:32 - INFO - farm.data_handler.data_silo -   Examples in test : 2647\n",
            "09/13/2021 16:28:32 - INFO - farm.data_handler.data_silo -   \n",
            "09/13/2021 16:28:32 - INFO - farm.data_handler.data_silo -   Longest sequence length observed after clipping:     128\n",
            "09/13/2021 16:28:32 - INFO - farm.data_handler.data_silo -   Average sequence length after clipping: 44.04588607594937\n",
            "09/13/2021 16:28:33 - INFO - farm.data_handler.data_silo -   Proportion clipped:      0.07658227848101266\n",
            "09/13/2021 16:28:33 - WARNING - farm.utils -   Failed to log params: INVALID_PARAMETER_VALUE: Changing param value is not allowed. Param with key='ave_seq_len' was already logged with value='43.41107594936709' for run ID='6543ccfdbba146ca82a34b6de1bbb983. Attempted logging new value '44.04588607594937'.\n",
            "09/13/2021 16:28:35 - WARNING - farm.modeling.language_model -   Could not automatically detect from language model name what language it is. \n",
            "\t We guess it's an *ENGLISH* model ... \n",
            "\t If not: Init the language model by supplying the 'language' param.\n",
            "09/13/2021 16:28:35 - INFO - farm.modeling.prediction_head -   Prediction head initialized with size [768, 2]\n",
            "09/13/2021 16:28:35 - INFO - farm.modeling.prediction_head -   Using class weights for task 'text_classification': [0.67970765 1.8911483 ]\n",
            "09/13/2021 16:28:35 - WARNING - farm.utils -   Failed to log params: INVALID_PARAMETER_VALUE: Changing param value is not allowed. Param with key='lm_name' was already logged with value='distilbert-base-uncased-finetuned-sst-2-english' for run ID='6543ccfdbba146ca82a34b6de1bbb983. Attempted logging new value 'distilbert-base-uncased'.\n",
            "09/13/2021 16:28:35 - INFO - farm.modeling.optimization -   Loading optimizer `TransformersAdamW`: '{'correct_bias': False, 'weight_decay': 0.01, 'lr': 1e-05}'\n",
            "09/13/2021 16:28:36 - INFO - farm.modeling.optimization -   Using scheduler 'get_linear_schedule_with_warmup'\n",
            "09/13/2021 16:28:36 - INFO - farm.modeling.optimization -   Loading schedule `get_linear_schedule_with_warmup`: '{'num_warmup_steps': 29.700000000000003, 'num_training_steps': 297}'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Sep 13 16:28:36 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.63.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   45C    P0    60W / 149W |   6881MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "09/13/2021 16:28:36 - INFO - farm.train -   \n",
            " \n",
            "\n",
            "          &&& &&  & &&             _____                   _             \n",
            "      && &\\/&\\|& ()|/ @, &&       / ____|                 (_)            \n",
            "      &\\/(/&/&||/& /_/)_&/_&     | |  __ _ __ _____      ___ _ __   __ _ \n",
            "   &() &\\/&|()|/&\\/ '%\" & ()     | | |_ | '__/ _ \\ \\ /\\ / / | '_ \\ / _` |\n",
            "  &_\\_&&_\\ |& |&&/&__%_/_& &&    | |__| | | | (_) \\ V  V /| | | | | (_| |\n",
            "&&   && & &| &| /& & % ()& /&&    \\_____|_|  \\___/ \\_/\\_/ |_|_| |_|\\__, |\n",
            " ()&_---()&\\&\\|&&-&&--%---()~                                       __/ |\n",
            "     &&     \\|||                                                   |___/\n",
            "             |||\n",
            "             |||\n",
            "             |||\n",
            "       , -=-~  .-^- _\n",
            "              `\n",
            "\n",
            "Train epoch 0/2 (Cur. train loss: 0.2932): 100%|██████████| 99/99 [01:02<00:00,  1.57it/s]\n",
            "Train epoch 1/2 (Cur. train loss: 0.4485):   1%|          | 1/99 [00:00<01:00,  1.63it/s]\n",
            "Evaluating: 100%|██████████| 25/25 [00:05<00:00,  4.58it/s]\n",
            "09/13/2021 16:29:46 - INFO - farm.eval -   \n",
            "\n",
            "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "***************************************************\n",
            "***** EVALUATION | DEV SET | AFTER 100 BATCHES *****\n",
            "***************************************************\n",
            "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "\n",
            "09/13/2021 16:29:46 - INFO - farm.eval -   \n",
            " _________ text_classification _________\n",
            "09/13/2021 16:29:46 - INFO - farm.eval -   loss: 0.41160049038587715\n",
            "09/13/2021 16:29:46 - INFO - farm.eval -   task_name: text_classification\n",
            "09/13/2021 16:29:46 - INFO - farm.eval -   f1_macro: 0.8167235596779491\n",
            "09/13/2021 16:29:46 - INFO - farm.eval -   report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9101    0.8831    0.8964       573\n",
            "           1     0.7100    0.7664    0.7371       214\n",
            "\n",
            "    accuracy                         0.8513       787\n",
            "   macro avg     0.8100    0.8247    0.8167       787\n",
            "weighted avg     0.8557    0.8513    0.8531       787\n",
            "\n",
            "Train epoch 1/2 (Cur. train loss: 0.3282): 100%|██████████| 99/99 [01:08<00:00,  1.46it/s]\n",
            "Train epoch 2/2 (Cur. train loss: 0.0590):   2%|▏         | 2/99 [00:01<01:00,  1.59it/s]\n",
            "Evaluating: 100%|██████████| 25/25 [00:05<00:00,  4.53it/s]\n",
            "09/13/2021 16:30:54 - INFO - farm.eval -   \n",
            "\n",
            "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "***************************************************\n",
            "***** EVALUATION | DEV SET | AFTER 200 BATCHES *****\n",
            "***************************************************\n",
            "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "\n",
            "09/13/2021 16:30:54 - INFO - farm.eval -   \n",
            " _________ text_classification _________\n",
            "09/13/2021 16:30:55 - INFO - farm.eval -   loss: 0.46831573646283664\n",
            "09/13/2021 16:30:55 - INFO - farm.eval -   task_name: text_classification\n",
            "09/13/2021 16:30:55 - INFO - farm.eval -   f1_macro: 0.8183597081874103\n",
            "09/13/2021 16:30:55 - INFO - farm.eval -   report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9075    0.8901    0.8987       573\n",
            "           1     0.7200    0.7570    0.7380       214\n",
            "\n",
            "    accuracy                         0.8539       787\n",
            "   macro avg     0.8137    0.8235    0.8184       787\n",
            "weighted avg     0.8565    0.8539    0.8550       787\n",
            "\n",
            "Train epoch 2/2 (Cur. train loss: 0.0175): 100%|██████████| 99/99 [01:07<00:00,  1.46it/s]\n",
            "Evaluating: 100%|██████████| 83/83 [00:18<00:00,  4.52it/s]\n",
            "09/13/2021 16:32:13 - INFO - farm.eval -   \n",
            "\n",
            "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "***************************************************\n",
            "***** EVALUATION | TEST SET | AFTER 297 BATCHES *****\n",
            "***************************************************\n",
            "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "\n",
            "09/13/2021 16:32:13 - INFO - farm.eval -   \n",
            " _________ text_classification _________\n",
            "09/13/2021 16:32:13 - INFO - farm.eval -   loss: 0.5051796574569172\n",
            "09/13/2021 16:32:13 - INFO - farm.eval -   task_name: text_classification\n",
            "09/13/2021 16:32:14 - INFO - farm.eval -   f1_macro: 0.8463072209522897\n",
            "09/13/2021 16:32:14 - INFO - farm.eval -   report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9131    0.9304    0.9217      1954\n",
            "           1     0.7927    0.7504    0.7709       693\n",
            "\n",
            "    accuracy                         0.8833      2647\n",
            "   macro avg     0.8529    0.8404    0.8463      2647\n",
            "weighted avg     0.8816    0.8833    0.8822      2647\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aiUubEaJiBus"
      },
      "source": [
        "## XLMRoBERTa <br>\n",
        "https://huggingface.co/xlm-roberta-base "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pmM3w0uviEls",
        "outputId": "b9bf8bae-c406-4da1-d26d-47f231d76ac8"
      },
      "source": [
        "from transformers import AutoTokenizer\n",
        "import torch\n",
        "from farm.modeling.tokenization import Tokenizer\n",
        "from farm.data_handler.processor import TextClassificationProcessor\n",
        "from farm.data_handler.data_silo import DataSilo\n",
        "from farm.modeling.language_model import LanguageModel\n",
        "from farm.modeling.prediction_head import TextClassificationHead\n",
        "from farm.modeling.adaptive_model import AdaptiveModel\n",
        "from farm.modeling.optimization import initialize_optimizer\n",
        "from farm.train import Trainer\n",
        "from farm.utils import MLFlowLogger\n",
        "\n",
        "# Here we initialize a tokenizer that will be used for preprocessing text\n",
        "# This is the BERT Tokenizer which uses the byte pair encoding method.\n",
        "# It is currently loaded with a German model\n",
        "\n",
        "#tokenizer = Tokenizer.load(\n",
        "#    pretrained_model_name_or_path=\"finiteautomata/bertweet-base-sentiment-analysis\",\n",
        "#    do_lower_case=False)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
        "\n",
        "# In order to prepare the data for the model, we need a set of\n",
        "# functions to transform data files into PyTorch Datasets.\n",
        "# We group these together in Processor objects.\n",
        "# We will need a new Processor object for each new source of data.\n",
        "# The abstract class can be found in farm.data_handling.processor.Processor\n",
        "# TOXIC = 1\n",
        "# OTHER = 0\n",
        "LABEL_LIST = [\"0\", \"1\"]\n",
        "processor = TextClassificationProcessor(tokenizer=tokenizer,\n",
        "                                        max_seq_len=128,\n",
        "                                        warmup = 600,\n",
        "                                        train_filename=\"train.tsv\",\n",
        "                                        test_filename=\"test_with_solutions.tsv\",\n",
        "                                        data_dir=\"../content\",\n",
        "                                        label_list=LABEL_LIST,\n",
        "                                        metric=\"f1_macro\",\n",
        "                                        text_column_name=\"Comment\",\n",
        "                                        label_column_name=\"Insult\")\n",
        "\n",
        "# We need a DataSilo in order to keep our train, dev and test sets separate.\n",
        "# The DataSilo will call the functions in the Processor to generate these sets.\n",
        "# From the DataSilo, we can fetch a PyTorch DataLoader object which will\n",
        "# be passed on to the model.\n",
        "# Here is a good place to define a batch size for the model\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "data_silo = DataSilo(\n",
        "    processor=processor,\n",
        "    batch_size=BATCH_SIZE)\n",
        "\n",
        "# The language model is the foundation on which modern NLP systems are built.\n",
        "# They encapsulate a general understanding of sentence semantics\n",
        "# and are not specific to any one task.\n",
        "\n",
        "# Here we are using Google's BERT model as implemented by HuggingFace. \n",
        "# The model being loaded is a German model that we trained. \n",
        "# You can also change the MODEL_NAME_OR_PATH to point to a BERT model that you\n",
        "# have saved or download one connected to the HuggingFace repository.\n",
        "# See farm.modeling.language_model.PRETRAINED_MODEL_ARCHIVE_MAP for a list of\n",
        "# available models\n",
        "\n",
        "MODEL_NAME_OR_PATH = \"xlm-roberta-base\"\n",
        "# MODEL_NAME_OR_PATH = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "language_model = LanguageModel.load(MODEL_NAME_OR_PATH)\n",
        "\n",
        "# A prediction head is a model that processes the output of the language model\n",
        "# for a specific task.\n",
        "# Prediction heads will look different depending on whether you're doing text classification\n",
        "# Named Entity Recognition (NER), question answering or some other task.\n",
        "# They should generate logits over the available prediction classes and contain methods\n",
        "# to convert these logits to losses or predictions \n",
        "\n",
        "# Here we use TextClassificationHead which receives a single fixed length sentence vector\n",
        "# and processes it using a feed forward neural network. layer_dims is a list of dimensions:\n",
        "# [input_dims, hidden_1_dims, hidden_2_dims ..., output_dims]\n",
        "\n",
        "# Here by default we have a single layer network.\n",
        "# It takes in a vector of length 768 (the default size of BERT's output).\n",
        "# It outputs a vector of length 2 (the number of classes in the GermEval18 (coarse) dataset)\n",
        "\n",
        "prediction_head = TextClassificationHead(num_labels=len(LABEL_LIST), class_weights=data_silo.calculate_class_weights(task_name=\"text_classification\"))\n",
        "\n",
        "# The language model and prediction head are coupled together in the Adaptive Model.\n",
        "# This class takes care of model saving and loading and also coordinates\n",
        "# cases where there is more than one prediction head.\n",
        "\n",
        "# EMBEDS_DROPOUT_PROB is the probability that an element of the output vector from the\n",
        "# language model will be set to zero.\n",
        "# EMBEDS_DROPOUT_PROB = 0.1 distilbert-base-uncased-finetuned-sst-2-english_1.PNG\n",
        "# EMBEDS_DROPOUT_PROB = 0.01\n",
        "# EMBEDS_DROPOUT_PROB = 0.2 distilbert-base-uncased-finetuned-sst-2-english_2.PNG\n",
        "EMBEDS_DROPOUT_PROB = 0.01\n",
        "\n",
        "model = AdaptiveModel(\n",
        "    language_model=language_model,\n",
        "    prediction_heads=[prediction_head],\n",
        "    embeds_dropout_prob=EMBEDS_DROPOUT_PROB,\n",
        "    lm_output_types=[\"per_sequence\"],\n",
        "    device=device)\n",
        "\n",
        "# Here we initialize a Bert Adam optimizer that has a linear warmup and warmdown\n",
        "# Here you can set learning rate, the warmup proportion and number of epochs to train for\n",
        "\n",
        "#LEARNING_RATE = 2e-5 # distilbert-base-uncased-finetuned-sst-2-english_1.PNG\n",
        "#LEARNING_RATE = 1e-7\n",
        "LEARNING_RATE = 1e-5  # distilbert-base-uncased-finetuned-sst-2-english_2.PNG\n",
        "\n",
        "N_EPOCHS = 3\n",
        "\n",
        "model, optimizer, lr_schedule = initialize_optimizer(\n",
        "    model=model,\n",
        "    device=device,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    n_batches=len(data_silo.loaders[\"train\"]),\n",
        "    n_epochs=N_EPOCHS)\n",
        "\n",
        "# Training loop handled by this\n",
        "# It will also trigger evaluation during training using the dev data\n",
        "# and after training using the test data.\n",
        "\n",
        "# Set N_GPU to a positive value if CUDA is available\n",
        "N_GPU = 1\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    optimizer=optimizer,\n",
        "    data_silo=data_silo,\n",
        "    epochs=N_EPOCHS,\n",
        "    n_gpu=N_GPU,\n",
        "    lr_schedule=lr_schedule,\n",
        "    device=device, \n",
        ")\n",
        "\n",
        "!nvidia-smi\n",
        "\n",
        "model = trainer.train()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "09/13/2021 16:32:15 - WARNING - farm.utils -   Failed to log params: INVALID_PARAMETER_VALUE: Changing param value is not allowed. Param with key='tokenizer' was already logged with value='DistilBertTokenizer' for run ID='6543ccfdbba146ca82a34b6de1bbb983. Attempted logging new value 'XLMRobertaTokenizer'.\n",
            "09/13/2021 16:32:15 - INFO - farm.data_handler.data_silo -   \n",
            "Loading data into the data silo ... \n",
            "              ______\n",
            "               |o  |   !\n",
            "   __          |:`_|---'-.\n",
            "  |__|______.-/ _ \\-----.|       \n",
            " (o)(o)------'\\ _ /     ( )      \n",
            " \n",
            "09/13/2021 16:32:15 - INFO - farm.data_handler.data_silo -   Loading train set from: ../content/train.tsv \n",
            "09/13/2021 16:32:15 - INFO - farm.data_handler.data_silo -   Got ya 1 parallel workers to convert 3947 dictionaries to pytorch datasets (chunksize = 790)...\n",
            "09/13/2021 16:32:15 - INFO - farm.data_handler.data_silo -    0 \n",
            "09/13/2021 16:32:15 - INFO - farm.data_handler.data_silo -   /w\\\n",
            "09/13/2021 16:32:15 - INFO - farm.data_handler.data_silo -   /'\\\n",
            "09/13/2021 16:32:15 - INFO - farm.data_handler.data_silo -   \n",
            "Preprocessing Dataset ../content/train.tsv:   0%|          | 0/3947 [00:00<?, ? Dicts/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils.py:460: FutureWarning: `is_pretokenized` is deprecated and will be removed in a future version, use `is_split_into_words` instead.\n",
            "  FutureWarning,\n",
            "09/13/2021 16:32:17 - INFO - farm.data_handler.processor -   *** Show 2 random examples ***\n",
            "09/13/2021 16:32:17 - INFO - farm.data_handler.processor -   \n",
            "\n",
            "      .--.        _____                       _      \n",
            "    .'_\\/_'.     / ____|                     | |     \n",
            "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
            "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
            "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
            "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
            "   (/\\||/                             |_|           \n",
            "______\\||/___________________________________________                     \n",
            "\n",
            "ID: 702-0\n",
            "Clear Text: \n",
            " \ttext_classification_label: 1\n",
            " \ttext: \"you are sickening and pathetic !!!!!!!!!!!!!!\"\n",
            "Tokenized: \n",
            " \ttokens: ['▁\"', 'you', '▁are', '▁si', 'cken', 'ing', '▁and', '▁pa', 'the', 'tic', '▁!!', '!!!!!!!!!!!!', '\"']\n",
            " \toffsets: [0, 1, 5, 9, 11, 15, 19, 23, 25, 28, 32, 34, 46]\n",
            " \tstart_of_word: [True, False, True, True, False, False, True, True, False, False, True, False, False]\n",
            "Features: \n",
            " \tinput_ids: [0, 44, 53927, 621, 78, 11960, 214, 136, 249, 2347, 9523, 6506, 191783, 58, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \ttext_classification_label_ids: [1]\n",
            "_____________________________________________________\n",
            "09/13/2021 16:32:17 - INFO - farm.data_handler.processor -   \n",
            "\n",
            "      .--.        _____                       _      \n",
            "    .'_\\/_'.     / ____|                     | |     \n",
            "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
            "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
            "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
            "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
            "   (/\\||/                             |_|           \n",
            "______\\||/___________________________________________                     \n",
            "\n",
            "ID: 268-0\n",
            "Clear Text: \n",
            " \ttext_classification_label: 0\n",
            " \ttext: \"\"I feel sorry for other grandparents that didnt have love and respect of their grandchildren.\"\\n---------------------------------------\\nHold it right there, grandma.\\n\\nIs a hug \"love,\" or is it \"respect\"? A child can show you proper respect without hugging you. Your inability to discern the distinction between the two is what's holding you back from understanding this column.\"\n",
            "Tokenized: \n",
            " \ttokens: ['▁\"', '\"', 'I', '▁feel', '▁sorry', '▁for', '▁other', '▁grand', 'parent', 's', '▁that', '▁didn', 't', '▁have', '▁love', '▁and', '▁respect', '▁of', '▁their', '▁grand', 'children', '.\"', '\\\\', 'n', '------', '-', '----------------', '----------------', '\\\\', 'n', 'Hol', 'd', '▁it', '▁right', '▁there', ',', '▁grand', 'ma', '.', '\\\\', 'n', '\\\\', 'n', 'Is', '▁a', '▁hug', '▁\"', 'love', ',', '\"', '▁or', '▁is', '▁it', '▁\"', 'respect', '\"?', '▁A', '▁child', '▁can', '▁show', '▁you', '▁proper', '▁respect', '▁without', '▁hu', 'gging', '▁you', '.', '▁Your', '▁in', 'ability', '▁to', '▁discern', '▁the', '▁distinctio', 'n', '▁between', '▁the', '▁two', '▁is', '▁what', \"'\", 's', '▁holding', '▁you', '▁back', '▁from', '▁understanding', '▁this', '▁col', 'um', 'n', '.\"']\n",
            " \toffsets: [0, 1, 2, 4, 9, 15, 19, 25, 30, 36, 38, 43, 47, 49, 54, 59, 63, 71, 74, 80, 85, 93, 95, 96, 97, 103, 104, 120, 136, 137, 138, 141, 143, 146, 152, 157, 159, 164, 166, 167, 168, 169, 170, 171, 174, 176, 180, 181, 185, 186, 188, 191, 194, 197, 198, 205, 208, 210, 216, 220, 225, 229, 236, 244, 252, 254, 260, 263, 265, 270, 272, 280, 283, 291, 295, 305, 307, 315, 319, 323, 326, 330, 331, 333, 341, 345, 350, 355, 369, 374, 377, 379, 380]\n",
            " \tstart_of_word: [True, False, False, True, True, True, True, True, False, False, True, True, False, True, True, True, True, True, True, True, False, False, False, False, False, False, False, False, False, False, False, False, True, True, True, False, True, False, False, False, False, False, False, False, True, True, True, False, False, False, True, True, True, True, False, False, True, True, True, True, True, True, True, True, True, False, True, False, True, True, False, True, True, True, True, False, True, True, True, True, True, False, False, True, True, True, True, True, True, True, False, False, False]\n",
            "Features: \n",
            " \tinput_ids: [0, 44, 58, 568, 12319, 59090, 100, 3789, 9963, 99547, 7, 450, 15935, 18, 765, 5161, 136, 15072, 111, 2363, 9963, 237052, 1242, 41872, 19, 110405, 9, 103428, 103428, 41872, 19, 113251, 71, 442, 7108, 2685, 4, 9963, 192, 5, 41872, 19, 41872, 19, 29598, 10, 35875, 44, 25003, 4, 58, 707, 83, 442, 44, 177981, 38843, 62, 29041, 831, 7639, 398, 27798, 15072, 15490, 4571, 36659, 398, 5, 14804, 23, 41159, 47, 205556, 70, 149067, 19, 17721, 70, 6626, 83, 2367, 25, 7, 104064, 398, 4420, 1295, 100094, 903, 3365, 316, 19, 1242, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \ttext_classification_label_ids: [0]\n",
            "_____________________________________________________\n",
            "Preprocessing Dataset ../content/train.tsv: 100%|██████████| 3947/3947 [00:10<00:00, 392.33 Dicts/s]\n",
            "09/13/2021 16:32:25 - INFO - farm.data_handler.data_silo -   Loading dev set as a slice of train set\n",
            "09/13/2021 16:32:25 - INFO - farm.data_handler.data_silo -   Took 787 samples out of train set to create dev set (dev split is roughly 0.1)\n",
            "09/13/2021 16:32:25 - INFO - farm.data_handler.data_silo -   Loading test set from: ../content/test_with_solutions.tsv\n",
            "09/13/2021 16:32:25 - INFO - farm.data_handler.data_silo -   Got ya 1 parallel workers to convert 2647 dictionaries to pytorch datasets (chunksize = 530)...\n",
            "09/13/2021 16:32:25 - INFO - farm.data_handler.data_silo -    0 \n",
            "09/13/2021 16:32:25 - INFO - farm.data_handler.data_silo -   /w\\\n",
            "09/13/2021 16:32:25 - INFO - farm.data_handler.data_silo -   /'\\\n",
            "09/13/2021 16:32:25 - INFO - farm.data_handler.data_silo -   \n",
            "Preprocessing Dataset ../content/test_with_solutions.tsv:   0%|          | 0/2647 [00:00<?, ? Dicts/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils.py:460: FutureWarning: `is_pretokenized` is deprecated and will be removed in a future version, use `is_split_into_words` instead.\n",
            "  FutureWarning,\n",
            "09/13/2021 16:32:28 - INFO - farm.data_handler.processor -   *** Show 2 random examples ***\n",
            "09/13/2021 16:32:28 - INFO - farm.data_handler.processor -   \n",
            "\n",
            "      .--.        _____                       _      \n",
            "    .'_\\/_'.     / ____|                     | |     \n",
            "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
            "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
            "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
            "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
            "   (/\\||/                             |_|           \n",
            "______\\||/___________________________________________                     \n",
            "\n",
            "ID: 107-0\n",
            "Clear Text: \n",
            " \ttext_classification_label: 1\n",
            " \ttext: \"stretch her ass open mah nigguh spit in her asshole and put ya dick deep in it give her an ANAL INFECTION BRUH!!!\"\n",
            "Tokenized: \n",
            " \ttokens: ['▁\"', 'stret', 'ch', '▁her', '▁ass', '▁open', '▁mah', '▁ni', 'gg', 'uh', '▁spi', 't', '▁in', '▁her', '▁ass', 'hole', '▁and', '▁put', '▁ya', '▁dick', '▁deep', '▁in', '▁it', '▁give', '▁her', '▁an', '▁ANAL', '▁IN', 'FE', 'C', 'TION', '▁BRU', 'H', '!!!', '\"']\n",
            " \toffsets: [0, 1, 6, 9, 13, 17, 22, 26, 28, 30, 33, 36, 38, 41, 45, 48, 53, 57, 61, 64, 69, 74, 77, 80, 85, 89, 92, 97, 99, 101, 102, 107, 110, 111, 114]\n",
            " \tstart_of_word: [True, False, False, True, True, True, True, True, False, False, True, False, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, False, False, False, True, False, False, False]\n",
            "Features: \n",
            " \tinput_ids: [0, 44, 50768, 206, 604, 20751, 9803, 7356, 300, 9815, 5951, 16195, 18, 23, 604, 20751, 70919, 136, 3884, 151, 99511, 53894, 23, 442, 8337, 604, 142, 173671, 5881, 30018, 441, 35213, 157382, 841, 1564, 58, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \ttext_classification_label_ids: [1]\n",
            "_____________________________________________________\n",
            "09/13/2021 16:32:28 - INFO - farm.data_handler.processor -   \n",
            "\n",
            "      .--.        _____                       _      \n",
            "    .'_\\/_'.     / ____|                     | |     \n",
            "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
            "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
            "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
            "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
            "   (/\\||/                             |_|           \n",
            "______\\||/___________________________________________                     \n",
            "\n",
            "ID: 33-0\n",
            "Clear Text: \n",
            " \ttext_classification_label: 0\n",
            " \ttext: \"Is that you, Yannick??????? Give it a rest.\"\n",
            "Tokenized: \n",
            " \ttokens: ['▁\"', 'Is', '▁that', '▁you', ',', '▁Yan', 'nick', '???????', '▁Give', '▁it', '▁a', '▁rest', '.\"']\n",
            " \toffsets: [0, 1, 4, 9, 12, 14, 17, 21, 29, 34, 37, 39, 43]\n",
            " \tstart_of_word: [True, False, True, True, False, True, False, False, True, True, True, True, False]\n",
            "Features: \n",
            " \tinput_ids: [0, 44, 29598, 450, 398, 4, 25970, 67127, 132985, 77878, 442, 10, 10588, 1242, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \ttext_classification_label_ids: [0]\n",
            "_____________________________________________________\n",
            "Preprocessing Dataset ../content/test_with_solutions.tsv: 100%|██████████| 2647/2647 [00:08<00:00, 321.82 Dicts/s]\n",
            "09/13/2021 16:32:34 - INFO - farm.data_handler.data_silo -   Examples in train: 3160\n",
            "09/13/2021 16:32:34 - INFO - farm.data_handler.data_silo -   Examples in dev  : 787\n",
            "09/13/2021 16:32:34 - INFO - farm.data_handler.data_silo -   Examples in test : 2647\n",
            "09/13/2021 16:32:34 - INFO - farm.data_handler.data_silo -   \n",
            "09/13/2021 16:32:34 - INFO - farm.data_handler.data_silo -   Longest sequence length observed after clipping:     128\n",
            "09/13/2021 16:32:34 - INFO - farm.data_handler.data_silo -   Average sequence length after clipping: 44.81677215189873\n",
            "09/13/2021 16:32:34 - INFO - farm.data_handler.data_silo -   Proportion clipped:      0.08132911392405064\n",
            "09/13/2021 16:32:34 - WARNING - farm.utils -   Failed to log params: INVALID_PARAMETER_VALUE: Changing param value is not allowed. Param with key='ave_seq_len' was already logged with value='43.41107594936709' for run ID='6543ccfdbba146ca82a34b6de1bbb983. Attempted logging new value '44.81677215189873'.\n",
            "09/13/2021 16:32:43 - WARNING - farm.modeling.language_model -   Could not automatically detect from language model name what language it is. \n",
            "\t We guess it's an *ENGLISH* model ... \n",
            "\t If not: Init the language model by supplying the 'language' param.\n",
            "09/13/2021 16:32:43 - INFO - farm.modeling.prediction_head -   Prediction head initialized with size [768, 2]\n",
            "09/13/2021 16:32:43 - INFO - farm.modeling.prediction_head -   Using class weights for task 'text_classification': [0.6814655 1.8776722]\n",
            "09/13/2021 16:32:43 - WARNING - farm.utils -   Failed to log params: INVALID_PARAMETER_VALUE: Changing param value is not allowed. Param with key='lm_type' was already logged with value='DistilBert' for run ID='6543ccfdbba146ca82a34b6de1bbb983. Attempted logging new value 'XLMRoberta'.\n",
            "09/13/2021 16:32:43 - INFO - farm.modeling.optimization -   Loading optimizer `TransformersAdamW`: '{'correct_bias': False, 'weight_decay': 0.01, 'lr': 1e-05}'\n",
            "09/13/2021 16:32:43 - INFO - farm.modeling.optimization -   Using scheduler 'get_linear_schedule_with_warmup'\n",
            "09/13/2021 16:32:43 - INFO - farm.modeling.optimization -   Loading schedule `get_linear_schedule_with_warmup`: '{'num_warmup_steps': 29.700000000000003, 'num_training_steps': 297}'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Sep 13 16:32:44 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.63.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   45C    P0    60W / 149W |   7615MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "09/13/2021 16:32:44 - INFO - farm.train -   \n",
            " \n",
            "\n",
            "          &&& &&  & &&             _____                   _             \n",
            "      && &\\/&\\|& ()|/ @, &&       / ____|                 (_)            \n",
            "      &\\/(/&/&||/& /_/)_&/_&     | |  __ _ __ _____      ___ _ __   __ _ \n",
            "   &() &\\/&|()|/&\\/ '%\" & ()     | | |_ | '__/ _ \\ \\ /\\ / / | '_ \\ / _` |\n",
            "  &_\\_&&_\\ |& |&&/&__%_/_& &&    | |__| | | | (_) \\ V  V /| | | | | (_| |\n",
            "&&   && & &| &| /& & % ()& /&&    \\_____|_|  \\___/ \\_/\\_/ |_|_| |_|\\__, |\n",
            " ()&_---()&\\&\\|&&-&&--%---()~                                       __/ |\n",
            "     &&     \\|||                                                   |___/\n",
            "             |||\n",
            "             |||\n",
            "             |||\n",
            "       , -=-~  .-^- _\n",
            "              `\n",
            "\n",
            "Train epoch 0/2 (Cur. train loss: 0.2652): 100%|██████████| 99/99 [02:15<00:00,  1.37s/it]\n",
            "Train epoch 1/2 (Cur. train loss: 0.4838):   1%|          | 1/99 [00:01<02:12,  1.35s/it]\n",
            "Evaluating:   0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|██████████| 25/25 [00:10<00:00,  2.29it/s]\n",
            "09/13/2021 16:35:13 - INFO - farm.eval -   \n",
            "\n",
            "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "***************************************************\n",
            "***** EVALUATION | DEV SET | AFTER 100 BATCHES *****\n",
            "***************************************************\n",
            "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "\n",
            "09/13/2021 16:35:13 - INFO - farm.eval -   \n",
            " _________ text_classification _________\n",
            "09/13/2021 16:35:13 - INFO - farm.eval -   loss: 0.42739837202971287\n",
            "09/13/2021 16:35:13 - INFO - farm.eval -   task_name: text_classification\n",
            "09/13/2021 16:35:13 - INFO - farm.eval -   f1_macro: 0.812041784993929\n",
            "09/13/2021 16:35:13 - INFO - farm.eval -   report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9137    0.8774    0.8952       579\n",
            "           1     0.6926    0.7692    0.7289       208\n",
            "\n",
            "    accuracy                         0.8488       787\n",
            "   macro avg     0.8032    0.8233    0.8120       787\n",
            "weighted avg     0.8553    0.8488    0.8512       787\n",
            "\n",
            "Train epoch 1/2 (Cur. train loss: 0.3661): 100%|██████████| 99/99 [02:24<00:00,  1.46s/it]\n",
            "Train epoch 2/2 (Cur. train loss: 0.1357):   2%|▏         | 2/99 [00:02<02:10,  1.34s/it]\n",
            "Evaluating:   0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|██████████| 25/25 [00:10<00:00,  2.30it/s]\n",
            "09/13/2021 16:37:39 - INFO - farm.eval -   \n",
            "\n",
            "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "***************************************************\n",
            "***** EVALUATION | DEV SET | AFTER 200 BATCHES *****\n",
            "***************************************************\n",
            "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "\n",
            "09/13/2021 16:37:39 - INFO - farm.eval -   \n",
            " _________ text_classification _________\n",
            "09/13/2021 16:37:39 - INFO - farm.eval -   loss: 0.3342036640477453\n",
            "09/13/2021 16:37:39 - INFO - farm.eval -   task_name: text_classification\n",
            "09/13/2021 16:37:39 - INFO - farm.eval -   f1_macro: 0.826210268197468\n",
            "09/13/2021 16:37:39 - INFO - farm.eval -   report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9444    0.8515    0.8955       579\n",
            "           1     0.6755    0.8606    0.7569       208\n",
            "\n",
            "    accuracy                         0.8539       787\n",
            "   macro avg     0.8100    0.8560    0.8262       787\n",
            "weighted avg     0.8734    0.8539    0.8589       787\n",
            "\n",
            "Train epoch 2/2 (Cur. train loss: 0.0246): 100%|██████████| 99/99 [02:24<00:00,  1.45s/it]\n",
            "Evaluating: 100%|██████████| 83/83 [00:36<00:00,  2.28it/s]\n",
            "09/13/2021 16:40:24 - INFO - farm.eval -   \n",
            "\n",
            "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "***************************************************\n",
            "***** EVALUATION | TEST SET | AFTER 297 BATCHES *****\n",
            "***************************************************\n",
            "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "\n",
            "09/13/2021 16:40:24 - INFO - farm.eval -   \n",
            " _________ text_classification _________\n",
            "09/13/2021 16:40:24 - INFO - farm.eval -   loss: 0.4870244042577048\n",
            "09/13/2021 16:40:24 - INFO - farm.eval -   task_name: text_classification\n",
            "09/13/2021 16:40:24 - INFO - farm.eval -   f1_macro: 0.8412243900149486\n",
            "09/13/2021 16:40:24 - INFO - farm.eval -   report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9242    0.9048    0.9144      1954\n",
            "           1     0.7466    0.7908    0.7680       693\n",
            "\n",
            "    accuracy                         0.8750      2647\n",
            "   macro avg     0.8354    0.8478    0.8412      2647\n",
            "weighted avg     0.8777    0.8750    0.8761      2647\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVnUyU9DhQo5"
      },
      "source": [
        "## XLNet Cased <br>\n",
        "https://huggingface.co/xlnet-base-cased "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j9iM9t_hhQ5d",
        "outputId": "bf2f2d24-6ce8-42fd-8948-282e9c833fe3"
      },
      "source": [
        "import torch\n",
        "from farm.modeling.tokenization import Tokenizer\n",
        "from farm.data_handler.processor import TextClassificationProcessor\n",
        "from farm.data_handler.data_silo import DataSilo\n",
        "from farm.modeling.language_model import LanguageModel\n",
        "from farm.modeling.prediction_head import TextClassificationHead\n",
        "from farm.modeling.adaptive_model import AdaptiveModel\n",
        "from farm.modeling.optimization import initialize_optimizer\n",
        "from farm.train import Trainer\n",
        "from farm.utils import MLFlowLogger\n",
        "\n",
        "# Here we initialize a tokenizer that will be used for preprocessing text\n",
        "# This is the BERT Tokenizer which uses the byte pair encoding method.\n",
        "# It is currently loaded with a German model\n",
        "\n",
        "#tokenizer = Tokenizer.load(\n",
        "#    pretrained_model_name_or_path=\"finiteautomata/bertweet-base-sentiment-analysis\",\n",
        "#    do_lower_case=False)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"xlnet-base-cased\")\n",
        "\n",
        "# In order to prepare the data for the model, we need a set of\n",
        "# functions to transform data files into PyTorch Datasets.\n",
        "# We group these together in Processor objects.\n",
        "# We will need a new Processor object for each new source of data.\n",
        "# The abstract class can be found in farm.data_handling.processor.Processor\n",
        "# TOXIC = 1\n",
        "# OTHER = 0\n",
        "LABEL_LIST = [\"0\", \"1\"]\n",
        "processor = TextClassificationProcessor(tokenizer=tokenizer,\n",
        "                                        max_seq_len=128,\n",
        "                                        warmup = 600,\n",
        "                                        train_filename=\"train.tsv\",\n",
        "                                        test_filename=\"test_with_solutions.tsv\",\n",
        "                                        data_dir=\"../content\",\n",
        "                                        label_list=LABEL_LIST,\n",
        "                                        metric=\"f1_macro\",\n",
        "                                        text_column_name=\"Comment\",\n",
        "                                        label_column_name=\"Insult\")\n",
        "\n",
        "# We need a DataSilo in order to keep our train, dev and test sets separate.\n",
        "# The DataSilo will call the functions in the Processor to generate these sets.\n",
        "# From the DataSilo, we can fetch a PyTorch DataLoader object which will\n",
        "# be passed on to the model.\n",
        "# Here is a good place to define a batch size for the model\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "data_silo = DataSilo(\n",
        "    processor=processor,\n",
        "    batch_size=BATCH_SIZE)\n",
        "\n",
        "# The language model is the foundation on which modern NLP systems are built.\n",
        "# They encapsulate a general understanding of sentence semantics\n",
        "# and are not specific to any one task.\n",
        "\n",
        "# Here we are using Google's BERT model as implemented by HuggingFace. \n",
        "# The model being loaded is a German model that we trained. \n",
        "# You can also change the MODEL_NAME_OR_PATH to point to a BERT model that you\n",
        "# have saved or download one connected to the HuggingFace repository.\n",
        "# See farm.modeling.language_model.PRETRAINED_MODEL_ARCHIVE_MAP for a list of\n",
        "# available models\n",
        "\n",
        "MODEL_NAME_OR_PATH = \"xlnet-base-cased\"\n",
        "# MODEL_NAME_OR_PATH = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "language_model = LanguageModel.load(MODEL_NAME_OR_PATH)\n",
        "\n",
        "# A prediction head is a model that processes the output of the language model\n",
        "# for a specific task.\n",
        "# Prediction heads will look different depending on whether you're doing text classification\n",
        "# Named Entity Recognition (NER), question answering or some other task.\n",
        "# They should generate logits over the available prediction classes and contain methods\n",
        "# to convert these logits to losses or predictions \n",
        "\n",
        "# Here we use TextClassificationHead which receives a single fixed length sentence vector\n",
        "# and processes it using a feed forward neural network. layer_dims is a list of dimensions:\n",
        "# [input_dims, hidden_1_dims, hidden_2_dims ..., output_dims]\n",
        "\n",
        "# Here by default we have a single layer network.\n",
        "# It takes in a vector of length 768 (the default size of BERT's output).\n",
        "# It outputs a vector of length 2 (the number of classes in the GermEval18 (coarse) dataset)\n",
        "\n",
        "prediction_head = TextClassificationHead(num_labels=len(LABEL_LIST), class_weights=data_silo.calculate_class_weights(task_name=\"text_classification\"))\n",
        "\n",
        "# The language model and prediction head are coupled together in the Adaptive Model.\n",
        "# This class takes care of model saving and loading and also coordinates\n",
        "# cases where there is more than one prediction head.\n",
        "\n",
        "# EMBEDS_DROPOUT_PROB is the probability that an element of the output vector from the\n",
        "# language model will be set to zero.\n",
        "# EMBEDS_DROPOUT_PROB = 0.1 distilbert-base-uncased-finetuned-sst-2-english_1.PNG\n",
        "# EMBEDS_DROPOUT_PROB = 0.01\n",
        "# EMBEDS_DROPOUT_PROB = 0.2 distilbert-base-uncased-finetuned-sst-2-english_2.PNG\n",
        "EMBEDS_DROPOUT_PROB = 0.01\n",
        "\n",
        "model = AdaptiveModel(\n",
        "    language_model=language_model,\n",
        "    prediction_heads=[prediction_head],\n",
        "    embeds_dropout_prob=EMBEDS_DROPOUT_PROB,\n",
        "    lm_output_types=[\"per_sequence\"],\n",
        "    device=device)\n",
        "\n",
        "# Here we initialize a Bert Adam optimizer that has a linear warmup and warmdown\n",
        "# Here you can set learning rate, the warmup proportion and number of epochs to train for\n",
        "\n",
        "#LEARNING_RATE = 2e-5 # distilbert-base-uncased-finetuned-sst-2-english_1.PNG\n",
        "#LEARNING_RATE = 1e-7\n",
        "LEARNING_RATE = 1e-5  # distilbert-base-uncased-finetuned-sst-2-english_2.PNG\n",
        "\n",
        "N_EPOCHS = 3\n",
        "\n",
        "model, optimizer, lr_schedule = initialize_optimizer(\n",
        "    model=model,\n",
        "    device=device,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    n_batches=len(data_silo.loaders[\"train\"]),\n",
        "    n_epochs=N_EPOCHS)\n",
        "\n",
        "# Training loop handled by this\n",
        "# It will also trigger evaluation during training using the dev data\n",
        "# and after training using the test data.\n",
        "\n",
        "# Set N_GPU to a positive value if CUDA is available\n",
        "N_GPU = 1\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    optimizer=optimizer,\n",
        "    data_silo=data_silo,\n",
        "    epochs=N_EPOCHS,\n",
        "    n_gpu=N_GPU,\n",
        "    lr_schedule=lr_schedule,\n",
        "    device=device, \n",
        ")\n",
        "\n",
        "!nvidia-smi\n",
        "\n",
        "model = trainer.train()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/configuration_xlnet.py:212: FutureWarning: This config doesn't use attention memories, a core feature of XLNet. Consider setting `men_len` to a non-zero value, for example `xlnet = XLNetLMHeadModel.from_pretrained('xlnet-base-cased'', mem_len=1024)`, for accurate training performance as well as an order of magnitude faster inference. Starting from version 3.5.0, the default parameter will be 1024, following the implementation in https://arxiv.org/abs/1906.08237\n",
            "  FutureWarning,\n",
            "09/13/2021 16:40:25 - WARNING - farm.utils -   Failed to log params: INVALID_PARAMETER_VALUE: Changing param value is not allowed. Param with key='tokenizer' was already logged with value='DistilBertTokenizer' for run ID='6543ccfdbba146ca82a34b6de1bbb983. Attempted logging new value 'XLNetTokenizer'.\n",
            "09/13/2021 16:40:25 - INFO - farm.data_handler.data_silo -   \n",
            "Loading data into the data silo ... \n",
            "              ______\n",
            "               |o  |   !\n",
            "   __          |:`_|---'-.\n",
            "  |__|______.-/ _ \\-----.|       \n",
            " (o)(o)------'\\ _ /     ( )      \n",
            " \n",
            "09/13/2021 16:40:25 - INFO - farm.data_handler.data_silo -   Loading train set from: ../content/train.tsv \n",
            "09/13/2021 16:40:26 - INFO - farm.data_handler.data_silo -   Got ya 1 parallel workers to convert 3947 dictionaries to pytorch datasets (chunksize = 790)...\n",
            "09/13/2021 16:40:26 - INFO - farm.data_handler.data_silo -    0 \n",
            "09/13/2021 16:40:26 - INFO - farm.data_handler.data_silo -   /w\\\n",
            "09/13/2021 16:40:26 - INFO - farm.data_handler.data_silo -   /'\\\n",
            "09/13/2021 16:40:26 - INFO - farm.data_handler.data_silo -   \n",
            "Preprocessing Dataset ../content/train.tsv:   0%|          | 0/3947 [00:00<?, ? Dicts/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils.py:460: FutureWarning: `is_pretokenized` is deprecated and will be removed in a future version, use `is_split_into_words` instead.\n",
            "  FutureWarning,\n",
            "09/13/2021 16:40:27 - INFO - farm.data_handler.processor -   *** Show 2 random examples ***\n",
            "09/13/2021 16:40:27 - INFO - farm.data_handler.processor -   \n",
            "\n",
            "      .--.        _____                       _      \n",
            "    .'_\\/_'.     / ____|                     | |     \n",
            "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
            "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
            "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
            "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
            "   (/\\||/                             |_|           \n",
            "______\\||/___________________________________________                     \n",
            "\n",
            "ID: 44-0\n",
            "Clear Text: \n",
            " \ttext_classification_label: 0\n",
            " \ttext: \"....oh fuck what an amazing cock its soooooooo dam big and a perfect foreskin one of the best dicks Ive ever seen\"\n",
            "Tokenized: \n",
            " \ttokens: ['▁', '\"', '.', '.', '.', '.', 'oh', '▁fuck', '▁what', '▁an', '▁amazing', '▁cock', '▁its', '▁so', 'oo', 'o', 'oo', 'oo', '▁dam', '▁big', '▁and', '▁a', '▁perfect', '▁for', 'es', 'kin', '▁one', '▁of', '▁the', '▁best', '▁', 'dick', 's', '▁I', 've', '▁ever', '▁seen', '\"']\n",
            " \toffsets: [0, 0, 1, 2, 3, 4, 5, 8, 13, 18, 21, 29, 34, 38, 40, 42, 43, 45, 48, 52, 56, 60, 62, 70, 73, 75, 79, 83, 86, 90, 95, 95, 99, 101, 102, 105, 110, 114]\n",
            " \tstart_of_word: [True, False, False, False, False, False, False, True, True, True, True, True, True, True, False, False, False, False, True, True, True, True, True, True, False, False, True, True, True, True, True, False, False, True, False, True, True, False]\n",
            "Features: \n",
            " \tinput_ids: [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 17, 12, 9, 9, 9, 9, 4470, 15707, 113, 48, 3704, 13173, 81, 102, 5449, 155, 5449, 5449, 7678, 534, 21, 24, 1705, 28, 202, 2160, 65, 20, 18, 252, 17, 17882, 23, 35, 189, 545, 566, 12, 4, 3]\n",
            " \tpadding_mask: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            " \tsegment_ids: [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2]\n",
            " \ttext_classification_label_ids: [0]\n",
            "_____________________________________________________\n",
            "09/13/2021 16:40:27 - INFO - farm.data_handler.processor -   \n",
            "\n",
            "      .--.        _____                       _      \n",
            "    .'_\\/_'.     / ____|                     | |     \n",
            "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
            "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
            "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
            "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
            "   (/\\||/                             |_|           \n",
            "______\\||/___________________________________________                     \n",
            "\n",
            "ID: 708-0\n",
            "Clear Text: \n",
            " \ttext_classification_label: 0\n",
            " \ttext: \"battier sucks again!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!1\"\n",
            "Tokenized: \n",
            " \ttokens: ['▁', '\"', 'bat', 'tier', '▁suck', 's', '▁again', '!!!!', '!!!!', '!!!!', '!!!!', '!!!!', '!!!!', '!!!!', '!!!!', '!!!!', '!!!!', '!!!!', '!!!!', '!!!!', '!!!!', '!!!!', '!!!!', '!!!!', '!!!!', '1', '\"']\n",
            " \toffsets: [0, 0, 1, 4, 9, 13, 15, 20, 24, 28, 32, 36, 40, 44, 48, 52, 56, 60, 64, 68, 72, 76, 80, 84, 88, 92, 93]\n",
            " \tstart_of_word: [True, False, False, False, True, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
            "Features: \n",
            " \tinput_ids: [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 17, 12, 5316, 5861, 11989, 23, 292, 21823, 21823, 21823, 21823, 21823, 21823, 21823, 21823, 21823, 21823, 21823, 21823, 21823, 21823, 21823, 21823, 21823, 21823, 174, 12, 4, 3]\n",
            " \tpadding_mask: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            " \tsegment_ids: [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2]\n",
            " \ttext_classification_label_ids: [0]\n",
            "_____________________________________________________\n",
            "Preprocessing Dataset ../content/train.tsv: 100%|██████████| 3947/3947 [00:09<00:00, 395.78 Dicts/s]\n",
            "09/13/2021 16:40:36 - INFO - farm.data_handler.data_silo -   Loading dev set as a slice of train set\n",
            "09/13/2021 16:40:36 - INFO - farm.data_handler.data_silo -   Took 787 samples out of train set to create dev set (dev split is roughly 0.1)\n",
            "09/13/2021 16:40:36 - INFO - farm.data_handler.data_silo -   Loading test set from: ../content/test_with_solutions.tsv\n",
            "09/13/2021 16:40:36 - INFO - farm.data_handler.data_silo -   Got ya 1 parallel workers to convert 2647 dictionaries to pytorch datasets (chunksize = 530)...\n",
            "09/13/2021 16:40:36 - INFO - farm.data_handler.data_silo -    0 \n",
            "09/13/2021 16:40:36 - INFO - farm.data_handler.data_silo -   /w\\\n",
            "09/13/2021 16:40:36 - INFO - farm.data_handler.data_silo -   / \\\n",
            "09/13/2021 16:40:36 - INFO - farm.data_handler.data_silo -   \n",
            "Preprocessing Dataset ../content/test_with_solutions.tsv:   0%|          | 0/2647 [00:00<?, ? Dicts/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils.py:460: FutureWarning: `is_pretokenized` is deprecated and will be removed in a future version, use `is_split_into_words` instead.\n",
            "  FutureWarning,\n",
            "09/13/2021 16:40:38 - INFO - farm.data_handler.processor -   *** Show 2 random examples ***\n",
            "09/13/2021 16:40:38 - INFO - farm.data_handler.processor -   \n",
            "\n",
            "      .--.        _____                       _      \n",
            "    .'_\\/_'.     / ____|                     | |     \n",
            "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
            "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
            "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
            "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
            "   (/\\||/                             |_|           \n",
            "______\\||/___________________________________________                     \n",
            "\n",
            "ID: 276-0\n",
            "Clear Text: \n",
            " \ttext_classification_label: 0\n",
            " \ttext: \"@james3012   really?? more name calling?? i would have never guessed it. what a sad little boy you must be. just because the big kids bully you at school doesnt mean you have to try to be some internet gangster because i promise you you are not scaring anyone. you must be a wh fan. once again you guys continue to prove my comments about no class no character true. maybe for now on that can be your clubs motto.... NO CLASS NO CHARACTER by the way your comment \"Just be glad someone from Manchester actually has a career\" doesnt really make much sense. actually it doesnt make any sense at all. but what else can we expect from dense people such as yourself and the other two i was talking to. keep supporting rubbish people who shouldnt be allowed to profit from the beautiful game. ok i will shut it because i know you are feeble minded so go back to your nintendo\"\n",
            "Tokenized: \n",
            " \ttokens: ['▁', '\"', '@', 'ja', 'mes', '30', '12', '▁really', '?', '?', '▁more', '▁name', '▁calling', '?', '?', '▁', 'i', '▁would', '▁have', '▁never', '▁guessed', '▁it', '.', '▁what', '▁a', '▁sad', '▁little', '▁boy', '▁you', '▁must', '▁be', '.', '▁just', '▁because', '▁the', '▁big', '▁kids', '▁bull', 'y', '▁you', '▁at', '▁school', '▁doesn', 't', '▁mean', '▁you', '▁have', '▁to', '▁try', '▁to', '▁be', '▁some', '▁internet', '▁gangster', '▁because', '▁', 'i', '▁promise', '▁you', '▁you', '▁are', '▁not', '▁scar', 'ing', '▁anyone', '.', '▁you', '▁must', '▁be', '▁a', '▁wh', '▁fan', '.', '▁once', '▁again', '▁you', '▁guys', '▁continue', '▁to', '▁prove', '▁my', '▁comments', '▁about', '▁no', '▁class', '▁no', '▁character', '▁true', '.', '▁maybe', '▁for', '▁now', '▁on', '▁that', '▁can', '▁be', '▁your', '▁clubs', '▁motto', '.', '.', '.', '.', '▁NO', '▁C', 'LAS', 'S', '▁NO', '▁', 'CHA', 'RAC', 'TER', '▁by', '▁the', '▁way', '▁your', '▁comment', '▁', '\"', 'Just', '▁be', '▁glad', '▁someone', '▁from', '▁Manchester', '▁actually']\n",
            " \toffsets: [0, 0, 1, 2, 4, 7, 9, 14, 20, 21, 23, 28, 33, 40, 41, 43, 43, 45, 51, 56, 62, 70, 72, 74, 79, 81, 85, 92, 96, 100, 105, 107, 109, 114, 122, 126, 130, 135, 139, 141, 145, 148, 155, 160, 162, 167, 171, 176, 179, 183, 186, 189, 194, 203, 212, 220, 220, 222, 230, 234, 238, 242, 246, 250, 254, 260, 262, 266, 271, 274, 276, 279, 282, 284, 289, 295, 299, 304, 313, 316, 322, 325, 334, 340, 343, 349, 352, 362, 366, 368, 374, 378, 382, 385, 390, 394, 397, 402, 408, 413, 414, 415, 416, 418, 421, 422, 425, 427, 430, 430, 433, 436, 440, 443, 447, 451, 456, 464, 464, 465, 470, 473, 478, 486, 491, 502]\n",
            " \tstart_of_word: [True, False, False, False, False, False, False, True, False, False, True, True, True, False, False, True, False, True, True, True, True, True, False, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, False, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, False, True, False, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, False, False, False, False, True, True, False, False, True, True, False, False, False, True, True, True, True, True, True, False, False, True, True, True, True, True, True]\n",
            "Features: \n",
            " \tinput_ids: [17, 12, 13304, 1653, 4316, 1496, 1396, 343, 82, 82, 70, 304, 2149, 82, 82, 17, 150, 74, 47, 287, 19259, 36, 9, 113, 24, 5694, 293, 2001, 44, 272, 39, 9, 125, 149, 18, 534, 1886, 6446, 117, 44, 38, 297, 855, 46, 1125, 44, 47, 22, 714, 22, 39, 106, 2476, 24495, 149, 17, 150, 3691, 44, 44, 41, 50, 12612, 56, 1216, 9, 44, 272, 39, 24, 9012, 3054, 9, 497, 292, 44, 2500, 786, 22, 3392, 94, 1992, 75, 116, 1075, 116, 1542, 1229, 9, 2163, 28, 145, 31, 29, 64, 39, 73, 4136, 21636, 9, 9, 9, 9, 6721, 330, 24304, 83, 6721, 17, 19909, 26530, 9645, 37, 18, 162, 73, 1709, 17, 12, 5817, 39, 5590, 886, 40, 5017, 995, 4, 3]\n",
            " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2]\n",
            " \ttext_classification_label_ids: [0]\n",
            "_____________________________________________________\n",
            "09/13/2021 16:40:38 - INFO - farm.data_handler.processor -   \n",
            "\n",
            "      .--.        _____                       _      \n",
            "    .'_\\/_'.     / ____|                     | |     \n",
            "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
            "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
            "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
            "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
            "   (/\\||/                             |_|           \n",
            "______\\||/___________________________________________                     \n",
            "\n",
            "ID: 182-0\n",
            "Clear Text: \n",
            " \ttext_classification_label: 1\n",
            " \ttext: \"Bigger than the Chicago Tribune and the Boston Globe? \\xa0Are you HIGH???? \\xa0 I've never even SEEN the daily caller, but I've bought the Tribune and read the Globe. \\xa0What else you got? \\xa0And by the way, your blog isn't legitimate. \\xa0It's a fucking blog. \\xa0A website. \\xa0I don't think that truly qualifies, no offense Raw. \\xa0But even YOU guys don't really count and you know it. \\xa0IF they don't want to give them a press pass there really isn't much that they can do to force the White House's hand. \\xa0If they give them a pass it's because they don't feel threatened and WANT him to keep up the bullshit to SHOW the country just what these people's true colors are. \\xa0Obama and pals ARE NOT playing the race card and never have, no matter how many times the right wing assholes keep repeating that bullshit. \\xa0But these people keep painting themselves into a corner of backwards-assed white power racist uncivilized and uninspiring troglodytes.\"\n",
            "Tokenized: \n",
            " \ttokens: ['▁', '\"', 'Big', 'ger', '▁than', '▁the', '▁Chicago', '▁Tribune', '▁and', '▁the', '▁Boston', '▁Globe', '?', '▁', '\\\\', 'xa', '0', 'Are', '▁you', '▁HI', 'GH', '?', '???', '▁', '\\\\', 'xa', '0', '▁I', \"'\", 've', '▁never', '▁even', '▁SE', 'EN', '▁the', '▁daily', '▁caller', ',', '▁but', '▁I', \"'\", 've', '▁bought', '▁the', '▁Tribune', '▁and', '▁read', '▁the', '▁Globe', '.', '▁', '\\\\', 'xa', '0', 'What', '▁else', '▁you', '▁got', '?', '▁', '\\\\', 'xa', '0', 'And', '▁by', '▁the', '▁way', ',', '▁your', '▁blog', '▁isn', \"'\", 't', '▁legitimate', '.', '▁', '\\\\', 'xa', '0', 'It', \"'\", 's', '▁a', '▁fucking', '▁blog', '.', '▁', '\\\\', 'xa', '0', 'A', '▁website', '.', '▁', '\\\\', 'xa', '0', 'I', '▁don', \"'\", 't', '▁think', '▁that', '▁truly', '▁qualifies', ',', '▁no', '▁offense', '▁R', 'aw', '.', '▁', '\\\\', 'xa', '0', 'But', '▁even', '▁YOU', '▁guys', '▁don', \"'\", 't', '▁really', '▁count', '▁and', '▁you']\n",
            " \toffsets: [0, 0, 1, 4, 8, 13, 17, 25, 33, 37, 41, 48, 53, 55, 55, 56, 58, 59, 63, 67, 69, 71, 72, 76, 76, 77, 79, 81, 82, 83, 86, 92, 97, 99, 102, 106, 112, 118, 120, 124, 125, 126, 129, 136, 140, 148, 152, 157, 161, 166, 168, 168, 169, 171, 172, 177, 182, 186, 189, 191, 191, 192, 194, 195, 199, 202, 206, 209, 211, 216, 221, 224, 225, 227, 237, 239, 239, 240, 242, 243, 245, 246, 248, 250, 258, 262, 264, 264, 265, 267, 268, 270, 277, 279, 279, 280, 282, 283, 285, 288, 289, 291, 297, 302, 308, 317, 319, 322, 330, 331, 333, 335, 335, 336, 338, 339, 343, 348, 352, 357, 360, 361, 363, 370, 376, 380]\n",
            " \tstart_of_word: [True, False, False, False, True, True, True, True, True, True, True, True, False, True, False, False, False, False, True, True, False, False, False, True, False, False, False, True, False, False, True, True, True, False, True, True, True, False, True, True, False, False, True, True, True, True, True, True, True, False, True, False, False, False, False, True, True, True, False, True, False, False, False, False, True, True, True, False, True, True, True, False, False, True, False, True, False, False, False, False, False, False, True, True, True, False, True, False, False, False, False, True, False, True, False, False, False, False, True, False, False, True, True, True, True, False, True, True, True, False, False, True, False, False, False, False, True, True, True, True, False, False, True, True, True, True]\n",
            "Features: \n",
            " \tinput_ids: [17, 12, 13174, 2371, 100, 18, 1763, 14508, 21, 18, 1843, 9125, 82, 17, 17666, 14151, 279, 5416, 44, 17213, 14524, 82, 22180, 17, 17666, 14151, 279, 35, 26, 189, 287, 176, 8442, 6543, 18, 1362, 18380, 19, 57, 35, 26, 189, 2242, 18, 14508, 21, 828, 18, 9125, 9, 17, 17666, 14151, 279, 979, 1104, 44, 345, 82, 17, 17666, 14151, 279, 1648, 37, 18, 162, 19, 73, 2780, 1601, 26, 46, 7296, 9, 17, 17666, 14151, 279, 289, 26, 23, 24, 19021, 2780, 9, 17, 17666, 14151, 279, 246, 1001, 9, 17, 17666, 14151, 279, 96, 220, 26, 46, 232, 29, 3291, 28717, 19, 116, 5986, 482, 3806, 9, 17, 17666, 14151, 279, 1294, 176, 13938, 2500, 220, 26, 46, 343, 3497, 21, 44, 4, 3]\n",
            " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2]\n",
            " \ttext_classification_label_ids: [1]\n",
            "_____________________________________________________\n",
            "Preprocessing Dataset ../content/test_with_solutions.tsv: 100%|██████████| 2647/2647 [00:07<00:00, 359.10 Dicts/s]\n",
            "09/13/2021 16:40:43 - INFO - farm.data_handler.data_silo -   Examples in train: 3160\n",
            "09/13/2021 16:40:43 - INFO - farm.data_handler.data_silo -   Examples in dev  : 787\n",
            "09/13/2021 16:40:43 - INFO - farm.data_handler.data_silo -   Examples in test : 2647\n",
            "09/13/2021 16:40:43 - INFO - farm.data_handler.data_silo -   \n",
            "09/13/2021 16:40:43 - INFO - farm.data_handler.data_silo -   Longest sequence length observed after clipping:     128\n",
            "09/13/2021 16:40:43 - INFO - farm.data_handler.data_silo -   Average sequence length after clipping: 46.91962025316456\n",
            "09/13/2021 16:40:43 - INFO - farm.data_handler.data_silo -   Proportion clipped:      0.08386075949367089\n",
            "09/13/2021 16:40:43 - WARNING - farm.utils -   Failed to log params: INVALID_PARAMETER_VALUE: Changing param value is not allowed. Param with key='ave_seq_len' was already logged with value='43.41107594936709' for run ID='6543ccfdbba146ca82a34b6de1bbb983. Attempted logging new value '46.91962025316456'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvQUsXu9kgSq"
      },
      "source": [
        "## Switching to NER (NOT WORKING)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7vnmJMT4MYg"
      },
      "source": [
        "# Import the new building blocks\n",
        "\n",
        "from farm.data_handler.processor import NERProcessor\n",
        "from farm.modeling.prediction_head import TokenClassificationHead\n",
        "ml_logger.init_experiment(experiment_name=\"Public_FARM\", run_name=\"Tutorial1_Colab_NER\")\n",
        "\n",
        "# This processor will preprocess the data for the CoNLL03 NER task\n",
        "ner_labels = [\"[PAD]\", \"X\", \"O\", \"B-MISC\", \"I-MISC\", \"B-PER\", \"I-PER\", \"B-ORG\", \"I-ORG\", \"B-LOC\", \"I-LOC\", \"B-OTH\", \"I-OTH\"]\n",
        "\n",
        "ner_processor = NERProcessor(tokenizer=tokenizer, \n",
        "                             max_seq_len=128, \n",
        "                             data_dir=\"../data/conll03-de\",\n",
        "                             label_list=ner_labels,\n",
        "                             metric=\"seq_f1\")\n",
        "\n",
        "# This prediction head is also a feed forward neural network but expects one\n",
        "# vector per token in the input sequence and will generate a set of logits\n",
        "# for each input\n",
        "\n",
        "ner_prediction_head = TokenClassificationHead(num_labels=len(ner_labels))\n",
        "\n",
        "# We can integrate these new pieces with the rest using this code\n",
        "# It is pretty much the same structure as what we had above for text classification\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "EMBEDS_DROPOUT_PROB = 0.1\n",
        "LEARNING_RATE = 2e-5\n",
        "N_EPOCHS = 1\n",
        "N_GPU = 1\n",
        "\n",
        "data_silo = DataSilo(\n",
        "    processor=ner_processor,\n",
        "    batch_size=BATCH_SIZE)\n",
        "\n",
        "model = AdaptiveModel(\n",
        "    language_model=language_model,\n",
        "    prediction_heads=[ner_prediction_head],\n",
        "    embeds_dropout_prob=EMBEDS_DROPOUT_PROB,\n",
        "    lm_output_types=[\"per_token\"],\n",
        "    device=device)\n",
        "\n",
        "model, optimizer, lr_schedule = initialize_optimizer(\n",
        "    model=model,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    n_batches=len(data_silo.loaders[\"train\"]),\n",
        "    n_epochs=N_EPOCHS,\n",
        "    device=device)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    optimizer=optimizer,\n",
        "    data_silo=data_silo,\n",
        "    epochs=N_EPOCHS,\n",
        "    n_gpu=N_GPU,\n",
        "    lr_schedule=lr_schedule,\n",
        "    device=device,\n",
        ")\n",
        "\n",
        "model = trainer.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prov6seQlmLq"
      },
      "source": [
        "# Import the new building blocks\n",
        "\n",
        "from farm.data_handler.processor import NERProcessor\n",
        "from farm.modeling.prediction_head import TokenClassificationHead\n",
        "ml_logger.init_experiment(experiment_name=\"Public_FARM\", run_name=\"Tutorial1_Colab_NER\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "reITkXdilqL9"
      },
      "source": [
        "# This processor will preprocess the data for the CoNLL03 NER task\n",
        "ner_labels = [\"1\", \"0\"]\n",
        "\n",
        "ner_processor = NERProcessor(tokenizer=tokenizer, \n",
        "                             max_seq_len=128,\n",
        "                             train_filename=\"train.tsv\",\n",
        "                             test_filename=\"test_with_solutions.tsv\",\n",
        "                             dev_filename=None,\n",
        "                             data_dir=\"../content\",\n",
        "                             label_list=ner_labels,\n",
        "                             metric=\"f1_macro\",\n",
        "                             text_column_name=\"Comment\",\n",
        "                              label_column_name=\"Insult\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJ-ddMWvlswY"
      },
      "source": [
        "# This prediction head is also a feed forward neural network but expects one\n",
        "# vector per token in the input sequence and will generate a set of logits\n",
        "# for each input\n",
        "\n",
        "ner_prediction_head = TokenClassificationHead(num_labels=len(ner_labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aY0IYYBXlu00"
      },
      "source": [
        "# We can integrate these new pieces with the rest using this code\n",
        "# It is pretty much the same structure as what we had above for text classification\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "EMBEDS_DROPOUT_PROB = 0.1\n",
        "LEARNING_RATE = 2e-5\n",
        "N_EPOCHS = 1\n",
        "N_GPU = 1\n",
        "\n",
        "data_silo = DataSilo(\n",
        "    processor=ner_processor,\n",
        "    batch_size=BATCH_SIZE)\n",
        "\n",
        "model = AdaptiveModel(\n",
        "    language_model=language_model,\n",
        "    prediction_heads=[ner_prediction_head],\n",
        "    embeds_dropout_prob=EMBEDS_DROPOUT_PROB,\n",
        "    lm_output_types=[\"per_sequence\"],\n",
        "    device=device)\n",
        "\n",
        "model, optimizer, lr_schedule = initialize_optimizer(\n",
        "    model=model,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    n_batches=len(data_silo.loaders[\"train\"]),\n",
        "    n_epochs=N_EPOCHS,\n",
        "    device=device)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    optimizer=optimizer,\n",
        "    data_silo=data_silo,\n",
        "    epochs=N_EPOCHS,\n",
        "    n_gpu=N_GPU,\n",
        "    lr_schedule=lr_schedule,\n",
        "    device=device,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rbTzCDslxH0"
      },
      "source": [
        "model = trainer.train()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
