{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Kopie von Transfer Learning mit FARM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTThkpiIXlJP"
      },
      "source": [
        "# Transfer Learning mit FARM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ipd2E0WSPDW"
      },
      "source": [
        "## Ziele\n",
        "\n",
        "\n",
        "*   BERT und Transfer Learning\n",
        "*   Einführung in FARM\n",
        "\n",
        "*   Wie funktioniert das Fine-Tuning eines BERT-Modells?\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7ivyIhQUc92"
      },
      "source": [
        "## BERT und Transfer Learning\n",
        "Transfer Learning ist eine Machine Learning-Methode, bei der ein Modell, das für einen bestimmten Task trainiert wurde, als Ausgangspunkt für ein Modell für einen anderen Task verwendet wird.\n",
        "\n",
        "Die Idee hinter diesem Konzept ist, dass man das Erlernte von einem Problem auf ein anderes überträgt. Wenn man z.B. Java als erste Programmiersprache gelernt hat, fällt es uns leichter, Python oder eine andere Programmiersprache zu erlernen. Denn die grundlegenden Programmierkonzepte aus Java/OOP können leicht auf jede andere Programmiersprache übertragen werden. Man muss also nicht von Null beginnnen, sondern kann auf existierendem Wissen aufbauen. Ähnlich verhält es sich auch mit dem Transfer Learning von Sprachmodellen wie BERT. Existierende BERT-Modelle wurden hauptsächlich auf der Wikipedia trainiert. Möchte man allerdings Textklassifikationen in bestimmten Domänen durchführen, ist die Wikipedia mit ihrem Vokabular nicht repräsentativ genug. Daher nutzt man das Fine-Tuning von vortrainierten BERT-Modellen, um sie an den neuen Task/das neue Problem anzupassen. Durch diesen Schritt erhofft man sich bessere Performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYna70JXkjFk"
      },
      "source": [
        "## FARM \n",
        "(**F**ramework for **A**dapting **R**epresentation **M**odels)\n",
        "![farm_logo](https://github.com/deepset-ai/FARM/raw/master/docs/img/farm_logo_text_right_wide.png?raw=true)\n",
        "... ermöglicht uns, das Transfer Learning mit BERT (und anderen Sprachmodellen) umzusetzen. Damit kann man einfach und schnell Modelle für Tasks wie Text Classification, NER und Question Answering erstellen:\n",
        "![task_table](https://miro.medium.com/max/3840/1*0RYwLSOMegTKfhnyTwkjIQ.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPltDefXjSiJ"
      },
      "source": [
        "# Fine-Tuning eines BERT-Modells mit FARM (adapted from FARM-Tutorial 1)\n",
        "\n",
        "Welcome to the FARM building blocks tutorial! There are many different ways to make use of this repository, but in this notebook, we will be going through the most import building blocks that will help you harvest the rewards of a successfully trained NLP model.\n",
        "\n",
        "Happy FARMing!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2cj67ZBjYSC"
      },
      "source": [
        "## 1) Text Classification\n",
        "\n",
        "GermEval 2018 (GermEval2018) (https://projects.fzai.h-da.de/iggsa/) is an open data set containing texts that need to be classified by whether they are offensive or not. There are a set of coarse and fine labels, but here we will only be looking at the coarse set which labels each example as either OFFENSE or OTHER. To tackle this task, we are going to build a classifier that is composed of Google's BERT language model and a feed forward neural network prediction head."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQz1S2d5jfST"
      },
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PeIWk6_4jiQ2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c12c4c2a-8efc-4307-b2f8-04aecd199c12"
      },
      "source": [
        "# Install FARM\n",
        "!pip install torch==1.6.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!pip install farm==0.5.0"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Requirement already satisfied: torch==1.6.0+cu101 in /usr/local/lib/python3.7/dist-packages (1.6.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.6.0+cu101) (1.19.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.6.0+cu101) (0.16.0)\n",
            "Requirement already satisfied: farm==0.5.0 in /usr/local/lib/python3.7/dist-packages (0.5.0)\n",
            "Requirement already satisfied: Werkzeug==0.16.1 in /usr/local/lib/python3.7/dist-packages (from farm==0.5.0) (0.16.1)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.7/dist-packages (from farm==0.5.0) (1.4.1)\n",
            "Requirement already satisfied: transformers==3.3.1 in /usr/local/lib/python3.7/dist-packages (from farm==0.5.0) (3.3.1)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (from farm==0.5.0) (0.0)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.7/dist-packages (from farm==0.5.0) (1.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from farm==0.5.0) (57.4.0)\n",
            "Requirement already satisfied: mlflow==1.0.0 in /usr/local/lib/python3.7/dist-packages (from farm==0.5.0) (1.0.0)\n",
            "Requirement already satisfied: flask-cors in /usr/local/lib/python3.7/dist-packages (from farm==0.5.0) (3.0.10)\n",
            "Requirement already satisfied: flask-restplus in /usr/local/lib/python3.7/dist-packages (from farm==0.5.0) (0.13.0)\n",
            "Requirement already satisfied: torch<1.7,>1.5 in /usr/local/lib/python3.7/dist-packages (from farm==0.5.0) (1.6.0+cu101)\n",
            "Requirement already satisfied: dotmap==1.3.0 in /usr/local/lib/python3.7/dist-packages (from farm==0.5.0) (1.3.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from farm==0.5.0) (0.3.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from farm==0.5.0) (4.62.0)\n",
            "Requirement already satisfied: seqeval==0.0.12 in /usr/local/lib/python3.7/dist-packages (from farm==0.5.0) (0.0.12)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from farm==0.5.0) (5.4.8)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from farm==0.5.0) (2.23.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.7/dist-packages (from farm==0.5.0) (1.18.38)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (from farm==0.5.0) (0.37.0)\n",
            "Requirement already satisfied: alembic in /usr/local/lib/python3.7/dist-packages (from mlflow==1.0.0->farm==0.5.0) (1.7.1)\n",
            "Requirement already satisfied: querystring-parser in /usr/local/lib/python3.7/dist-packages (from mlflow==1.0.0->farm==0.5.0) (1.2.4)\n",
            "Requirement already satisfied: docker>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from mlflow==1.0.0->farm==0.5.0) (5.0.2)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from mlflow==1.0.0->farm==0.5.0) (3.17.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from mlflow==1.0.0->farm==0.5.0) (3.13)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mlflow==1.0.0->farm==0.5.0) (1.19.5)\n",
            "Requirement already satisfied: gitpython>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from mlflow==1.0.0->farm==0.5.0) (3.1.18)\n",
            "Requirement already satisfied: simplejson in /usr/local/lib/python3.7/dist-packages (from mlflow==1.0.0->farm==0.5.0) (3.17.5)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from mlflow==1.0.0->farm==0.5.0) (2.8.2)\n",
            "Requirement already satisfied: sqlparse in /usr/local/lib/python3.7/dist-packages (from mlflow==1.0.0->farm==0.5.0) (0.4.1)\n",
            "Requirement already satisfied: sqlalchemy in /usr/local/lib/python3.7/dist-packages (from mlflow==1.0.0->farm==0.5.0) (1.4.22)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from mlflow==1.0.0->farm==0.5.0) (1.1.5)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from mlflow==1.0.0->farm==0.5.0) (7.1.2)\n",
            "Requirement already satisfied: gunicorn in /usr/local/lib/python3.7/dist-packages (from mlflow==1.0.0->farm==0.5.0) (20.1.0)\n",
            "Requirement already satisfied: databricks-cli>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from mlflow==1.0.0->farm==0.5.0) (0.15.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from mlflow==1.0.0->farm==0.5.0) (1.3.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from mlflow==1.0.0->farm==0.5.0) (1.15.0)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from mlflow==1.0.0->farm==0.5.0) (0.3)\n",
            "Requirement already satisfied: Keras>=2.2.4 in /usr/local/lib/python3.7/dist-packages (from seqeval==0.0.12->farm==0.5.0) (2.6.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.3.1->farm==0.5.0) (21.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3.3.1->farm==0.5.0) (2019.12.20)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.7/dist-packages (from transformers==3.3.1->farm==0.5.0) (0.1.96)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc2 in /usr/local/lib/python3.7/dist-packages (from transformers==3.3.1->farm==0.5.0) (0.8.1rc2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.3.1->farm==0.5.0) (3.0.12)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==3.3.1->farm==0.5.0) (0.0.45)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.7/dist-packages (from databricks-cli>=0.8.0->mlflow==1.0.0->farm==0.5.0) (0.8.9)\n",
            "Requirement already satisfied: websocket-client>=0.32.0 in /usr/local/lib/python3.7/dist-packages (from docker>=3.6.0->mlflow==1.0.0->farm==0.5.0) (1.2.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.0 in /usr/local/lib/python3.7/dist-packages (from gitpython>=2.1.0->mlflow==1.0.0->farm==0.5.0) (3.7.4.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from gitpython>=2.1.0->mlflow==1.0.0->farm==0.5.0) (4.0.7)\n",
            "Requirement already satisfied: smmap<5,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->gitpython>=2.1.0->mlflow==1.0.0->farm==0.5.0) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->farm==0.5.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->farm==0.5.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->farm==0.5.0) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->farm==0.5.0) (1.25.11)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch<1.7,>1.5->farm==0.5.0) (0.16.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from alembic->mlflow==1.0.0->farm==0.5.0) (5.2.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from alembic->mlflow==1.0.0->farm==0.5.0) (4.6.4)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.7/dist-packages (from alembic->mlflow==1.0.0->farm==0.5.0) (1.1.5)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy->mlflow==1.0.0->farm==0.5.0) (1.1.1)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3->farm==0.5.0) (0.10.0)\n",
            "Requirement already satisfied: botocore<1.22.0,>=1.21.38 in /usr/local/lib/python3.7/dist-packages (from boto3->farm==0.5.0) (1.21.38)\n",
            "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from boto3->farm==0.5.0) (0.5.0)\n",
            "Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from flask->farm==0.5.0) (2.11.3)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from flask->farm==0.5.0) (1.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->flask->farm==0.5.0) (2.0.1)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from flask-restplus->farm==0.5.0) (2018.9)\n",
            "Requirement already satisfied: aniso8601>=0.82 in /usr/local/lib/python3.7/dist-packages (from flask-restplus->farm==0.5.0) (9.0.1)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from flask-restplus->farm==0.5.0) (2.6.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->alembic->mlflow==1.0.0->farm==0.5.0) (3.5.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3.3.1->farm==0.5.0) (2.4.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.3.1->farm==0.5.0) (1.0.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn->farm==0.5.0) (0.22.2.post1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6RKA1bxkKij",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f28c8b3b-49ca-4544-9071-3ece99e8c17c"
      },
      "source": [
        "# Here are the imports we need\n",
        "\n",
        "import torch\n",
        "from farm.modeling.tokenization import Tokenizer\n",
        "from farm.data_handler.processor import TextClassificationProcessor\n",
        "from farm.data_handler.data_silo import DataSilo\n",
        "from farm.modeling.language_model import LanguageModel\n",
        "from farm.modeling.prediction_head import TextClassificationHead\n",
        "from farm.modeling.adaptive_model import AdaptiveModel\n",
        "from farm.modeling.optimization import initialize_optimizer\n",
        "from farm.train import Trainer\n",
        "from farm.utils import MLFlowLogger\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "09/09/2021 13:56:17 - INFO - farm.modeling.prediction_head -   Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XL-on42YOWwC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1cf84ff-2e74-49c4-fd0d-4395625f31a0"
      },
      "source": [
        "# Farm allows simple logging of many parameters & metrics. Let's use the MLflow framework to track our experiment ...\n",
        "# You will see your results on https://public-mlflow.deepset.ai/\n",
        "ml_logger = MLFlowLogger(tracking_uri=\"https://public-mlflow.deepset.ai/\")\n",
        "ml_logger.init_experiment(experiment_name=\"Public_FARM\", run_name=\"Tutorial1_Colab\")\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " __          __  _                            _        \n",
            " \\ \\        / / | |                          | |       \n",
            "  \\ \\  /\\  / /__| | ___ ___  _ __ ___   ___  | |_ ___  \n",
            "   \\ \\/  \\/ / _ \\ |/ __/ _ \\| '_ ` _ \\ / _ \\ | __/ _ \\ \n",
            "    \\  /\\  /  __/ | (_| (_) | | | | | |  __/ | || (_) |\n",
            "     \\/  \\/ \\___|_|\\___\\___/|_| |_| |_|\\___|  \\__\\___/ \n",
            "  ______      _____  __  __  \n",
            " |  ____/\\   |  __ \\|  \\/  |              _.-^-._    .--.\n",
            " | |__ /  \\  | |__) | \\  / |           .-'   _   '-. |__|\n",
            " |  __/ /\\ \\ |  _  /| |\\/| |          /     |_|     \\|  |\n",
            " | | / ____ \\| | \\ \\| |  | |         /               \\  |\n",
            " |_|/_/    \\_\\_|  \\_\\_|  |_|        /|     _____     |\\ |\n",
            "                                     |    |==|==|    |  |\n",
            "|---||---|---|---|---|---|---|---|---|    |--|--|    |  |\n",
            "|---||---|---|---|---|---|---|---|---|    |==|==|    |  |\n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            " \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6uTw7w3kPcJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85383bb0-db6c-46ae-dc44-ee0d0b27c2fd"
      },
      "source": [
        "# We need to fetch the right device to drive the growth of our model\n",
        "# Make sure that you have gpu turned on in this notebook by going to\n",
        "# Runtime>Change runtime type and select GPU as Hardware accelerator.\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Devices available: {}\".format(device))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Devices available: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPogG9KLk-Mv"
      },
      "source": [
        "### Data Handling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMQtJfq775KM"
      },
      "source": [
        "In FARM the Processor contains the functions which handle the conversion from file or request to PyTorch Datasets. In essence, it prepares data to be consumed by the modelling components. This is done in stages to allow for easier debugging. It should be able to handle file input or requests. This class contains everything that needs to be customized when adapting a new dataset. Custom datasets can be handled by extending the Processor (e.g. see CONLLProcessor).\n",
        "\n",
        "The DataSilo is a generic class that stores the train, dev and test data sets. It calls upon the methods from the Processor to do the loading and then exposes a DataLoader for each set. In cases where there is not a separate dev file, it will create one by slicing the train set.\n",
        "![data_handling](https://farm.deepset.ai/_images/data_silo_no_bg.jpg)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-2k0Zock2J5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da5b8402-5a34-4742-e380-a22b1ce70e53"
      },
      "source": [
        "# Here we initialize a tokenizer that will be used for preprocessing text\n",
        "# This is the BERT Tokenizer which uses the byte pair encoding method.\n",
        "# It is currently loaded with a German model\n",
        "\n",
        "tokenizer = Tokenizer.load(\n",
        "    pretrained_model_name_or_path=\"distilbert-base-uncased-finetuned-sst-2-english\",\n",
        "    do_lower_case=False)\n",
        "\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "09/09/2021 13:56:18 - INFO - farm.modeling.tokenization -   Loading tokenizer of type 'DistilBertTokenizer'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08ycfnEIlJa4"
      },
      "source": [
        "# In order to prepare the data for the model, we need a set of\n",
        "# functions to transform data files into PyTorch Datasets.\n",
        "# We group these together in Processor objects.\n",
        "# We will need a new Processor object for each new source of data.\n",
        "# The abstract class can be found in farm.data_handling.processor.Processor\n",
        "# TOXIC = 1\n",
        "# OTHER = 0\n",
        "LABEL_LIST = [\"0\", \"1\"]\n",
        "processor = TextClassificationProcessor(tokenizer=tokenizer,\n",
        "                                        max_seq_len=128,\n",
        "                                        warmup = 600,\n",
        "                                        train_filename=\"train.tsv\",\n",
        "                                        test_filename=\"test_with_solutions.tsv\",\n",
        "                                        data_dir=\"../content/detecting-insults-in-social-commentary\",\n",
        "                                        label_list=LABEL_LIST,\n",
        "                                        metric=\"f1_macro\",\n",
        "                                        text_column_name=\"Comment\",\n",
        "                                        label_column_name=\"Insult\")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zds83m5klLW8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0ab7c31-c333-4b8c-8c39-9430025ecb9f"
      },
      "source": [
        "# We need a DataSilo in order to keep our train, dev and test sets separate.\n",
        "# The DataSilo will call the functions in the Processor to generate these sets.\n",
        "# From the DataSilo, we can fetch a PyTorch DataLoader object which will\n",
        "# be passed on to the model.\n",
        "# Here is a good place to define a batch size for the model\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "data_silo = DataSilo(\n",
        "    processor=processor,\n",
        "    batch_size=BATCH_SIZE)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "09/09/2021 13:56:20 - INFO - farm.data_handler.data_silo -   \n",
            "Loading data into the data silo ... \n",
            "              ______\n",
            "               |o  |   !\n",
            "   __          |:`_|---'-.\n",
            "  |__|______.-/ _ \\-----.|       \n",
            " (o)(o)------'\\ _ /     ( )      \n",
            " \n",
            "09/09/2021 13:56:20 - INFO - farm.data_handler.data_silo -   Loading train set from: ../content/detecting-insults-in-social-commentary/train.tsv \n",
            "09/09/2021 13:56:20 - INFO - farm.data_handler.data_silo -   Got ya 1 parallel workers to convert 3947 dictionaries to pytorch datasets (chunksize = 790)...\n",
            "09/09/2021 13:56:20 - INFO - farm.data_handler.data_silo -    0 \n",
            "09/09/2021 13:56:20 - INFO - farm.data_handler.data_silo -   /w\\\n",
            "09/09/2021 13:56:20 - INFO - farm.data_handler.data_silo -   /'\\\n",
            "09/09/2021 13:56:20 - INFO - farm.data_handler.data_silo -   \n",
            "Preprocessing Dataset ../content/detecting-insults-in-social-commentary/train.tsv:   0%|          | 0/3947 [00:00<?, ? Dicts/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils.py:460: FutureWarning: `is_pretokenized` is deprecated and will be removed in a future version, use `is_split_into_words` instead.\n",
            "  FutureWarning,\n",
            "09/09/2021 13:56:22 - INFO - farm.data_handler.processor -   *** Show 2 random examples ***\n",
            "09/09/2021 13:56:22 - INFO - farm.data_handler.processor -   \n",
            "\n",
            "      .--.        _____                       _      \n",
            "    .'_\\/_'.     / ____|                     | |     \n",
            "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
            "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
            "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
            "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
            "   (/\\||/                             |_|           \n",
            "______\\||/___________________________________________                     \n",
            "\n",
            "ID: 355-0\n",
            "Clear Text: \n",
            " \ttext_classification_label: 0\n",
            " \ttext: \"And the last day is upon us. Will I be the lucky one? Doubt it, but it's still a sexy piece of hardware!\"\n",
            "Tokenized: \n",
            " \ttokens: ['\"', '[UNK]', 'the', 'last', 'day', 'is', 'upon', 'us', '.', '[UNK]', '[UNK]', 'be', 'the', 'lucky', 'one', '?', '[UNK]', 'it', ',', 'but', 'it', \"'\", 's', 'still', 'a', 'sexy', 'piece', 'of', 'hardware', '!', '\"']\n",
            " \toffsets: [0, 1, 5, 9, 14, 18, 21, 26, 28, 30, 35, 37, 40, 44, 50, 53, 55, 61, 63, 65, 69, 71, 72, 74, 80, 82, 87, 93, 96, 104, 105]\n",
            " \tstart_of_word: [True, False, True, True, True, True, True, True, False, True, True, True, True, True, True, False, True, True, False, True, True, False, False, True, True, True, True, True, True, False, False]\n",
            "Features: \n",
            " \tinput_ids: [101, 1000, 100, 1996, 2197, 2154, 2003, 2588, 2149, 1012, 100, 100, 2022, 1996, 5341, 2028, 1029, 100, 2009, 1010, 2021, 2009, 1005, 1055, 2145, 1037, 7916, 3538, 1997, 8051, 999, 1000, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \ttext_classification_label_ids: [0]\n",
            "_____________________________________________________\n",
            "09/09/2021 13:56:22 - INFO - farm.data_handler.processor -   \n",
            "\n",
            "      .--.        _____                       _      \n",
            "    .'_\\/_'.     / ____|                     | |     \n",
            "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
            "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
            "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
            "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
            "   (/\\||/                             |_|           \n",
            "______\\||/___________________________________________                     \n",
            "\n",
            "ID: 685-0\n",
            "Clear Text: \n",
            " \ttext_classification_label: 0\n",
            " \ttext: \"@Blaine Jungwirth @clemsaucy Futurama style\"\n",
            "Tokenized: \n",
            " \ttokens: ['\"', '@', '[UNK]', '[UNK]', '@', 'cl', '##em', '##sau', '##cy', '[UNK]', 'style', '\"']\n",
            " \toffsets: [0, 1, 2, 9, 19, 20, 22, 24, 27, 30, 39, 44]\n",
            " \tstart_of_word: [True, False, False, True, True, False, False, False, False, True, True, False]\n",
            "Features: \n",
            " \tinput_ids: [101, 1000, 1030, 100, 100, 1030, 18856, 6633, 23823, 5666, 100, 2806, 1000, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \ttext_classification_label_ids: [0]\n",
            "_____________________________________________________\n",
            "Preprocessing Dataset ../content/detecting-insults-in-social-commentary/train.tsv: 100%|██████████| 3947/3947 [00:10<00:00, 386.04 Dicts/s]\n",
            "09/09/2021 13:56:30 - INFO - farm.data_handler.data_silo -   Loading dev set as a slice of train set\n",
            "09/09/2021 13:56:30 - INFO - farm.data_handler.data_silo -   Took 787 samples out of train set to create dev set (dev split is roughly 0.1)\n",
            "09/09/2021 13:56:30 - INFO - farm.data_handler.data_silo -   Loading test set from: ../content/detecting-insults-in-social-commentary/test_with_solutions.tsv\n",
            "09/09/2021 13:56:30 - INFO - farm.data_handler.data_silo -   Got ya 1 parallel workers to convert 2647 dictionaries to pytorch datasets (chunksize = 530)...\n",
            "09/09/2021 13:56:30 - INFO - farm.data_handler.data_silo -    0 \n",
            "09/09/2021 13:56:30 - INFO - farm.data_handler.data_silo -   /w\\\n",
            "09/09/2021 13:56:30 - INFO - farm.data_handler.data_silo -   / \\\n",
            "09/09/2021 13:56:30 - INFO - farm.data_handler.data_silo -   \n",
            "Preprocessing Dataset ../content/detecting-insults-in-social-commentary/test_with_solutions.tsv:   0%|          | 0/2647 [00:00<?, ? Dicts/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils.py:460: FutureWarning: `is_pretokenized` is deprecated and will be removed in a future version, use `is_split_into_words` instead.\n",
            "  FutureWarning,\n",
            "09/09/2021 13:56:33 - INFO - farm.data_handler.processor -   *** Show 2 random examples ***\n",
            "09/09/2021 13:56:33 - INFO - farm.data_handler.processor -   \n",
            "\n",
            "      .--.        _____                       _      \n",
            "    .'_\\/_'.     / ____|                     | |     \n",
            "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
            "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
            "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
            "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
            "   (/\\||/                             |_|           \n",
            "______\\||/___________________________________________                     \n",
            "\n",
            "ID: 489-0\n",
            "Clear Text: \n",
            " \ttext_classification_label: 0\n",
            " \ttext: \"ok. Off you go.\"\n",
            "Tokenized: \n",
            " \ttokens: ['\"', 'ok', '.', '[UNK]', 'you', 'go', '.', '\"']\n",
            " \toffsets: [0, 1, 3, 5, 9, 13, 15, 16]\n",
            " \tstart_of_word: [True, False, False, True, True, True, False, False]\n",
            "Features: \n",
            " \tinput_ids: [101, 1000, 7929, 1012, 100, 2017, 2175, 1012, 1000, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \ttext_classification_label_ids: [0]\n",
            "_____________________________________________________\n",
            "09/09/2021 13:56:33 - INFO - farm.data_handler.processor -   \n",
            "\n",
            "      .--.        _____                       _      \n",
            "    .'_\\/_'.     / ____|                     | |     \n",
            "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
            "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
            "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
            "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
            "   (/\\||/                             |_|           \n",
            "______\\||/___________________________________________                     \n",
            "\n",
            "ID: 227-0\n",
            "Clear Text: \n",
            " \ttext_classification_label: 0\n",
            " \ttext: \"It is entirely relevant, but you are afraid of the point I will draw if you were to answer the question truthfully. Your choice to evade is duly noted.\"\n",
            "Tokenized: \n",
            " \ttokens: ['\"', '[UNK]', 'is', 'entirely', 'relevant', ',', 'but', 'you', 'are', 'afraid', 'of', 'the', 'point', '[UNK]', 'will', 'draw', 'if', 'you', 'were', 'to', 'answer', 'the', 'question', 'truth', '##fully', '.', '[UNK]', 'choice', 'to', 'evade', 'is', 'duly', 'noted', '.', '\"']\n",
            " \toffsets: [0, 1, 4, 7, 16, 24, 26, 30, 34, 38, 45, 48, 52, 58, 60, 65, 70, 73, 77, 82, 85, 92, 96, 105, 110, 115, 117, 122, 129, 132, 138, 141, 146, 151, 152]\n",
            " \tstart_of_word: [True, False, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, True, True, True, True, True, True, True, False, False]\n",
            "Features: \n",
            " \tinput_ids: [101, 1000, 100, 2003, 4498, 7882, 1010, 2021, 2017, 2024, 4452, 1997, 1996, 2391, 100, 2097, 4009, 2065, 2017, 2020, 2000, 3437, 1996, 3160, 3606, 7699, 1012, 100, 3601, 2000, 26399, 2003, 25073, 3264, 1012, 1000, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \ttext_classification_label_ids: [0]\n",
            "_____________________________________________________\n",
            "Preprocessing Dataset ../content/detecting-insults-in-social-commentary/test_with_solutions.tsv: 100%|██████████| 2647/2647 [00:07<00:00, 349.54 Dicts/s]\n",
            "09/09/2021 13:56:38 - INFO - farm.data_handler.data_silo -   Examples in train: 3160\n",
            "09/09/2021 13:56:38 - INFO - farm.data_handler.data_silo -   Examples in dev  : 787\n",
            "09/09/2021 13:56:38 - INFO - farm.data_handler.data_silo -   Examples in test : 2647\n",
            "09/09/2021 13:56:38 - INFO - farm.data_handler.data_silo -   \n",
            "09/09/2021 13:56:38 - INFO - farm.data_handler.data_silo -   Longest sequence length observed after clipping:     128\n",
            "09/09/2021 13:56:38 - INFO - farm.data_handler.data_silo -   Average sequence length after clipping: 43.08987341772152\n",
            "09/09/2021 13:56:38 - INFO - farm.data_handler.data_silo -   Proportion clipped:      0.07436708860759493\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YG8toIFclN2F"
      },
      "source": [
        "### Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cw4CJvz5lRbj"
      },
      "source": [
        "![modeling](https://farm.deepset.ai/_images/adaptive_model_no_bg.jpg)\n",
        "In FARM, we make a strong distinction between the language model and prediction head so that you can mix and match different building blocks for your needs.\n",
        "\n",
        "For example, in the transfer learning paradigm, you might have the one language model that you will be using for both document classification and NER. Or perhaps you have a pretrained language model which you would like to adapt to your domain, then use it for a downstream task such as question answering. \n",
        "\n",
        "All this is possible within FARM and requires only the replacement of a few modular components, as we shall see below.\n",
        "\n",
        "Let's first have a look at how we might set up a model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4TaqNaTlVmm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4ddd14d-331c-49f8-9555-8a4fe2dab1c8"
      },
      "source": [
        "# The language model is the foundation on which modern NLP systems are built.\n",
        "# They encapsulate a general understanding of sentence semantics\n",
        "# and are not specific to any one task.\n",
        "\n",
        "# Here we are using Google's BERT model as implemented by HuggingFace. \n",
        "# The model being loaded is a German model that we trained. \n",
        "# You can also change the MODEL_NAME_OR_PATH to point to a BERT model that you\n",
        "# have saved or download one connected to the HuggingFace repository.\n",
        "# See farm.modeling.language_model.PRETRAINED_MODEL_ARCHIVE_MAP for a list of\n",
        "# available models\n",
        "\n",
        "MODEL_NAME_OR_PATH = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "# MODEL_NAME_OR_PATH = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "language_model = LanguageModel.load(MODEL_NAME_OR_PATH)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "09/09/2021 13:56:40 - INFO - farm.modeling.language_model -   Automatically detected language from language model name: english\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKpqm6solWce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d68ee5b3-7d24-4fe0-8f22-b9c37f4f5694"
      },
      "source": [
        "# A prediction head is a model that processes the output of the language model\n",
        "# for a specific task.\n",
        "# Prediction heads will look different depending on whether you're doing text classification\n",
        "# Named Entity Recognition (NER), question answering or some other task.\n",
        "# They should generate logits over the available prediction classes and contain methods\n",
        "# to convert these logits to losses or predictions \n",
        "\n",
        "# Here we use TextClassificationHead which receives a single fixed length sentence vector\n",
        "# and processes it using a feed forward neural network. layer_dims is a list of dimensions:\n",
        "# [input_dims, hidden_1_dims, hidden_2_dims ..., output_dims]\n",
        "\n",
        "# Here by default we have a single layer network.\n",
        "# It takes in a vector of length 768 (the default size of BERT's output).\n",
        "# It outputs a vector of length 2 (the number of classes in the GermEval18 (coarse) dataset)\n",
        "\n",
        "prediction_head = TextClassificationHead(num_labels=len(LABEL_LIST), class_weights=data_silo.calculate_class_weights(task_name=\"text_classification\"))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "09/09/2021 13:56:41 - INFO - farm.modeling.prediction_head -   Prediction head initialized with size [768, 2]\n",
            "09/09/2021 13:56:41 - INFO - farm.modeling.prediction_head -   Using class weights for task 'text_classification': [0.6791237 1.8956834]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQL1HGo2lZEo"
      },
      "source": [
        "# The language model and prediction head are coupled together in the Adaptive Model.\n",
        "# This class takes care of model saving and loading and also coordinates\n",
        "# cases where there is more than one prediction head.\n",
        "\n",
        "# EMBEDS_DROPOUT_PROB is the probability that an element of the output vector from the\n",
        "# language model will be set to zero.\n",
        "# EMBEDS_DROPOUT_PROB = 0.1 distilbert-base-uncased-finetuned-sst-2-english_1.PNG\n",
        "# EMBEDS_DROPOUT_PROB = 0.01\n",
        "# EMBEDS_DROPOUT_PROB = 0.2 distilbert-base-uncased-finetuned-sst-2-english_2.PNG\n",
        "EMBEDS_DROPOUT_PROB = 0.01\n",
        "\n",
        "model = AdaptiveModel(\n",
        "    language_model=language_model,\n",
        "    prediction_heads=[prediction_head],\n",
        "    embeds_dropout_prob=EMBEDS_DROPOUT_PROB,\n",
        "    lm_output_types=[\"per_sequence\"],\n",
        "    device=device)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UE861jLalax1"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4db05tC7lcKy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5bb14b8-7c13-41a2-e319-6d7cae8c4342"
      },
      "source": [
        "# Here we initialize a Bert Adam optimizer that has a linear warmup and warmdown\n",
        "# Here you can set learning rate, the warmup proportion and number of epochs to train for\n",
        "\n",
        "#LEARNING_RATE = 2e-5 # distilbert-base-uncased-finetuned-sst-2-english_1.PNG\n",
        "#LEARNING_RATE = 1e-7\n",
        "LEARNING_RATE = 1e-5  # distilbert-base-uncased-finetuned-sst-2-english_2.PNG\n",
        "\n",
        "N_EPOCHS = 3\n",
        "\n",
        "model, optimizer, lr_schedule = initialize_optimizer(\n",
        "    model=model,\n",
        "    device=device,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    n_batches=len(data_silo.loaders[\"train\"]),\n",
        "    n_epochs=N_EPOCHS)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "09/09/2021 13:56:43 - INFO - farm.modeling.optimization -   Loading optimizer `TransformersAdamW`: '{'correct_bias': False, 'weight_decay': 0.01, 'lr': 1e-05}'\n",
            "09/09/2021 13:56:44 - INFO - farm.modeling.optimization -   Using scheduler 'get_linear_schedule_with_warmup'\n",
            "09/09/2021 13:56:44 - INFO - farm.modeling.optimization -   Loading schedule `get_linear_schedule_with_warmup`: '{'num_warmup_steps': 29.700000000000003, 'num_training_steps': 297}'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpkluUz3lfJn"
      },
      "source": [
        "# Training loop handled by this\n",
        "# It will also trigger evaluation during training using the dev data\n",
        "# and after training using the test data.\n",
        "\n",
        "# Set N_GPU to a positive value if CUDA is available\n",
        "N_GPU = 1\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    optimizer=optimizer,\n",
        "    data_silo=data_silo,\n",
        "    epochs=N_EPOCHS,\n",
        "    n_gpu=N_GPU,\n",
        "    lr_schedule=lr_schedule,\n",
        "    device=device, \n",
        ")"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWLMesyre1FB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b26f3569-cb88-487b-afe4-75db3834e833"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Sep  9 13:56:46 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.63.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   64C    P0    72W / 149W |    628MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VuSTk5zAlc0A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a20cdbf6-e10b-4a47-d913-cff20579e021"
      },
      "source": [
        "\n",
        "model = trainer.train()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "09/09/2021 13:56:46 - INFO - farm.train -   \n",
            " \n",
            "\n",
            "          &&& &&  & &&             _____                   _             \n",
            "      && &\\/&\\|& ()|/ @, &&       / ____|                 (_)            \n",
            "      &\\/(/&/&||/& /_/)_&/_&     | |  __ _ __ _____      ___ _ __   __ _ \n",
            "   &() &\\/&|()|/&\\/ '%\" & ()     | | |_ | '__/ _ \\ \\ /\\ / / | '_ \\ / _` |\n",
            "  &_\\_&&_\\ |& |&&/&__%_/_& &&    | |__| | | | (_) \\ V  V /| | | | | (_| |\n",
            "&&   && & &| &| /& & % ()& /&&    \\_____|_|  \\___/ \\_/\\_/ |_|_| |_|\\__, |\n",
            " ()&_---()&\\&\\|&&-&&--%---()~                                       __/ |\n",
            "     &&     \\|||                                                   |___/\n",
            "             |||\n",
            "             |||\n",
            "             |||\n",
            "       , -=-~  .-^- _\n",
            "              `\n",
            "\n",
            "Train epoch 0/2 (Cur. train loss: 0.3698): 100%|██████████| 99/99 [01:10<00:00,  1.40it/s]\n",
            "Train epoch 1/2 (Cur. train loss: 0.1316):   1%|          | 1/99 [00:00<01:04,  1.53it/s]\n",
            "Evaluating: 100%|██████████| 25/25 [00:05<00:00,  4.29it/s]\n",
            "09/09/2021 13:58:04 - INFO - farm.eval -   \n",
            "\n",
            "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "***************************************************\n",
            "***** EVALUATION | DEV SET | AFTER 100 BATCHES *****\n",
            "***************************************************\n",
            "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "\n",
            "09/09/2021 13:58:04 - INFO - farm.eval -   \n",
            " _________ text_classification _________\n",
            "09/09/2021 13:58:05 - INFO - farm.eval -   loss: 0.4374873301882907\n",
            "09/09/2021 13:58:05 - INFO - farm.eval -   task_name: text_classification\n",
            "09/09/2021 13:58:05 - INFO - farm.eval -   f1_macro: 0.7650980001989851\n",
            "09/09/2021 13:58:05 - INFO - farm.eval -   report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9283    0.7706    0.8421       571\n",
            "           1     0.5815    0.8426    0.6881       216\n",
            "\n",
            "    accuracy                         0.7903       787\n",
            "   macro avg     0.7549    0.8066    0.7651       787\n",
            "weighted avg     0.8331    0.7903    0.7998       787\n",
            "\n",
            "Train epoch 1/2 (Cur. train loss: 0.6274): 100%|██████████| 99/99 [01:16<00:00,  1.29it/s]\n",
            "Train epoch 2/2 (Cur. train loss: 0.1884):   2%|▏         | 2/99 [00:01<01:03,  1.53it/s]\n",
            "Evaluating: 100%|██████████| 25/25 [00:05<00:00,  4.30it/s]\n",
            "09/09/2021 13:59:21 - INFO - farm.eval -   \n",
            "\n",
            "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "***************************************************\n",
            "***** EVALUATION | DEV SET | AFTER 200 BATCHES *****\n",
            "***************************************************\n",
            "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "\n",
            "09/09/2021 13:59:21 - INFO - farm.eval -   \n",
            " _________ text_classification _________\n",
            "09/09/2021 13:59:22 - INFO - farm.eval -   loss: 0.43551503718278006\n",
            "09/09/2021 13:59:22 - INFO - farm.eval -   task_name: text_classification\n",
            "09/09/2021 13:59:22 - INFO - farm.eval -   f1_macro: 0.7816979009925068\n",
            "09/09/2021 13:59:22 - INFO - farm.eval -   report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9079    0.8284    0.8663       571\n",
            "           1     0.6316    0.7778    0.6971       216\n",
            "\n",
            "    accuracy                         0.8145       787\n",
            "   macro avg     0.7697    0.8031    0.7817       787\n",
            "weighted avg     0.8320    0.8145    0.8199       787\n",
            "\n",
            "Train epoch 2/2 (Cur. train loss: 0.0883): 100%|██████████| 99/99 [01:16<00:00,  1.30it/s]\n",
            "Evaluating: 100%|██████████| 83/83 [00:19<00:00,  4.25it/s]\n",
            "09/09/2021 14:00:49 - INFO - farm.eval -   \n",
            "\n",
            "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "***************************************************\n",
            "***** EVALUATION | TEST SET | AFTER 297 BATCHES *****\n",
            "***************************************************\n",
            "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "\n",
            "09/09/2021 14:00:49 - INFO - farm.eval -   \n",
            " _________ text_classification _________\n",
            "09/09/2021 14:00:49 - INFO - farm.eval -   loss: 0.5152641850505003\n",
            "09/09/2021 14:00:49 - INFO - farm.eval -   task_name: text_classification\n",
            "09/09/2021 14:00:50 - INFO - farm.eval -   f1_macro: 0.8051246217321448\n",
            "09/09/2021 14:00:50 - INFO - farm.eval -   report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9016    0.8910    0.8963      1954\n",
            "           1     0.7025    0.7258    0.7140       693\n",
            "\n",
            "    accuracy                         0.8478      2647\n",
            "   macro avg     0.8021    0.8084    0.8051      2647\n",
            "weighted avg     0.8495    0.8478    0.8485      2647\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0r8b3etug_F4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "135900aa-d47d-42b6-9d11-a8d1f3100be2"
      },
      "source": [
        "# Test your model on a sample (Inference)\n",
        "from farm.infer import Inferencer\n",
        "from pprint import PrettyPrinter\n",
        "\n",
        "infer_model = Inferencer(processor=processor, model=model, task_type=\"text_classification\", gpu=True)\n",
        "\n",
        "basic_texts = [\n",
        "    {\"text\": \"Martin is an idiot\"},\n",
        "    {\"text\": \"Martin Müller plays Voleyball in Berlin\"},\n",
        "]\n",
        "result = infer_model.inference_from_dicts(dicts=basic_texts)\n",
        "PrettyPrinter().pprint(result)\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "09/09/2021 14:00:50 - INFO - farm.utils -   device: cuda n_gpu: 1, distributed training: False, automatic mixed precision training: None\n",
            "09/09/2021 14:00:50 - INFO - farm.infer -   Got ya 1 parallel workers to do inference ...\n",
            "09/09/2021 14:00:50 - INFO - farm.infer -    0 \n",
            "09/09/2021 14:00:50 - INFO - farm.infer -   /w\\\n",
            "09/09/2021 14:00:50 - INFO - farm.infer -   /'\\\n",
            "09/09/2021 14:00:50 - INFO - farm.infer -   \n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils.py:460: FutureWarning: `is_pretokenized` is deprecated and will be removed in a future version, use `is_split_into_words` instead.\n",
            "  FutureWarning,\n",
            "09/09/2021 14:00:50 - INFO - farm.data_handler.processor -   *** Show 2 random examples ***\n",
            "09/09/2021 14:00:50 - INFO - farm.data_handler.processor -   \n",
            "\n",
            "      .--.        _____                       _      \n",
            "    .'_\\/_'.     / ____|                     | |     \n",
            "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
            "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
            "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
            "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
            "   (/\\||/                             |_|           \n",
            "______\\||/___________________________________________                     \n",
            "\n",
            "ID: 1-0\n",
            "Clear Text: \n",
            " \ttext: Martin Müller plays Voleyball in Berlin\n",
            "Tokenized: \n",
            " \ttokens: ['[UNK]', '[UNK]', 'plays', '[UNK]', 'in', '[UNK]']\n",
            " \toffsets: [0, 7, 14, 20, 30, 33]\n",
            " \tstart_of_word: [True, True, True, True, True, True]\n",
            "Features: \n",
            " \tinput_ids: [101, 100, 100, 3248, 100, 1999, 100, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "_____________________________________________________\n",
            "09/09/2021 14:00:50 - INFO - farm.data_handler.processor -   \n",
            "\n",
            "      .--.        _____                       _      \n",
            "    .'_\\/_'.     / ____|                     | |     \n",
            "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
            "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
            "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
            "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
            "   (/\\||/                             |_|           \n",
            "______\\||/___________________________________________                     \n",
            "\n",
            "ID: 1-0\n",
            "Clear Text: \n",
            " \ttext: Martin Müller plays Voleyball in Berlin\n",
            "Tokenized: \n",
            " \ttokens: ['[UNK]', '[UNK]', 'plays', '[UNK]', 'in', '[UNK]']\n",
            " \toffsets: [0, 7, 14, 20, 30, 33]\n",
            " \tstart_of_word: [True, True, True, True, True, True]\n",
            "Features: \n",
            " \tinput_ids: [101, 100, 100, 3248, 100, 1999, 100, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "_____________________________________________________\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00, 28.57 Batches/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'predictions': [{'context': 'Martin is an idiot',\n",
            "                   'end': None,\n",
            "                   'label': '1',\n",
            "                   'probability': 0.9921326,\n",
            "                   'start': None},\n",
            "                  {'context': 'Martin Müller plays Voleyball in Berlin',\n",
            "                   'end': None,\n",
            "                   'label': '0',\n",
            "                   'probability': 0.8594164,\n",
            "                   'start': None}],\n",
            "  'task': 'text_classification'}]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KiT5IEsV1BhU"
      },
      "source": [
        "model.save('../data/detecting-insults-in-social-commentary/first_try/model')\n",
        "processor.save('../data/detecting-insults-in-social-commentary/first_try/processor')"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0BAYL1Iliiv"
      },
      "source": [
        "# Switch to NER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYEnZo5cll3J"
      },
      "source": [
        "In a transfer learning paradigm, there is a core computation that is shared amongst all tasks. FARM's modular structure means that you can easily swap out different building blocks to make the same language model work for many different tasks.\n",
        "\n",
        "We can adapt the above text classification model to NER by simply switching out the processor and prediction head."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prov6seQlmLq"
      },
      "source": [
        "# Import the new building blocks\n",
        "\n",
        "from farm.data_handler.processor import NERProcessor\n",
        "from farm.modeling.prediction_head import TokenClassificationHead\n",
        "ml_logger.init_experiment(experiment_name=\"Public_FARM\", run_name=\"Tutorial1_Colab_NER\")\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "reITkXdilqL9"
      },
      "source": [
        "# This processor will preprocess the data for the CoNLL03 NER task\n",
        "ner_labels = [\"[PAD]\", \"X\", \"O\", \"B-MISC\", \"I-MISC\", \"B-PER\", \"I-PER\", \"B-ORG\", \"I-ORG\", \"B-LOC\", \"I-LOC\", \"B-OTH\", \"I-OTH\"]\n",
        "\n",
        "ner_processor = NERProcessor(tokenizer=tokenizer, \n",
        "                             max_seq_len=128, \n",
        "                             data_dir=\"../data/conll03-de\",\n",
        "                             label_list=ner_labels,\n",
        "                             metric=\"seq_f1\")"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJ-ddMWvlswY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f16880dc-6359-43d6-a6c5-2154f8e89677"
      },
      "source": [
        "# This prediction head is also a feed forward neural network but expects one\n",
        "# vector per token in the input sequence and will generate a set of logits\n",
        "# for each input\n",
        "\n",
        "ner_prediction_head = TokenClassificationHead(num_labels=len(ner_labels))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "09/09/2021 14:00:53 - INFO - farm.modeling.prediction_head -   Prediction head initialized with size [768, 13]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aY0IYYBXlu00",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f01a7690-1f67-42a3-e449-3c4f4e0b400f"
      },
      "source": [
        "# We can integrate these new pieces with the rest using this code\n",
        "# It is pretty much the same structure as what we had above for text classification\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "EMBEDS_DROPOUT_PROB = 0.1\n",
        "LEARNING_RATE = 2e-5\n",
        "N_EPOCHS = 1\n",
        "N_GPU = 1\n",
        "\n",
        "data_silo = DataSilo(\n",
        "    processor=ner_processor,\n",
        "    batch_size=BATCH_SIZE)\n",
        "\n",
        "model = AdaptiveModel(\n",
        "    language_model=language_model,\n",
        "    prediction_heads=[ner_prediction_head],\n",
        "    embeds_dropout_prob=EMBEDS_DROPOUT_PROB,\n",
        "    lm_output_types=[\"per_token\"],\n",
        "    device=device)\n",
        "\n",
        "model, optimizer, lr_schedule = initialize_optimizer(\n",
        "    model=model,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    n_batches=len(data_silo.loaders[\"train\"]),\n",
        "    n_epochs=N_EPOCHS,\n",
        "    device=device)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    optimizer=optimizer,\n",
        "    data_silo=data_silo,\n",
        "    epochs=N_EPOCHS,\n",
        "    n_gpu=N_GPU,\n",
        "    lr_schedule=lr_schedule,\n",
        "    device=device,\n",
        ")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "09/09/2021 14:00:53 - INFO - farm.data_handler.data_silo -   \n",
            "Loading data into the data silo ... \n",
            "              ______\n",
            "               |o  |   !\n",
            "   __          |:`_|---'-.\n",
            "  |__|______.-/ _ \\-----.|       \n",
            " (o)(o)------'\\ _ /     ( )      \n",
            " \n",
            "09/09/2021 14:00:53 - INFO - farm.data_handler.data_silo -   Loading train set from: ../data/conll03-de/train.txt \n",
            "09/09/2021 14:00:53 - ERROR - farm.data_handler.utils -   Separator \t for dataset German CONLL03 does not match the requirements. Setting seperator to whitespace\n",
            "09/09/2021 14:00:53 - INFO - farm.data_handler.utils -    Couldn't find ../data/conll03-de/train.txt locally. Trying to download ...\n",
            "09/09/2021 14:00:53 - INFO - farm.data_handler.utils -   downloading and extracting file conll03-de to dir /data\n",
            "09/09/2021 14:00:55 - INFO - farm.data_handler.data_silo -   Got ya 1 parallel workers to convert 12152 dictionaries to pytorch datasets (chunksize = 2000)...\n",
            "09/09/2021 14:00:55 - INFO - farm.data_handler.data_silo -    0 \n",
            "09/09/2021 14:00:55 - INFO - farm.data_handler.data_silo -   /w\\\n",
            "09/09/2021 14:00:55 - INFO - farm.data_handler.data_silo -   / \\\n",
            "09/09/2021 14:00:55 - INFO - farm.data_handler.data_silo -   \n",
            "Preprocessing Dataset ../data/conll03-de/train.txt:   0%|          | 0/12152 [00:00<?, ? Dicts/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils.py:460: FutureWarning: `is_pretokenized` is deprecated and will be removed in a future version, use `is_split_into_words` instead.\n",
            "  FutureWarning,\n",
            "09/09/2021 14:00:58 - INFO - farm.data_handler.processor -   *** Show 2 random examples ***\n",
            "09/09/2021 14:00:58 - INFO - farm.data_handler.processor -   \n",
            "\n",
            "      .--.        _____                       _      \n",
            "    .'_\\/_'.     / ____|                     | |     \n",
            "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
            "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
            "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
            "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
            "   (/\\||/                             |_|           \n",
            "______\\||/___________________________________________                     \n",
            "\n",
            "ID: 1534-0\n",
            "Clear Text: \n",
            " \ttext: Henrik Ibsen :\n",
            " \tner_label: ['B-PER', 'I-PER', 'O']\n",
            "Tokenized: \n",
            " \ttokens: ['[UNK]', '[UNK]', ':']\n",
            " \toffsets: [0, 7, 13]\n",
            " \tstart_of_word: [True, True, True]\n",
            "Features: \n",
            " \tinput_ids: [101, 100, 100, 1024, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tpadding_mask: [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tinitial_mask: [0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tner_label_ids: [1, 5, 6, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "_____________________________________________________\n",
            "09/09/2021 14:00:58 - INFO - farm.data_handler.processor -   \n",
            "\n",
            "      .--.        _____                       _      \n",
            "    .'_\\/_'.     / ____|                     | |     \n",
            "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
            "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
            "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
            "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
            "   (/\\||/                             |_|           \n",
            "______\\||/___________________________________________                     \n",
            "\n",
            "ID: 257-0\n",
            "Clear Text: \n",
            " \ttext: 30 65 09 .\n",
            " \tner_label: ['O', 'O', 'O', 'O']\n",
            "Tokenized: \n",
            " \ttokens: ['30', '65', '09', '.']\n",
            " \toffsets: [0, 3, 6, 9]\n",
            " \tstart_of_word: [True, True, True, True]\n",
            "Features: \n",
            " \tinput_ids: [101, 2382, 3515, 5641, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tpadding_mask: [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tinitial_mask: [0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tner_label_ids: [1, 2, 2, 2, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "_____________________________________________________\n",
            "Preprocessing Dataset ../data/conll03-de/train.txt: 100%|██████████| 12152/12152 [00:18<00:00, 639.73 Dicts/s]\n",
            "09/09/2021 14:01:14 - INFO - farm.data_handler.data_silo -   Loading dev set from: ../data/conll03-de/dev.txt\n",
            "09/09/2021 14:01:14 - ERROR - farm.data_handler.utils -   Separator \t for dataset German CONLL03 does not match the requirements. Setting seperator to whitespace\n",
            "09/09/2021 14:01:14 - INFO - farm.data_handler.data_silo -   Got ya 1 parallel workers to convert 2867 dictionaries to pytorch datasets (chunksize = 574)...\n",
            "09/09/2021 14:01:14 - INFO - farm.data_handler.data_silo -    0 \n",
            "09/09/2021 14:01:14 - INFO - farm.data_handler.data_silo -   /w\\\n",
            "09/09/2021 14:01:14 - INFO - farm.data_handler.data_silo -   /'\\\n",
            "09/09/2021 14:01:14 - INFO - farm.data_handler.data_silo -   \n",
            "Preprocessing Dataset ../data/conll03-de/dev.txt:   0%|          | 0/2867 [00:00<?, ? Dicts/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils.py:460: FutureWarning: `is_pretokenized` is deprecated and will be removed in a future version, use `is_split_into_words` instead.\n",
            "  FutureWarning,\n",
            "09/09/2021 14:01:15 - INFO - farm.data_handler.processor -   *** Show 2 random examples ***\n",
            "09/09/2021 14:01:15 - INFO - farm.data_handler.processor -   \n",
            "\n",
            "      .--.        _____                       _      \n",
            "    .'_\\/_'.     / ____|                     | |     \n",
            "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
            "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
            "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
            "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
            "   (/\\||/                             |_|           \n",
            "______\\||/___________________________________________                     \n",
            "\n",
            "ID: 255-0\n",
            "Clear Text: \n",
            " \ttext: Außerdem dürfen die Institute ihrer Klientel künftig Geld zu Zinsen leihen , die unter dem Diskontsatz von derzeit 3,25 Prozent liegen .\n",
            " \tner_label: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
            "Tokenized: \n",
            " \ttokens: ['[UNK]', '[UNK]', 'die', '[UNK]', 'i', '##hrer', '[UNK]', '[UNK]', '[UNK]', 'zu', '[UNK]', 'lei', '##hen', ',', 'die', 'un', '##ter', 'dem', '[UNK]', 'von', 'der', '##ze', '##it', '3', ',', '25', '[UNK]', 'liege', '##n', '.']\n",
            " \toffsets: [0, 9, 16, 20, 30, 31, 36, 45, 53, 58, 61, 68, 71, 75, 77, 81, 83, 87, 91, 103, 107, 110, 112, 115, 116, 117, 120, 128, 133, 135]\n",
            " \tstart_of_word: [True, True, True, True, True, False, True, True, True, True, True, True, False, True, True, True, False, True, True, True, True, False, False, True, False, False, True, True, False, True]\n",
            "Features: \n",
            " \tinput_ids: [101, 100, 100, 3280, 100, 1045, 17875, 100, 100, 100, 16950, 100, 26947, 10222, 1010, 3280, 4895, 3334, 17183, 100, 3854, 4315, 4371, 4183, 1017, 1010, 2423, 100, 17766, 2078, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tinitial_mask: [0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tner_label_ids: [1, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 1, 2, 2, 2, 2, 1, 1, 2, 1, 1, 2, 2, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "_____________________________________________________\n",
            "09/09/2021 14:01:15 - INFO - farm.data_handler.processor -   \n",
            "\n",
            "      .--.        _____                       _      \n",
            "    .'_\\/_'.     / ____|                     | |     \n",
            "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
            "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
            "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
            "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
            "   (/\\||/                             |_|           \n",
            "______\\||/___________________________________________                     \n",
            "\n",
            "ID: 156-0\n",
            "Clear Text: \n",
            " \ttext: Eher anachronistisch-bizarr wirkt dagegen die Nachbildung des Schlosses Neuschwanstein aus 52 Kilogramm Sterling-Silber von dem Madrider Büro Pedro Durán .\n",
            " \tner_label: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'I-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'B-ORG', 'I-ORG', 'O']\n",
            "Tokenized: \n",
            " \ttokens: ['[UNK]', 'ana', '##ch', '##ron', '##ist', '##isch', '-', 'bi', '##zar', '##r', 'wi', '##rk', '##t', 'da', '##ge', '##gen', 'die', '[UNK]', 'des', '[UNK]', '[UNK]', 'aus', '52', '[UNK]', '[UNK]', '-', '[UNK]', 'von', 'dem', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '.']\n",
            " \toffsets: [0, 5, 8, 10, 13, 16, 20, 21, 23, 26, 28, 30, 32, 34, 36, 38, 42, 46, 58, 62, 72, 87, 91, 94, 104, 105, 106, 120, 124, 128, 137, 142, 148, 154]\n",
            " \tstart_of_word: [True, True, False, False, False, False, False, False, False, False, True, False, False, True, False, False, True, True, True, True, True, True, True, True, True, False, False, True, True, True, True, True, True, True]\n",
            "Features: \n",
            " \tinput_ids: [101, 100, 9617, 2818, 4948, 2923, 19946, 1011, 12170, 9057, 2099, 15536, 8024, 2102, 4830, 3351, 6914, 3280, 100, 4078, 100, 100, 17151, 4720, 100, 100, 1011, 100, 3854, 17183, 100, 100, 100, 100, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tinitial_mask: [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tner_label_ids: [1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 2, 2, 2, 9, 10, 2, 2, 2, 2, 1, 1, 2, 2, 3, 2, 7, 8, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "_____________________________________________________\n",
            "Preprocessing Dataset ../data/conll03-de/dev.txt: 100%|██████████| 2867/2867 [00:04<00:00, 601.94 Dicts/s]\n",
            "09/09/2021 14:01:19 - INFO - farm.data_handler.data_silo -   Loading test set from: ../data/conll03-de/test.txt\n",
            "09/09/2021 14:01:19 - ERROR - farm.data_handler.utils -   Separator \t for dataset German CONLL03 does not match the requirements. Setting seperator to whitespace\n",
            "09/09/2021 14:01:19 - INFO - farm.data_handler.data_silo -   Got ya 1 parallel workers to convert 3005 dictionaries to pytorch datasets (chunksize = 601)...\n",
            "09/09/2021 14:01:19 - INFO - farm.data_handler.data_silo -    0 \n",
            "09/09/2021 14:01:19 - INFO - farm.data_handler.data_silo -   /w\\\n",
            "09/09/2021 14:01:19 - INFO - farm.data_handler.data_silo -   /'\\\n",
            "09/09/2021 14:01:19 - INFO - farm.data_handler.data_silo -   \n",
            "Preprocessing Dataset ../data/conll03-de/test.txt:   0%|          | 0/3005 [00:00<?, ? Dicts/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils.py:460: FutureWarning: `is_pretokenized` is deprecated and will be removed in a future version, use `is_split_into_words` instead.\n",
            "  FutureWarning,\n",
            "09/09/2021 14:01:20 - INFO - farm.data_handler.processor -   *** Show 2 random examples ***\n",
            "09/09/2021 14:01:20 - INFO - farm.data_handler.processor -   \n",
            "\n",
            "      .--.        _____                       _      \n",
            "    .'_\\/_'.     / ____|                     | |     \n",
            "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
            "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
            "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
            "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
            "   (/\\||/                             |_|           \n",
            "______\\||/___________________________________________                     \n",
            "\n",
            "ID: 391-0\n",
            "Clear Text: \n",
            " \ttext: Während sich die Fahrer nur leicht verletzten , mußte die Frau mit einem Schock ins Krankenhaus eingeliefert werden .\n",
            " \tner_label: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
            "Tokenized: \n",
            " \ttokens: ['[UNK]', 'sic', '##h', 'die', '[UNK]', 'nur', 'lei', '##cht', 've', '##rle', '##tz', '##ten', ',', 'mu', '##ß', '##te', 'die', '[UNK]', 'mit', 'eine', '##m', '[UNK]', 'ins', '[UNK]', 'ein', '##gel', '##ie', '##fer', '##t', 'we', '##rden', '.']\n",
            " \toffsets: [0, 8, 11, 13, 17, 24, 28, 31, 35, 37, 40, 42, 46, 48, 50, 51, 54, 58, 63, 67, 71, 73, 80, 84, 96, 99, 102, 104, 107, 109, 111, 116]\n",
            " \tstart_of_word: [True, True, False, True, True, True, True, False, True, False, False, False, True, True, False, False, True, True, True, True, False, True, True, True, True, False, False, False, False, True, False, True]\n",
            "Features: \n",
            " \tinput_ids: [101, 100, 14387, 2232, 3280, 100, 27617, 26947, 10143, 2310, 20927, 5753, 6528, 1010, 14163, 19310, 2618, 3280, 100, 10210, 27665, 2213, 100, 16021, 100, 16417, 12439, 2666, 7512, 2102, 2057, 18246, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tinitial_mask: [0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tner_label_ids: [1, 2, 2, 1, 2, 2, 2, 2, 1, 2, 1, 1, 1, 2, 2, 1, 1, 2, 2, 2, 2, 1, 2, 2, 2, 2, 1, 1, 1, 1, 2, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "_____________________________________________________\n",
            "09/09/2021 14:01:20 - INFO - farm.data_handler.processor -   \n",
            "\n",
            "      .--.        _____                       _      \n",
            "    .'_\\/_'.     / ____|                     | |     \n",
            "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
            "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
            "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
            "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
            "   (/\\||/                             |_|           \n",
            "______\\||/___________________________________________                     \n",
            "\n",
            "ID: 13-0\n",
            "Clear Text: \n",
            " \ttext: Zumal es sich auch noch um einen gemeinsamen Antrag der Koalitionäre handelte , war die Abstimmung eindeutig .\n",
            " \tner_label: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
            "Tokenized: \n",
            " \ttokens: ['[UNK]', 'es', 'sic', '##h', 'au', '##ch', 'no', '##ch', 'um', 'eine', '##n', 'gem', '##ein', '##sam', '##en', '[UNK]', 'der', '[UNK]', 'handel', '##te', ',', 'war', 'die', '[UNK]', 'ein', '##de', '##uti', '##g', '.']\n",
            " \toffsets: [0, 6, 9, 12, 14, 16, 19, 21, 24, 27, 31, 33, 36, 39, 42, 45, 52, 56, 69, 75, 78, 80, 84, 88, 99, 102, 104, 107, 109]\n",
            " \tstart_of_word: [True, True, True, False, True, False, True, False, True, True, False, True, False, False, False, True, True, True, True, False, True, True, True, True, True, False, False, False, True]\n",
            "Features: \n",
            " \tinput_ids: [101, 100, 9686, 14387, 2232, 8740, 2818, 2053, 2818, 8529, 27665, 2078, 17070, 12377, 21559, 2368, 100, 4315, 100, 21465, 2618, 1010, 2162, 3280, 100, 16417, 3207, 21823, 2290, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tinitial_mask: [0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tner_label_ids: [1, 2, 2, 2, 1, 2, 1, 2, 1, 2, 2, 1, 2, 1, 1, 1, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 1, 1, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "_____________________________________________________\n",
            "Preprocessing Dataset ../data/conll03-de/test.txt: 100%|██████████| 3005/3005 [00:04<00:00, 609.69 Dicts/s]\n",
            "09/09/2021 14:01:24 - INFO - farm.data_handler.data_silo -   Examples in train: 12152\n",
            "09/09/2021 14:01:24 - INFO - farm.data_handler.data_silo -   Examples in dev  : 2867\n",
            "09/09/2021 14:01:24 - INFO - farm.data_handler.data_silo -   Examples in test : 3005\n",
            "09/09/2021 14:01:24 - INFO - farm.data_handler.data_silo -   \n",
            "09/09/2021 14:01:24 - INFO - farm.data_handler.data_silo -   Longest sequence length observed after clipping:     128\n",
            "09/09/2021 14:01:24 - INFO - farm.data_handler.data_silo -   Average sequence length after clipping: 26.812870309414087\n",
            "09/09/2021 14:01:24 - INFO - farm.data_handler.data_silo -   Proportion clipped:      0.0003291639236339697\n",
            "09/09/2021 14:01:26 - INFO - farm.modeling.optimization -   Loading optimizer `TransformersAdamW`: '{'correct_bias': False, 'weight_decay': 0.01, 'lr': 2e-05}'\n",
            "09/09/2021 14:01:27 - INFO - farm.modeling.optimization -   Using scheduler 'get_linear_schedule_with_warmup'\n",
            "09/09/2021 14:01:27 - INFO - farm.modeling.optimization -   Loading schedule `get_linear_schedule_with_warmup`: '{'num_warmup_steps': 38.0, 'num_training_steps': 380}'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rbTzCDslxH0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4665a7eb-838a-4875-bf27-d7fabc85d460"
      },
      "source": [
        "model = trainer.train()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "09/09/2021 14:01:28 - INFO - farm.train -   \n",
            " \n",
            "\n",
            "          &&& &&  & &&             _____                   _             \n",
            "      && &\\/&\\|& ()|/ @, &&       / ____|                 (_)            \n",
            "      &\\/(/&/&||/& /_/)_&/_&     | |  __ _ __ _____      ___ _ __   __ _ \n",
            "   &() &\\/&|()|/&\\/ '%\" & ()     | | |_ | '__/ _ \\ \\ /\\ / / | '_ \\ / _` |\n",
            "  &_\\_&&_\\ |& |&&/&__%_/_& &&    | |__| | | | (_) \\ V  V /| | | | | (_| |\n",
            "&&   && & &| &| /& & % ()& /&&    \\_____|_|  \\___/ \\_/\\_/ |_|_| |_|\\__, |\n",
            " ()&_---()&\\&\\|&&-&&--%---()~                                       __/ |\n",
            "     &&     \\|||                                                   |___/\n",
            "             |||\n",
            "             |||\n",
            "             |||\n",
            "       , -=-~  .-^- _\n",
            "              `\n",
            "\n",
            "Train epoch 0/0 (Cur. train loss: 0.1866):  26%|██▋       | 100/380 [01:10<03:06,  1.50it/s]\n",
            "Evaluating:   0%|          | 0/90 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating:  36%|███▌      | 32/90 [00:10<00:18,  3.17it/s]\u001b[A\n",
            "Evaluating: 100%|██████████| 90/90 [00:28<00:00,  3.19it/s]\n",
            "09/09/2021 14:03:09 - INFO - farm.eval -   \n",
            "\n",
            "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "***************************************************\n",
            "***** EVALUATION | DEV SET | AFTER 100 BATCHES *****\n",
            "***************************************************\n",
            "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "\n",
            "09/09/2021 14:03:09 - INFO - farm.eval -   \n",
            " _________ ner _________\n",
            "09/09/2021 14:03:10 - INFO - farm.eval -   loss: 8.583065744805179\n",
            "09/09/2021 14:03:10 - INFO - farm.eval -   task_name: ner\n",
            "09/09/2021 14:03:10 - INFO - farm.eval -   seq_f1: 0.05785123966942149\n",
            "09/09/2021 14:03:10 - INFO - farm.eval -   report: \n",
            "            precision    recall  f1-score   support\n",
            "\n",
            "     MISC       0.31      0.04      0.07      1010\n",
            "      ORG       0.02      0.00      0.00      1207\n",
            "      PER       0.00      0.00      0.00      1401\n",
            "      LOC       0.30      0.11      0.16      1181\n",
            "\n",
            "micro avg       0.17      0.04      0.06      4799\n",
            "macro avg       0.15      0.04      0.05      4799\n",
            "\n",
            "Train epoch 0/0 (Cur. train loss: 0.1414):  53%|█████▎    | 200/380 [02:50<01:58,  1.52it/s]\n",
            "Evaluating:   0%|          | 0/90 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating:  36%|███▌      | 32/90 [00:10<00:18,  3.15it/s]\u001b[A\n",
            "Evaluating: 100%|██████████| 90/90 [00:28<00:00,  3.14it/s]\n",
            "09/09/2021 14:04:49 - INFO - farm.eval -   \n",
            "\n",
            "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "***************************************************\n",
            "***** EVALUATION | DEV SET | AFTER 200 BATCHES *****\n",
            "***************************************************\n",
            "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "\n",
            "09/09/2021 14:04:49 - INFO - farm.eval -   \n",
            " _________ ner _________\n",
            "09/09/2021 14:04:50 - INFO - farm.eval -   loss: 7.092319750644457\n",
            "09/09/2021 14:04:50 - INFO - farm.eval -   task_name: ner\n",
            "09/09/2021 14:04:50 - INFO - farm.eval -   seq_f1: 0.20022282349196244\n",
            "09/09/2021 14:04:50 - INFO - farm.eval -   report: \n",
            "            precision    recall  f1-score   support\n",
            "\n",
            "     MISC       0.87      0.06      0.12      1010\n",
            "      ORG       0.50      0.22      0.30      1207\n",
            "      PER       0.18      0.03      0.05      1401\n",
            "      LOC       0.54      0.22      0.31      1181\n",
            "\n",
            "micro avg       0.42      0.13      0.20      4799\n",
            "macro avg       0.49      0.13      0.19      4799\n",
            "\n",
            "Train epoch 0/0 (Cur. train loss: 0.1596):  79%|███████▉  | 300/380 [04:30<00:52,  1.53it/s]\n",
            "Evaluating:   0%|          | 0/90 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating:  36%|███▌      | 32/90 [00:10<00:18,  3.17it/s]\u001b[A\n",
            "Evaluating: 100%|██████████| 90/90 [00:28<00:00,  3.18it/s]\n",
            "09/09/2021 14:06:29 - INFO - farm.eval -   \n",
            "\n",
            "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "***************************************************\n",
            "***** EVALUATION | DEV SET | AFTER 300 BATCHES *****\n",
            "***************************************************\n",
            "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "\n",
            "09/09/2021 14:06:29 - INFO - farm.eval -   \n",
            " _________ ner _________\n",
            "09/09/2021 14:06:29 - INFO - farm.eval -   loss: 6.629850498674471\n",
            "09/09/2021 14:06:29 - INFO - farm.eval -   task_name: ner\n",
            "09/09/2021 14:06:30 - INFO - farm.eval -   seq_f1: 0.28800687777618567\n",
            "09/09/2021 14:06:30 - INFO - farm.eval -   report: \n",
            "            precision    recall  f1-score   support\n",
            "\n",
            "     MISC       0.74      0.13      0.23      1010\n",
            "      ORG       0.65      0.25      0.36      1207\n",
            "      PER       0.38      0.14      0.20      1401\n",
            "      LOC       0.46      0.32      0.37      1181\n",
            "\n",
            "micro avg       0.46      0.21      0.29      4799\n",
            "macro avg       0.54      0.21      0.29      4799\n",
            "\n",
            "Train epoch 0/0 (Cur. train loss: 0.0989): 100%|██████████| 380/380 [05:56<00:00,  1.07it/s]\n",
            "Evaluating: 100%|██████████| 94/94 [00:29<00:00,  3.16it/s]\n",
            "09/09/2021 14:07:55 - INFO - farm.eval -   \n",
            "\n",
            "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "***************************************************\n",
            "***** EVALUATION | TEST SET | AFTER 380 BATCHES *****\n",
            "***************************************************\n",
            "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "\n",
            "09/09/2021 14:07:55 - INFO - farm.eval -   \n",
            " _________ ner _________\n",
            "09/09/2021 14:07:55 - INFO - farm.eval -   loss: 5.213216389990884\n",
            "09/09/2021 14:07:55 - INFO - farm.eval -   task_name: ner\n",
            "09/09/2021 14:07:56 - INFO - farm.eval -   seq_f1: 0.23796448837634998\n",
            "09/09/2021 14:07:56 - INFO - farm.eval -   report: \n",
            "            precision    recall  f1-score   support\n",
            "\n",
            "     MISC       0.58      0.12      0.20       670\n",
            "      ORG       0.19      0.07      0.10       773\n",
            "      LOC       0.43      0.31      0.36      1035\n",
            "      PER       0.38      0.16      0.23      1195\n",
            "\n",
            "micro avg       0.36      0.18      0.24      3673\n",
            "macro avg       0.39      0.18      0.23      3673\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGvyT2i5WX-1"
      },
      "source": [
        "## Aufgabe\n",
        "Verwenden Sie den imdb-Datensatz aus der letzten Sitzung und nutzen Sie ein englisches BERT-Modell (z.B. ```bert-base-uncased```), um es für die Sentiment-Analyse von Film-Reviews finezutunen. Bilden Sie Gruppen und gehen Sie wie folgt vor:\n",
        "\n",
        "1. Verwenden Sie ein Subset des Datensatzes. Grund: Es ist unklar, wie viel die (kostenlose) Google Colab-GPU zu leisten im Stande ist. Verwenden Sie innerhalb der Gruppe unterschiedliche Samplezahlen des Datensatzes und testen Sie aus, wo die Grenzen liegen. \n",
        "2. Adjustieren Sie verschiedene Parameter (Learning Rate, Dropout Rate ...), um Unterschiede in der Performance festzustellen.\n",
        "3. Speichern Sie das fine-getunte BERT-Modell ab: \n",
        "```python\n",
        "model.save('path/to/directory')\n",
        "processor.save('path/to/directory')\n",
        "```\n"
      ]
    }
  ]
}