{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3BlVdla0FzLp"
   },
   "source": [
    "# **SVM applied to Detecting Insults in Social Commentary;. Available from: https://kaggle.com/c/detecting-insults-in-social-commentary.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sawqcsoPBDT4"
   },
   "source": [
    "# Quelle: https://github.com/importdata/Twitter-Sentiment-Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g4vxVy1UXOny"
   },
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "cNxovLONW70d"
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8uUGAY3SXVFt"
   },
   "source": [
    "Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "nKFJK1dKXW0P",
    "outputId": "e0b26eeb-e4e8-4826-e5d8-84b3cb38d38a"
   },
   "outputs": [],
   "source": [
    "# Je nach Dataset m√ºssen wir dieses evtl aufteilen um Trainings- und Testdaten zu erhalten (70/30)\n",
    "\n",
    "# training data\n",
    "train = pd.read_csv(\"./datasets/detecting-insults-in-social-commentary/train.csv\")\n",
    "\n",
    "# test data without labels to see how our classifiers perform on unlabeled and unseen data\n",
    "test = pd.read_csv(\"./datasets/detecting-insults-in-social-commentary/test.csv\")\n",
    "\n",
    "# test data with labels to calculate metrices\n",
    "testWithSolutions = pd.read_csv(\"./datasets/detecting-insults-in-social-commentary/test_with_solutions.csv\")\n",
    "\n",
    "# impermium_verification_labels.csv\n",
    "impermium_verification_labels = pd.read_csv(\"./datasets/detecting-insults-in-social-commentary/impermium_verification_labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "KFMhnQyGYAKS"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Insult</th>\n",
       "      <th>Date</th>\n",
       "      <th>Comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3942</th>\n",
       "      <td>1</td>\n",
       "      <td>20120502172717Z</td>\n",
       "      <td>\"you are both morons and that is never happening\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3943</th>\n",
       "      <td>0</td>\n",
       "      <td>20120528164814Z</td>\n",
       "      <td>\"Many toolbars include spell check, like Yahoo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3944</th>\n",
       "      <td>0</td>\n",
       "      <td>20120620142813Z</td>\n",
       "      <td>\"@LambeauOrWrigley\\xa0\\xa0@K.Moss\\xa0\\nSioux F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3945</th>\n",
       "      <td>0</td>\n",
       "      <td>20120528205648Z</td>\n",
       "      <td>\"How about Felix? He is sure turning into one ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3946</th>\n",
       "      <td>0</td>\n",
       "      <td>20120515200734Z</td>\n",
       "      <td>\"You're all upset, defending this hipster band...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Insult             Date  \\\n",
       "3942       1  20120502172717Z   \n",
       "3943       0  20120528164814Z   \n",
       "3944       0  20120620142813Z   \n",
       "3945       0  20120528205648Z   \n",
       "3946       0  20120515200734Z   \n",
       "\n",
       "                                                Comment  \n",
       "3942  \"you are both morons and that is never happening\"  \n",
       "3943  \"Many toolbars include spell check, like Yahoo...  \n",
       "3944  \"@LambeauOrWrigley\\xa0\\xa0@K.Moss\\xa0\\nSioux F...  \n",
       "3945  \"How about Felix? He is sure turning into one ...  \n",
       "3946  \"You're all upset, defending this hipster band...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()\n",
    "train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "VMoFzxdnYKbq"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Date</th>\n",
       "      <th>Comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2230</th>\n",
       "      <td>2231</td>\n",
       "      <td>20120528100303Z</td>\n",
       "      <td>\"FUCKIN LAME COME ON WTF STOP FUCKING OVER MY ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2231</th>\n",
       "      <td>2232</td>\n",
       "      <td>20120531185813Z</td>\n",
       "      <td>\"YOU SHUT YOUR IGNORANT PIE HOLE YOU LITTLE IN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2232</th>\n",
       "      <td>2233</td>\n",
       "      <td>20120529130822Z</td>\n",
       "      <td>\"sweetie pie is looking very much like her cou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2233</th>\n",
       "      <td>2234</td>\n",
       "      <td>20120531045826Z</td>\n",
       "      <td>\"ball4real where are you with your miami g-ayn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2234</th>\n",
       "      <td>2235</td>\n",
       "      <td>20120531184524Z</td>\n",
       "      <td>\"Man....if you are a 3 point shooter, you must...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id             Date                                            Comment\n",
       "2230  2231  20120528100303Z  \"FUCKIN LAME COME ON WTF STOP FUCKING OVER MY ...\n",
       "2231  2232  20120531185813Z  \"YOU SHUT YOUR IGNORANT PIE HOLE YOU LITTLE IN...\n",
       "2232  2233  20120529130822Z  \"sweetie pie is looking very much like her cou...\n",
       "2233  2234  20120531045826Z  \"ball4real where are you with your miami g-ayn...\n",
       "2234  2235  20120531184524Z  \"Man....if you are a 3 point shooter, you must..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()\n",
    "test.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Insult</th>\n",
       "      <th>Date</th>\n",
       "      <th>Comment</th>\n",
       "      <th>Usage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2642</th>\n",
       "      <td>0</td>\n",
       "      <td>20120612003508Z</td>\n",
       "      <td>\"Never really gave it much thought. I just fig...</td>\n",
       "      <td>PrivateTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2643</th>\n",
       "      <td>0</td>\n",
       "      <td>20120619210456Z</td>\n",
       "      <td>\"Nadie se salva de la regla 34 xd\"</td>\n",
       "      <td>PrivateTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2644</th>\n",
       "      <td>0</td>\n",
       "      <td>20120528234613Z</td>\n",
       "      <td>\"Question: Are you a boy or a girl?\"</td>\n",
       "      <td>PrivateTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2645</th>\n",
       "      <td>1</td>\n",
       "      <td>20120619153537Z</td>\n",
       "      <td>\"Leave your email or phone number and maybe yo...</td>\n",
       "      <td>PrivateTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2646</th>\n",
       "      <td>0</td>\n",
       "      <td>20120620000237Z</td>\n",
       "      <td>\"From the scenarios you present, I see you bel...</td>\n",
       "      <td>PrivateTest</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Insult             Date  \\\n",
       "2642       0  20120612003508Z   \n",
       "2643       0  20120619210456Z   \n",
       "2644       0  20120528234613Z   \n",
       "2645       1  20120619153537Z   \n",
       "2646       0  20120620000237Z   \n",
       "\n",
       "                                                Comment        Usage  \n",
       "2642  \"Never really gave it much thought. I just fig...  PrivateTest  \n",
       "2643                 \"Nadie se salva de la regla 34 xd\"  PrivateTest  \n",
       "2644               \"Question: Are you a boy or a girl?\"  PrivateTest  \n",
       "2645  \"Leave your email or phone number and maybe yo...  PrivateTest  \n",
       "2646  \"From the scenarios you present, I see you bel...  PrivateTest  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testWithSolutions.head()\n",
    "testWithSolutions.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "adGauMfDZBZx"
   },
   "source": [
    "Eintr√§ge der verschiedenen Label z√§hlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "WjgHPQl7ZAVh"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2898"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# non-toxic comment\n",
    "# Spalte der Label in Anf√ºhrungszeichen angeben\n",
    "sum(train[\"Insult\"] == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "zE6NjkZBj77q"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1049"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# toxic comment\n",
    "sum(train[\"Insult\"] == 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QXRRDki7dgYF"
   },
   "source": [
    "√úberpr√ºfung ob fehlende Eintr√§ge vorhanden sind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Xfxr36hvdyA9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Insult       0\n",
       "Date       718\n",
       "Comment      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isnull().sum()\n",
    "# train.isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "anGQX6UQfJD_"
   },
   "source": [
    "M√ºssen die Daten noch aufger√§umt werden?\n",
    "\n",
    "\n",
    "*   Sind special characters vorhanden? -> remove\n",
    "*   also remove Punctuation (wie geht man mit \"z.B.\" oder \"U.S.A\" um?)\n",
    "*   tweet-preprocessor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "MqEHkMXTfcV0"
   },
   "outputs": [],
   "source": [
    "# remove special characters -> regex schnell und einfach\n",
    "import re\n",
    "\n",
    "#set up punctuations we want to be replaced\n",
    "REPLACE_NO_SPACE = re.compile(\"(\\.)|(\\;)|(\\:)|(\\!)|(\\')|(\\?)|(\\,)|(\\\")|(\\|)|(\\()|(\\))|(\\[)|(\\])|(\\%)|(\\$)|(\\>)|(\\<)|(\\{)|(\\})\")\n",
    "REPLACE_WITH_SPACE = re.compile(\"(<br\\s/><br\\s/?)|(-)|(/)|(:).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "qb1Vi_z8lxOU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tweet-preprocessor in c:\\users\\julia\\anaconda3\\lib\\site-packages (0.6.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tweet-preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "cYkl2o4sHH0P"
   },
   "outputs": [],
   "source": [
    "import preprocessor as p\n",
    "\n",
    "# custum function to clean the dataset (combining tweet_preprocessor and reguar expression)\n",
    "def clean_tweets(df):\n",
    "  tempArr = []\n",
    "  for line in df:\n",
    "    # send to tweet_processor\n",
    "    tmpL = p.clean(line)\n",
    "    # remove puctuation\n",
    "    tmpL = REPLACE_NO_SPACE.sub(\"\", tmpL.lower()) # convert all tweets to lower cases\n",
    "    tmpL = REPLACE_WITH_SPACE.sub(\" \", tmpL)\n",
    "    tempArr.append(tmpL)\n",
    "  return tempArr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "PwKnep-hHdk_"
   },
   "outputs": [],
   "source": [
    "# clean training data\n",
    "train_tweet = clean_tweets(train[\"Comment\"])\n",
    "train_tweet = pd.DataFrame(train_tweet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "dkRRpEiXHfh8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Insult</th>\n",
       "      <th>Date</th>\n",
       "      <th>Comment</th>\n",
       "      <th>clean_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>20120618192155Z</td>\n",
       "      <td>\"You fuck your dad.\"</td>\n",
       "      <td>you fuck your dad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>20120528192215Z</td>\n",
       "      <td>\"i really don't understand your point.\\xa0 It ...</td>\n",
       "      <td>i really dont understand your point\\xa0 it see...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"A\\\\xc2\\\\xa0majority of Canadians can and has ...</td>\n",
       "      <td>a\\\\xc2\\\\xa0majority of canadians can and has b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"listen if you dont wanna get married to a man...</td>\n",
       "      <td>listen if you dont wanna get married to a man ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>20120619094753Z</td>\n",
       "      <td>\"C\\xe1c b\\u1ea1n xu\\u1ed1ng \\u0111\\u01b0\\u1edd...</td>\n",
       "      <td>c\\xe1c b\\u1ea1n xu\\u1ed1ng \\u0111\\u01b0\\u1eddn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>20120620171226Z</td>\n",
       "      <td>\"@SDL OK, but I would hope they'd sign him to ...</td>\n",
       "      <td>ok but i would hope theyd sign him to a one y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>20120503012628Z</td>\n",
       "      <td>\"Yeah and where are you now?\"</td>\n",
       "      <td>yeah and where are you now</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"shut the fuck up. you and the rest of your fa...</td>\n",
       "      <td>shut the fuck up you and the rest of your fagg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>20120502173553Z</td>\n",
       "      <td>\"Either you are fake or extremely stupid...may...</td>\n",
       "      <td>either you are fake or extremely stupidmaybe both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>20120620160512Z</td>\n",
       "      <td>\"That you are an idiot who understands neither...</td>\n",
       "      <td>that you are an idiot who understands neither ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Insult             Date                                            Comment  \\\n",
       "0       1  20120618192155Z                               \"You fuck your dad.\"   \n",
       "1       0  20120528192215Z  \"i really don't understand your point.\\xa0 It ...   \n",
       "2       0              NaN  \"A\\\\xc2\\\\xa0majority of Canadians can and has ...   \n",
       "3       0              NaN  \"listen if you dont wanna get married to a man...   \n",
       "4       0  20120619094753Z  \"C\\xe1c b\\u1ea1n xu\\u1ed1ng \\u0111\\u01b0\\u1edd...   \n",
       "5       0  20120620171226Z  \"@SDL OK, but I would hope they'd sign him to ...   \n",
       "6       0  20120503012628Z                      \"Yeah and where are you now?\"   \n",
       "7       1              NaN  \"shut the fuck up. you and the rest of your fa...   \n",
       "8       1  20120502173553Z  \"Either you are fake or extremely stupid...may...   \n",
       "9       1  20120620160512Z  \"That you are an idiot who understands neither...   \n",
       "\n",
       "                                         clean_tweet  \n",
       "0                                  you fuck your dad  \n",
       "1  i really dont understand your point\\xa0 it see...  \n",
       "2  a\\\\xc2\\\\xa0majority of canadians can and has b...  \n",
       "3  listen if you dont wanna get married to a man ...  \n",
       "4  c\\xe1c b\\u1ea1n xu\\u1ed1ng \\u0111\\u01b0\\u1eddn...  \n",
       "5   ok but i would hope theyd sign him to a one y...  \n",
       "6                         yeah and where are you now  \n",
       "7  shut the fuck up you and the rest of your fagg...  \n",
       "8  either you are fake or extremely stupidmaybe both  \n",
       "9  that you are an idiot who understands neither ...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# append cleaned tweets to the training data\n",
    "train[\"clean_tweet\"] = train_tweet\n",
    "\n",
    "# compare the cleaned and uncleaned tweets\n",
    "train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "HHBArjuNHhMq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        id             Date  \\\n",
      "0        1  20120603163526Z   \n",
      "1        2  20120531215447Z   \n",
      "2        3  20120823164228Z   \n",
      "3        4  20120826010752Z   \n",
      "4        5  20120602223825Z   \n",
      "...    ...              ...   \n",
      "2230  2231  20120528100303Z   \n",
      "2231  2232  20120531185813Z   \n",
      "2232  2233  20120529130822Z   \n",
      "2233  2234  20120531045826Z   \n",
      "2234  2235  20120531184524Z   \n",
      "\n",
      "                                                Comment  \\\n",
      "0                    \"like this if you are a tribe fan\"   \n",
      "1                 \"you're idiot.......................\"   \n",
      "2     \"I am a woman Babs, and the only \"war on women...   \n",
      "3     \"WOW & YOU BENEFITTED SO MANY WINS THIS YEAR F...   \n",
      "4     \"haha green me red you now loser whos winning ...   \n",
      "...                                                 ...   \n",
      "2230  \"FUCKIN LAME COME ON WTF STOP FUCKING OVER MY ...   \n",
      "2231  \"YOU SHUT YOUR IGNORANT PIE HOLE YOU LITTLE IN...   \n",
      "2232  \"sweetie pie is looking very much like her cou...   \n",
      "2233  \"ball4real where are you with your miami g-ayn...   \n",
      "2234  \"Man....if you are a 3 point shooter, you must...   \n",
      "\n",
      "                                            clean_tweet  \n",
      "0                      like this if you are a tribe fan  \n",
      "1                                           youre idiot  \n",
      "2     i am a woman babs and the only war on women i ...  \n",
      "3     wow & you benefitted so many wins this year fr...  \n",
      "4     haha green me red you now loser whos winning n...  \n",
      "...                                                 ...  \n",
      "2230  fuckin lame come on wtf stop fucking over my b...  \n",
      "2231  you shut your ignorant pie hole you little ins...  \n",
      "2232  sweetie pie is looking very much like her cous...  \n",
      "2233   ball4real where are you with your miami g ayness  \n",
      "2234  manif you are a point shooter you must love pl...  \n",
      "\n",
      "[2235 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# clean the test data and append the cleaned tweets to the test data\n",
    "test_tweet = clean_tweets(test[\"Comment\"])\n",
    "test_tweet = pd.DataFrame(test_tweet)\n",
    "# append cleaned tweets to the training data\n",
    "test[\"clean_tweet\"] = test_tweet\n",
    "\n",
    "# compare the cleaned and uncleaned tweets\n",
    "test.tail()\n",
    "\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "HHBArjuNHhMq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Insult             Date  \\\n",
      "0          0              NaN   \n",
      "1          0  20120618222256Z   \n",
      "2          1  20120618213617Z   \n",
      "3          0              NaN   \n",
      "4          0  20120620003825Z   \n",
      "...      ...              ...   \n",
      "2642       0  20120612003508Z   \n",
      "2643       0  20120619210456Z   \n",
      "2644       0  20120528234613Z   \n",
      "2645       1  20120619153537Z   \n",
      "2646       0  20120620000237Z   \n",
      "\n",
      "                                                Comment        Usage  \\\n",
      "0     \"THE DRUDGE REPORT\\\\n\\\\n\\\\n\\\\nYou won't see th...   PublicTest   \n",
      "1     \"@ian21\\xa0\"Roger Clemens is the fucking man, ...   PublicTest   \n",
      "2     \"Agree with Alan you are an extremest idiot.  ...   PublicTest   \n",
      "3     \"Really?\\\\n\\\\nI see Marc Lamont Hill on variou...  PrivateTest   \n",
      "4     \"Really suck isn't the word, when many of our ...  PrivateTest   \n",
      "...                                                 ...          ...   \n",
      "2642  \"Never really gave it much thought. I just fig...  PrivateTest   \n",
      "2643                 \"Nadie se salva de la regla 34 xd\"  PrivateTest   \n",
      "2644               \"Question: Are you a boy or a girl?\"  PrivateTest   \n",
      "2645  \"Leave your email or phone number and maybe yo...  PrivateTest   \n",
      "2646  \"From the scenarios you present, I see you bel...  PrivateTest   \n",
      "\n",
      "                                            clean_tweet  \n",
      "0     the drudge report\\\\n\\\\n\\\\n\\\\nyou wont see this...  \n",
      "1     \\xa0roger clemens is the fucking man and never...  \n",
      "2     agree with alan you are an extremest idiot you...  \n",
      "3     really\\\\n\\\\ni see marc lamont hill on various ...  \n",
      "4     really suck isnt the word when many of our nuc...  \n",
      "...                                                 ...  \n",
      "2642  never really gave it much thought i just figur...  \n",
      "2643                      nadie se salva de la regla xd  \n",
      "2644                   question are you a boy or a girl  \n",
      "2645  leave your email or phone number and maybe you...  \n",
      "2646  from the scenarios you present i see you belie...  \n",
      "\n",
      "[2647 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# clean the testWithSolutions data and append the cleaned tweets to the test data\n",
    "testWithSolutions_tweet = clean_tweets(testWithSolutions[\"Comment\"])\n",
    "testWithSolutions_tweet = pd.DataFrame(testWithSolutions_tweet)\n",
    "# append cleaned tweets to the training data\n",
    "testWithSolutions[\"clean_tweet\"] = testWithSolutions_tweet\n",
    "\n",
    "# compare the cleaned and uncleaned tweets\n",
    "testWithSolutions.tail()\n",
    "\n",
    "print(testWithSolutions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "dn4Ff2klHoh4"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# extract the labels from the train data\n",
    "\n",
    "# statt \"Insult\" hier wieder die Spalte angeben\n",
    "\n",
    "y_train = train.Insult.values\n",
    "y_testWithSolutions = testWithSolutions.Insult.values\n",
    "x_testWithSolutions = testWithSolutions.clean_tweet.values\n",
    "x_train = train.clean_tweet.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EB_jZITFHrq4"
   },
   "source": [
    "Vectorize tweets using CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "TLqDp-6cHuij"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "ZHe05P6xHv9B"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>also</th>\n",
       "      <th>bla</th>\n",
       "      <th>blaa</th>\n",
       "      <th>blaaa</th>\n",
       "      <th>document</th>\n",
       "      <th>is</th>\n",
       "      <th>just</th>\n",
       "      <th>letter</th>\n",
       "      <th>like</th>\n",
       "      <th>more</th>\n",
       "      <th>recognize</th>\n",
       "      <th>test</th>\n",
       "      <th>than</th>\n",
       "      <th>this</th>\n",
       "      <th>trains</th>\n",
       "      <th>with</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   also  bla  blaa  blaaa  document  is  just  letter  like  more  recognize  \\\n",
       "0     0    0     0      0         1   1     0       0     0     0          0   \n",
       "1     0    0     0      0         0   0     0       0     1     0          0   \n",
       "2     0    1     1      1         0   0     0       0     0     0          0   \n",
       "3     1    0     0      0         0   0     1       1     0     1          1   \n",
       "\n",
       "   test  than  this  trains  with  words  \n",
       "0     1     0     1       0     0      0  \n",
       "1     0     0     0       1     0      0  \n",
       "2     0     0     0       0     0      0  \n",
       "3     0     1     0       0     1      1  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = [\"This is a test document.\",\n",
    "             \"I like trains\",\n",
    "             \"Bla Blaa Blaaa\",\n",
    "            \"I also just recognize words with more than 1 letter!!\"]\n",
    "\n",
    "# initializing the countvectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# tokenize and make the document into a matrix\n",
    "document_term_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "# check the result\n",
    "pd.DataFrame(document_term_matrix.toarray(), columns = vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "BI6co_n-Hzv3"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# vectorize tweets for model building\n",
    "vectorizer = CountVectorizer(binary=True, stop_words='english')\n",
    "\n",
    "# learn a vocabulary dictionary of all tokens in the raw documents\n",
    "vectorizer.fit(list(x_train) + list(x_testWithSolutions))\n",
    "\n",
    "# transform documents to document-term matrix\n",
    "x_train_vec = vectorizer.transform(x_train)\n",
    "x_test_vec = vectorizer.transform(x_testWithSolutions)\n",
    "#x_valid_vec = vectorizer.transform(x_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8L5jCEOMH18Y"
   },
   "source": [
    "\n",
    "Model building\n",
    "\n",
    "Apply Support Vetor Classifier (SVC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "eBs7NwOvH21j"
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "# classify using support vector classifier\n",
    "svm = svm.SVC(kernel = 'linear', probability=True)\n",
    "\n",
    "# fit the SVC model based on the given training data\n",
    "prob = svm.fit(x_train_vec, y_train).predict_proba(x_test_vec)\n",
    "\n",
    "predictions = svm.predict(x_train_vec)\n",
    "predictionsToTest = svm.predict(x_test_vec)\n",
    "\n",
    "# perform classification and prediction on samples in x_test / DAMIT berechnet man die Metriken\n",
    "#y_pred_svm = svm.predict(x_valid_vec)\n",
    "# perform classification and prediction on samples in x_test / NICHT geeignet zum Berechnen von Metriken, \n",
    "# da hier keine Labels zum √úberpr√ºfen unserer predicteten Labels vorhanden sind\n",
    "#y_final_pred_svm = svm.predict(x_test_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1a7G1QcqH4u2"
   },
   "source": [
    "Accuracy score for SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "LliFtFtnH7vk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for SVC is:  80.80846241027578 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Accuracy score for SVC is: \", accuracy_score(predictionsToTest, y_testWithSolutions) * 100, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.85      0.87      2072\n",
      "           1       0.55      0.66      0.60       575\n",
      "\n",
      "    accuracy                           0.81      2647\n",
      "   macro avg       0.72      0.75      0.74      2647\n",
      "weighted avg       0.82      0.81      0.81      2647\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7365946270378189"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(predictionsToTest,y_testWithSolutions))\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "f1_score(predictionsToTest,y_testWithSolutions, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.81 accuracy with a standard deviation of 0.02\n"
     ]
    }
   ],
   "source": [
    "# Computing cross validation for svm like cell below\n",
    "# from: https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation-and-model-selection \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import svm\n",
    "\n",
    "\n",
    "\n",
    "X  = train.Comment.values\n",
    "X_vec = vectorizer.transform(X)\n",
    "X_vec = X_vec.toarray()\n",
    "\n",
    "y = train.Insult.values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "#X_train.shape, y_train.shape\n",
    "#X_test.shape, y_test.shape\n",
    "\n",
    "X_train_vec = vectorizer.transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "X_train_vec = X_train_vec.toarray()\n",
    "X_test_vec = X_test_vec.toarray()\n",
    "\n",
    "clf2 = svm.SVC(kernel='linear', C=1).fit(X_train_vec, y_train)\n",
    "clf2.score(X_test_vec, y_test)\n",
    "\n",
    "\n",
    "\n",
    "scores = cross_val_score(clf2, X_vec, y, cv=5)\n",
    "scores\n",
    "\n",
    "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k-fold cross validation for SVM (infinity loop?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from: https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation-and-model-selection \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "X  = train.Comment.values\n",
    "X_vec = vectorizer.transform(X)\n",
    "X_vec = X_vec.toarray()\n",
    "\n",
    "y = train.Insult.values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "#X_train.shape, y_train.shape\n",
    "#X_test.shape, y_test.shape\n",
    "\n",
    "X_train_vec = vectorizer.transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "X_train_vec = X_train_vec.toarray()\n",
    "X_test_vec = X_test_vec.toarray()\n",
    "\n",
    "clf = svm.fit(X_train_vec, y_train)\n",
    "clf.score(X_test_vec, y_test)\n",
    "\n",
    "\n",
    "\n",
    "scores = cross_val_score(clf, X_vec, y, cv=5)\n",
    "scores\n",
    "\n",
    "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Pm1xuajTMUu"
   },
   "source": [
    "Jetzt DataFrame mit √úbersicht √ºber toxische und nicht toxische Kommentare erstellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xp1jY48SIoNX"
   },
   "outputs": [],
   "source": [
    "#print(y_pred_svm)\n",
    "#print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xmRvb-LvJQx2"
   },
   "outputs": [],
   "source": [
    "#representation = pd.DataFrame(x_test)\n",
    "#print(representation)\n",
    "\n",
    "# Das sind die Ergebnisse unseres Klassifikators\n",
    "\n",
    "classifiedResults = {'Comment':x_valid, 'Prediction':y_pred_svm}\n",
    "df = pd.DataFrame(classifiedResults,columns=['Comment','Prediction','Result'])\n",
    "print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D9-7edN_VMTT"
   },
   "source": [
    "Ergebnis unseres Klassifikators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0BlUYYXMO5sU"
   },
   "outputs": [],
   "source": [
    "# Ersetzen von Spaltenwerten mit Bedingungen in Pandas DataFrame\n",
    "# https://www.delftstack.com/de/howto/python-pandas/pandas-replace-values-in-column/#ersetzen-von-spaltenwerten-mit-bedingungen-in-pandas-dataframe\n",
    "df.loc[df.Prediction<1,'Result']='non-toxic'\n",
    "df.loc[df.Prediction>0,'Result']='toxic'\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-0PrNKn9UhVF"
   },
   "outputs": [],
   "source": [
    "# Das sind die von Experten gelabelten Ergebnisse zum Vergleich\n",
    "\n",
    "labeledResults = {'Comment':x_valid, 'Prediction':y_valid}\n",
    "dfCheck = pd.DataFrame(classifiedResults,columns=['Comment','Prediction','Result'])\n",
    "# Ersetzen von Spaltenwerten mit Bedingungen in Pandas DataFrame\n",
    "# https://www.delftstack.com/de/howto/python-pandas/pandas-replace-values-in-column/#ersetzen-von-spaltenwerten-mit-bedingungen-in-pandas-dataframe\n",
    "dfCheck.loc[df.Prediction<1,'Result']='non-toxic'\n",
    "dfCheck.loc[df.Prediction>0,'Result']='toxic'\n",
    "\n",
    "print(dfCheck)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FkbCrc0eQVXu"
   },
   "outputs": [],
   "source": [
    "#toxicComments = []\n",
    "#for comment in df:\n",
    "#  toxicComments.append(df.Comment)\n",
    "\n",
    "#print(toxicComments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qqQzla3sGXB1"
   },
   "source": [
    "# **Na√Øve Bayes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "urpxpuwtJOpW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 2647 points : 865\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "naiveBayes = GaussianNB()\n",
    "\n",
    "y_train = train.Insult.values\n",
    "x_train = train.clean_tweet.values\n",
    "\n",
    "y_testWithSolutions = testWithSolutions.Insult.values\n",
    "x_testWithSolutions = testWithSolutions.clean_tweet.values\n",
    "\n",
    "\n",
    "\n",
    "# vectorize tweets for model building\n",
    "vectorizer = CountVectorizer(binary=True, stop_words='english')\n",
    "\n",
    "# learn a vocabulary dictionary of all tokens in the raw documents\n",
    "vectorizer.fit(list(x_train) + list(x_testWithSolutions))\n",
    "\n",
    "# transform documents to document-term matrix\n",
    "x_train_vec = vectorizer.transform(x_train)\n",
    "x_test_vec = vectorizer.transform(x_testWithSolutions)\n",
    "\n",
    "# Error : A sparse matrix was passed, but dense data is required. Use X.toarray() to convert to a dense numpy array.\n",
    "x_train_vec = x_train_vec.toarray()\n",
    "x_test_vec = x_test_vec.toarray()\n",
    "\n",
    "\n",
    "y_pred = naiveBayes.fit(x_train_vec, y_train).predict(x_test_vec)\n",
    "\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (x_test_vec.shape[0], (y_testWithSolutions != y_pred).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.81      0.77      1751\n",
      "           1       0.52      0.40      0.46       896\n",
      "\n",
      "    accuracy                           0.67      2647\n",
      "   macro avg       0.62      0.61      0.61      2647\n",
      "weighted avg       0.66      0.67      0.66      2647\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6110820935768768"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_pred,y_testWithSolutions))\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "f1_score(y_pred,y_testWithSolutions, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k-fold cross validation for Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.67 accuracy with a standard deviation of 0.01\n"
     ]
    }
   ],
   "source": [
    "# from: https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation-and-model-selection \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "X  = train.Comment.values\n",
    "X_vec = vectorizer.transform(X)\n",
    "X_vec = X_vec.toarray()\n",
    "\n",
    "y = train.Insult.values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "X_train.shape, y_train.shape\n",
    "X_test.shape, y_test.shape\n",
    "\n",
    "X_train_vec = vectorizer.transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "X_train_vec = X_train_vec.toarray()\n",
    "X_test_vec = X_test_vec.toarray()\n",
    "\n",
    "clf = naiveBayes.fit(X_train_vec, y_train)\n",
    "clf.score(X_test_vec, y_test)\n",
    "\n",
    "scores = cross_val_score(clf, X_vec, y, cv=5)\n",
    "#scores\n",
    "\n",
    "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.60 f1_macro with a standard deviation of 0.01\n"
     ]
    }
   ],
   "source": [
    "# computing cross-validated f1_macro\n",
    "scores = cross_val_score(clf, X_vec, y, cv=5, scoring='f1_macro')\n",
    "print(\"%0.2f f1_macro with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Logistic Regression** Quelle: https://towardsdatascience.com/logistic-regression-using-python-sklearn-numpy-mnist-handwriting-recognition-matplotlib-a6b31e2b166a und https://www.youtube.com/watch?v=liOeXqqhmwc&list=PLU_Ql5itzbrYUG3t1Z6df8jbb4VpF4sjX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1. Import the model you want to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2. Make an instance of the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all parameters not specified are set to their defaults\n",
    "logisticRegr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# extract the labels from the train data\n",
    "\n",
    "# statt \"Insult\" hier wieder die Spalte angeben\n",
    "\n",
    "y = train.Insult.values\n",
    "x_test = test.clean_tweet.values\n",
    "\n",
    "y_train = train.Insult.values\n",
    "y_testWithSolutions = testWithSolutions.Insult.values\n",
    "x_testWithSolutions = testWithSolutions.clean_tweet.values\n",
    "x_train = train.clean_tweet.values\n",
    "\n",
    "\n",
    "\n",
    "# use 70% for the training and 30% for Crossfold Validation (CV)\n",
    "#x_train, x_valid, y_train, y_valid = train_test_split(train.clean_tweet.values, y, \n",
    "#                                                    stratify=y, \n",
    "#                                                    random_state=1, \n",
    "#                                                    test_size=0.3, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3.**Vectorize the comments using CountVectorizer like before, or tf-idf Vectorization** Converting the text into numbers so the computer could understand it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# vectorize tweets for model building\n",
    "vectorizer = CountVectorizer(binary=True, stop_words='english')\n",
    "\n",
    "# learn a vocabulary dictionary of all tokens in the raw documents\n",
    "vectorizer.fit(list(x_train) + list(x_testWithSolutions))\n",
    "\n",
    "# transform documents to document-term matrix\n",
    "x_train_vec = vectorizer.transform(x_train)\n",
    "x_test_vec = vectorizer.transform(x_testWithSolutions)\n",
    "#x_valid_vec = vectorizer.transform(x_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4. Training the model on the data, storing the information learned from the data\n",
    "\n",
    "Model is learning the relationship between digits (x_train) and labels (y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this line of code failes, because the computer can't understand the text!\n",
    "# -> tf-idf Vectorization (convert text into numbers)\n",
    "#print(x_test_vec)\n",
    "logisticRegr.fit(x_train_vec, y_train)\n",
    "\n",
    "\n",
    "predictions = logisticRegr.predict(x_train_vec)\n",
    "predictionsToTest = logisticRegr.predict(x_test_vec)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5. Predict labels for new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on entire test data\n",
    "# DAMIT berechnet man die Metriken\n",
    "#predictions = logisticRegr.predict(x_test_vec)\n",
    "\n",
    "# NICHT geeignet zum Berechnen von Metriken, \n",
    "# da hier keine Labels zum √úberpr√ºfen unserer predicteten Labels vorhanden sind\n",
    "#final_predictions = logisticRegr.predict(x_test_vec)\n",
    "#print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measuring Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use score method to get accuracy of model\n",
    "#score = logisticRegr.score(predictionsToTest, y_testWithSolutions)\n",
    "#print(\"Accuracy using Logistic Regression: \"+str(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.84      0.89      2191\n",
      "           1       0.49      0.74      0.59       456\n",
      "\n",
      "    accuracy                           0.82      2647\n",
      "   macro avg       0.71      0.79      0.74      2647\n",
      "weighted avg       0.86      0.82      0.83      2647\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7360005711160174"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(predictionsToTest,y_testWithSolutions))\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "f1_score(predictionsToTest,y_testWithSolutions, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k-fold cross validation for linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.83 accuracy with a standard deviation of 0.01\n"
     ]
    }
   ],
   "source": [
    "# from: https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation-and-model-selection \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "X  = train.Comment.values\n",
    "X_vec = vectorizer.transform(X)\n",
    "X_vec = X_vec.toarray()\n",
    "\n",
    "y = train.Insult.values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "X_train.shape, y_train.shape\n",
    "X_test.shape, y_test.shape\n",
    "\n",
    "X_train_vec = vectorizer.transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "X_train_vec = X_train_vec.toarray()\n",
    "X_test_vec = X_test_vec.toarray()\n",
    "\n",
    "clf = logisticRegr.fit(X_train_vec, y_train)\n",
    "clf.score(X_test_vec, y_test)\n",
    "\n",
    "scores = cross_val_score(clf, X_vec, y, cv=5)\n",
    "#scores\n",
    "\n",
    "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75 f1_macro with a standard deviation of 0.02\n"
     ]
    }
   ],
   "source": [
    "# computing cross-validated f1_macro\n",
    "scores = cross_val_score(clf, X_vec, y, cv=5, scoring='f1_macro')\n",
    "print(\"%0.2f f1_macro with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datensatz TestWithSolutions zur Berechnung der Metriken ausprobieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import ##\n",
    "testWithSolutions = pd.read_csv(\"./datasets/detecting-insults-in-social-commentary/test_with_solutions.csv\")\n",
    "testWithSolutions.head()\n",
    "\n",
    "## Cleaning Data ##\n",
    "# clean testWithSolutions data\n",
    "testWithSolutions_tweet = clean_tweets(testWithSolutions[\"Comment\"])\n",
    "testWithSolutions_tweet = pd.DataFrame(testWithSolutions_tweet)\n",
    "# append cleaned tweets to the training data\n",
    "testWithSolutions[\"clean_tweet\"] = testWithSolutions_tweet\n",
    "# compare the cleaned and uncleaned tweets\n",
    "testWithSolutions.head(10)\n",
    "    \n",
    "## Getting values ##\n",
    "y = testWithSolutions.Insult.values\n",
    "X = testWithSolutions.clean_tweet.values\n",
    "\n",
    "## test and train split ##\n",
    "x_train, x_test, y_train, y_test = train_test_split(testWithSolutions.clean_tweet.values, y, \n",
    "                                                    stratify=y, \n",
    "                                                    random_state=1, \n",
    "                                                    test_size=0.3, shuffle=True)\n",
    "\n",
    "## Vectorizing ##\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# vectorize tweets for model building\n",
    "vectorizer = CountVectorizer(binary=True, stop_words='english')\n",
    "# learn a vocabulary dictionary of all tokens in the raw documents\n",
    "vectorizer.fit(list(x_train) + list(X)+list(x_test))\n",
    "# transform documents to document-term matrix\n",
    "X_vec = vectorizer.transform(X)\n",
    "x_train_vec = vectorizer.transform(x_train)\n",
    "x_test_vec = vectorizer.transform(x_test)\n",
    "\n",
    "## apply model (here: LogisticRegression) ##\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logisticRegr = LogisticRegression()\n",
    "    \n",
    "# train model on data\n",
    "logisticRegr.fit(x_train_vec, y_train)\n",
    "#logisticRegr.fit(X_vec, y)\n",
    "#logisticRegr.fit(X_vec, y_train)\n",
    "    \n",
    "# make prediction\n",
    "#predictions = logisticRegr.predict(X_vec)\n",
    "predictions = logisticRegr.predict(x_test_vec)\n",
    "final_predictions = logisticRegr.predict(x_test_vec)\n",
    "#print(predictions)\n",
    "\n",
    "## Use score method to get accuracy of model ##\n",
    "#score = logisticRegr.score(X_vec, y)\n",
    "#print(\"Accuracy using Logistic Regression: \"+str(score))\n",
    "score = logisticRegr.score(x_test_vec, y_test)\n",
    "print(\"Accuracy using Logistic Regression: \"+str(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.metrics import classification_report\n",
    "#print(classification_report(x_test_vec,y_test))\n",
    "\n",
    "#from sklearn.metrics import f1_score\n",
    "#f1_score(x_test_vec,y_test, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy.matcher import Matcher, PhraseMatcher\n",
    "import seaborn as sns\n",
    "from spacy.pipeline import EntityRuler\n",
    "from spacy import displacy\n",
    "from spacy.lang.de import German\n",
    "from spacy.lang.en import English\n",
    "import re\n",
    "from spacy.pipeline import EntityRuler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = English()\n",
    "ruler = EntityRuler(nlp)\n",
    "#english_ruler = nlp.add_pipe(nlp.create_pipe('entity_ruler', config={\"validate\":True}))\n",
    "english_ruler = nlp.add_pipe(ruler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Anlegen einer Liste toxischer Begrifflichkeiten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./datasets/hatebase/lexicon.txt') as f:\n",
    "    lexicon = f.read().lower().splitlines()\n",
    "    \n",
    "#print(lexicon[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Anlegen des Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_pattern = [\n",
    "    {\"label\": \"TOXIC\", \"pattern\": str(toxic_word)} for toxic_word in list(nlp.pipe(lexicon))\n",
    "]\n",
    "#print(toxic_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = toxic_pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruler.add_patterns(pattern)\n",
    "#print(ruler.patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = testWithSolutions.clean_tweet.values\n",
    "\n",
    "comments_processed = nlp(np.array2string(X))\n",
    "print(comments_processed.ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([(ent.text, ent.label_) for ent in comments_processed.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colors = {\"TOXIC\": \"blue\"}\n",
    "#options ={\"ents\": [\"TOXIC\"], \"colors\": colors}\n",
    "print(comments_processed.ents)\n",
    "print(comments_processed[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(comments_processed, style=\"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter_toxic = 0\n",
    "for ent in comments_processed.ents:\n",
    "    if ent.label_ == \"TOXIC\":\n",
    "        counter_toxic = counter_toxic + 1\n",
    "print(counter_toxic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testWithSolutions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l2Z9PMTkA7yf"
   },
   "source": [
    "# Using Transformers from Huggingface (https://huggingface.co/course/chapter2/2?fw=tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NjD2Rdax4ZpB"
   },
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7rz8VZbC29Oq"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "classifier([\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\", \n",
    "    \"I hate this so much!\",\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "66axxXjR0_1W"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lKVeOjIl5JpT"
   },
   "outputs": [],
   "source": [
    "raw_inputs = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\", \n",
    "    \"I hate this so much!\",\n",
    "]\n",
    "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"tf\")\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zpo1orB_5REv"
   },
   "outputs": [],
   "source": [
    "from transformers import TFAutoModel\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = TFAutoModel.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1cSat_yz5mPX"
   },
   "outputs": [],
   "source": [
    "outputs = model(inputs)\n",
    "print(outputs.last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WfHyh28i5zNB"
   },
   "outputs": [],
   "source": [
    "from transformers import TFAutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "outputs = model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_S0pp4wr53qU"
   },
   "outputs": [],
   "source": [
    "print(outputs.logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "791qLsEB6Be8"
   },
   "outputs": [],
   "source": [
    "print(outputs.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JHJIOrVI6DjO"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "predictions = tf.math.softmax(outputs.logits, axis=-1)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eJIXorOd6H9h"
   },
   "outputs": [],
   "source": [
    "model.config.id2label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l_JAaDFf6RuK"
   },
   "source": [
    "Now we can conclude that the model predicted the following:\n",
    "\n",
    "    First sentence: NEGATIVE: 0.0402, POSITIVE: 0.9598\n",
    "    Second sentence: NEGATIVE: 0.9995, POSITIVE: 0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gEeVZbsa6g01"
   },
   "outputs": [],
   "source": [
    "from transformers import BertConfig, TFBertModel\n",
    "\n",
    "# Building the config\n",
    "config = BertConfig()\n",
    "\n",
    "# Building the model from the config\n",
    "model = TFBertModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "djN3bRMz6kIq"
   },
   "outputs": [],
   "source": [
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5aNZwm1F60yy"
   },
   "source": [
    "Different loading methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1k5F3TuE61Zo"
   },
   "outputs": [],
   "source": [
    "from transformers import BertConfig, TFBertModel\n",
    "\n",
    "config = BertConfig()\n",
    "model = TFBertModel(config)\n",
    "\n",
    "# Model is randomly initialized!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cR8sUend8LH2"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "  \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W4BlZaXbGK_o"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IbSHb0y6BvQ9"
   },
   "source": [
    "**Preprocessing**\n",
    "\n",
    "\n",
    "*   normalization\n",
    "*   case folding\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "InfoLing2_Projekt.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
